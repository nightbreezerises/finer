Namespace(config_file_env='./configs/env_machine.yml', config_file_expt='./configs/expts/dog120_all.yml', alpha=0.7, N_tta=10, num_per_category='random', num_runs=10)
Number of GPUs: 1
Device ID: 0 Device Name: NVIDIA RTX A6000
['Dalmatian', 'Whippet', 'Boxer', 'Bulldog', 'Pitbull', 'Aberdeen Terrier', 'Dachshund mix', 'Chihuahua', 'Boston Terrier', 'Dalmatian', 'Pekingese', 'English Bulldog', 'Beagle', 'Foxhound', 'Basset Hound', 'Bulldog', 'Griffon Bruxellois', 'Afghan Dog', 'Basset Hound', 'Bloodhound', 'Beagle', 'Toy Chihuahua', 'Wire Fox Terrier', 'Smooth Coat Chihuahua', 'Belgian Malinois', 'Pomeranian', 'Japanese Chin', 'Scottie', 'Dachshund', 'Boston Terrier', 'French Bulldog', 'English Bulldog', 'Foxhound', 'Beagle', 'Basset Hound', 'Spitz', 'Bichon Frise', 'Keeshond', 'Foxhound', 'Poodle', 'King Charles Spaniel', 'Cocker Spaniel', 'English Springer Spaniel', 'Afghan Dog Breed', 'Afghan Hound', 'Breed Names', 'Bloodhound', 'Shih Tzu mix', 'Akita', 'Standard Poodle', 'Hound', 'Poodle', 'Bichon Frise', 'Shih Tzu', 'Hound Dog', 'Otterhound', 'St. Bernard', 'Bernese Mountain Dog', 'Newfoundland', 'Red Setter', 'English Setter', 'Pembroke Welsh Corgi', 'Golden Retriever', 'Labrador Retriever', 'Chesapeake Bay Retriever', 'Doberman Pinscher', 'Toy Poodle', 'Terrier mix', 'Chihuahua', 'St. Bernard mix', 'Toy Fox Terrier', 'Small Terrier', 'Irish Setter', 'Pit Bull Terrier', 'American Staffordshire Terrier', 'American Bulldog', 'Yorkshire Terrier', 'Berner Sennenhund', 'Scottish Terrier', 'Tibetan Mastiff', 'Labrador Retriever', 'German Shorthaired Pointer', 'Weimaraner', 'Border Terrier', 'Schnauzer mix', 'Peruvian Inca Orchid', 'Poodle mix', 'Spitz', 'Dachshund', 'Scottish Terrier', 'Jack Russell Terrier', 'Long Coat Chihuahua', 'Basset Hound mix', 'Miniature Pinscher', 'Sable and white hound', 'Possible Breeds', 'Fox Terrier', 'American Cocker Spaniel', 'terrier', 'Short-haired Chihuahua', 'Red Foxhound', 'Alaskan Malamute', 'Great Dane', 'Rottie', 'Old English Sheepdog', 'Greyhound', 'bulldog', 'Pocket Beagle', 'Lhasa Apso', 'Alaskan Husky', 'Maltese', 'Shar Pei', 'Samoyed', 'Pug', 'Chihuahua', 'Dachshund', 'Terrier', 'West Highland White Terrier', 'Hound Terrier Mix', 'Highland Terrier', 'Rottweiler', 'Shih Tzu', 'Chihuahua', 'Dalmatian', 'Boston Terrier', 'Harrier', 'Corgi Mix', 'Yorkshire Terrier', 'Pug', 'French Bulldog', 'French Bulldog', 'Pug', 'Boston Terrier', 'Chesapeake Bay Retriever', 'Boston Terrier', 'French Bulldog', 'Dalmatian', 'Westie', 'Tervuren', 'Irish Wolfhound', 'Mastiff', 'Basset Hound', 'Bloodhound', 'Coonhound', 'Bullmastiff', 'Fox Terrier', 'Rat Terrier', 'Jack Russell Terrier', 'Basset Hound', 'Boxer', 'American Eskimo Dog', 'Cavalier King Charles Spaniel', 'Cardigan Welsh Corgi', 'Afghan Breed', 'Toy Spaniel', 'Black and Tan Coonhound', 'Poodles', 'Swiss Mountain Dog', 'Belgian Tervuren', 'Italian Greyhound', 'Rhodesian Ridgeback', 'Small Terrier mix', 'Chihuahua', 'Weimaraner', 'chihuahua', 'Hound dog', 'Bulldog', 'Boxer', 'Golden Retriever', 'Beagle', 'Black Terrier', 'Boston Terrier', 'Foxhound', 'Basset Hound', 'Coonhound', 'Xoloitzcuintli', 'Chihuahua', 'Maltese', 'Bichon Frise', 'Airedale Terrier', 'Afghan Saluki', 'Bloodhound', 'Basset Hound', 'Coonhound', 'Terrier', 'Coonhound', 'Chihuahua-Terrier Mix', 'Alsatian', 'Pointer', 'Boxer', 'Vizsla', 'Cairn Terrier', 'Papillon', 'Chihuahua mix', 'Hound dog', 'Border Collie', 'Belgian Shepherd', 'Siberian Husky', 'Gordon Setter', 'Borzoi', 'Pug', 'Boston Terrier', 'French Bulldog', 'Terrier Mix', 'Husky', 'American Staffordshire Terrier', 'Terrier mix', 'GSD', 'Schnauzer', 'Continental Toy Spaniel', 'Cocker Spaniel', 'Chinese Crested', 'French Bulldog', 'Boston Terrier', 'Pug', 'Briard', 'Bouvier des Flandres', 'Doberman Pinscher', 'Dachshund', 'Boxer', 'Long-haired Chihuahua', 'Smooth-coat Chihuahua', 'Cavalier King Charles Spaniel', 'Hound', 'Poodle', 'Bearded Collie', 'Hound mix', 'Bulldog', 'Basset Hound', 'English Cocker Spaniel', 'Shih Tzu/Chihuahua mix', 'Lurcher', 'Hound', 'King Charles Spaniel', 'Basset Hound', 'Beagle', 'Dachshund', 'Basset Hound', 'Labrador Retriever', 'German Shepherd', 'St. Bernard', 'Afghan Shepherd', 'Vizsla', 'Manchester Terrier', 'Saluki', 'French Bulldog', 'Smooth-coated Chihuahua', 'Rat Terrier', 'Foxhound mix', 'Irish Terrier', 'Beagle mix', 'Affenpinscher', 'Coton de Tulear', 'Great Pyrenees', 'Beagle', 'Greyhound', 'Hound', 'Australian Terrier', 'Bernese Mountain Dog', 'Bernese Cattle Dog', 'Corgi', 'Hound', 'Beagle', 'Mixed Breed', 'English Beagle', 'Afghan Hound breed information', 'Papillon breed', 'Treeing Walker Coonhound', 'Australian Shepherd', 'German Rottweiler', 'Chihuahua puppies', 'Papillon dog', 'Chihuahua terrier mix', 'Mastiff', 'Bloodhound', 'Newfoundland', 'Shar-Pei']
0it [00:00, ?it/s]1it [00:01,  1.22s/it]2it [00:02,  1.00s/it]3it [00:03,  1.20s/it]4it [00:06,  1.92s/it]5it [00:08,  2.02s/it]6it [00:10,  2.08s/it]7it [00:13,  2.27s/it]8it [00:15,  2.21s/it]9it [00:18,  2.40s/it]10it [00:20,  2.41s/it]11it [00:23,  2.39s/it]12it [00:26,  2.54s/it]13it [00:29,  2.71s/it]14it [00:31,  2.67s/it]15it [00:33,  2.46s/it]16it [00:35,  2.32s/it]17it [00:37,  2.26s/it]18it [00:40,  2.37s/it]19it [00:42,  2.31s/it]20it [00:45,  2.59s/it]21it [00:49,  2.80s/it]22it [00:52,  2.96s/it]23it [00:55,  2.97s/it]24it [00:58,  3.01s/it]25it [01:01,  2.87s/it]26it [01:04,  2.87s/it]27it [01:06,  2.69s/it]28it [01:08,  2.61s/it]29it [01:11,  2.65s/it]30it [01:14,  2.70s/it]31it [01:18,  3.09s/it]32it [01:21,  3.01s/it]33it [01:24,  2.97s/it]34it [01:27,  2.99s/it]35it [01:29,  2.94s/it]36it [01:32,  2.89s/it]37it [01:36,  3.04s/it]38it [01:39,  3.29s/it]39it [01:42,  3.16s/it]40it [01:45,  2.99s/it]41it [01:47,  2.80s/it]42it [01:50,  2.87s/it]43it [01:53,  2.84s/it]44it [01:56,  2.84s/it]45it [01:59,  2.86s/it]46it [02:02,  2.83s/it]47it [02:05,  2.99s/it]48it [02:07,  2.87s/it]49it [02:10,  2.83s/it]50it [02:13,  2.82s/it]51it [02:16,  2.84s/it]52it [02:19,  2.77s/it]53it [02:21,  2.83s/it]54it [02:25,  2.91s/it]55it [02:27,  2.88s/it]56it [02:31,  2.96s/it]57it [02:34,  2.99s/it]58it [02:37,  2.99s/it]59it [02:39,  2.94s/it]60it [02:43,  3.14s/it]61it [02:46,  3.08s/it]62it [02:49,  3.12s/it]63it [02:53,  3.20s/it]64it [02:55,  3.06s/it]65it [02:58,  3.02s/it]66it [03:01,  2.93s/it]67it [03:03,  2.81s/it]68it [03:06,  2.67s/it]69it [03:08,  2.46s/it]70it [03:10,  2.41s/it]71it [03:12,  2.39s/it]72it [03:15,  2.51s/it]73it [03:18,  2.53s/it]74it [03:20,  2.55s/it]75it [03:23,  2.47s/it]76it [03:25,  2.57s/it]77it [03:28,  2.51s/it]78it [03:30,  2.46s/it]79it [03:33,  2.45s/it]80it [03:35,  2.45s/it]81it [03:37,  2.41s/it]82it [03:40,  2.39s/it]83it [03:43,  2.53s/it]84it [03:45,  2.49s/it]85it [03:48,  2.53s/it]86it [03:50,  2.45s/it]87it [03:52,  2.31s/it]88it [03:54,  2.32s/it]89it [03:57,  2.32s/it]90it [03:58,  2.19s/it]91it [04:00,  2.13s/it]92it [04:03,  2.36s/it]93it [04:06,  2.42s/it]94it [04:08,  2.37s/it]95it [04:11,  2.56s/it]96it [04:14,  2.76s/it]97it [04:17,  2.71s/it]98it [04:20,  2.87s/it]99it [04:23,  3.00s/it]100it [04:26,  2.85s/it]101it [04:29,  2.91s/it]102it [04:31,  2.77s/it]103it [04:34,  2.64s/it]104it [04:36,  2.58s/it]105it [04:39,  2.60s/it]106it [04:41,  2.54s/it]107it [04:44,  2.59s/it]108it [04:47,  2.77s/it]109it [04:50,  2.87s/it]110it [04:53,  2.80s/it]111it [04:56,  2.98s/it]112it [04:59,  2.99s/it]113it [05:03,  3.12s/it]114it [05:06,  3.28s/it]115it [05:09,  3.06s/it]116it [05:13,  3.27s/it]117it [05:16,  3.21s/it]118it [05:19,  3.07s/it]119it [05:21,  2.97s/it]120it [05:24,  2.84s/it]121it [05:27,  2.83s/it]122it [05:30,  3.00s/it]123it [05:33,  2.98s/it]124it [05:36,  3.11s/it]125it [05:40,  3.15s/it]126it [05:42,  2.99s/it]127it [05:45,  3.03s/it]128it [05:49,  3.07s/it]129it [05:52,  3.05s/it]130it [05:54,  3.01s/it]131it [05:58,  3.17s/it]132it [06:01,  3.22s/it]133it [06:05,  3.23s/it]134it [06:07,  3.04s/it]135it [06:10,  2.95s/it]136it [06:13,  2.97s/it]137it [06:16,  3.13s/it]138it [06:20,  3.17s/it]139it [06:22,  3.02s/it]140it [06:25,  2.95s/it]141it [06:28,  2.95s/it]142it [06:31,  2.94s/it]143it [06:34,  3.09s/it]144it [06:37,  2.93s/it]145it [06:40,  2.86s/it]146it [06:43,  2.88s/it]147it [06:45,  2.86s/it]148it [06:49,  3.02s/it]149it [06:52,  3.06s/it]150it [06:55,  2.99s/it]151it [06:57,  2.89s/it]152it [07:00,  2.77s/it]153it [07:03,  2.76s/it]154it [07:06,  3.02s/it]155it [07:09,  2.82s/it]156it [07:11,  2.74s/it]157it [07:14,  2.81s/it]158it [07:17,  2.85s/it]159it [07:20,  2.72s/it]160it [07:22,  2.71s/it]161it [07:25,  2.67s/it]162it [07:28,  2.73s/it]163it [07:31,  2.86s/it]164it [07:35,  3.22s/it]165it [07:38,  3.29s/it]166it [07:41,  3.14s/it]167it [07:45,  3.20s/it]168it [07:48,  3.17s/it]169it [07:51,  3.14s/it]170it [07:54,  3.11s/it]171it [07:56,  2.95s/it]172it [07:59,  2.91s/it]173it [08:02,  2.87s/it]174it [08:05,  3.00s/it]175it [08:08,  3.03s/it]176it [08:11,  2.97s/it]177it [08:14,  2.95s/it]178it [08:17,  2.97s/it]179it [08:20,  2.94s/it]180it [08:23,  2.95s/it]181it [08:26,  2.96s/it]182it [08:29,  2.87s/it]183it [08:32,  3.02s/it]184it [08:35,  3.01s/it]185it [08:38,  3.00s/it]186it [08:41,  3.04s/it]187it [08:45,  3.27s/it]188it [08:48,  3.24s/it]189it [08:52,  3.36s/it]190it [08:54,  3.22s/it]191it [08:58,  3.22s/it]192it [09:01,  3.32s/it]193it [09:04,  3.19s/it]194it [09:07,  3.17s/it]195it [09:10,  3.13s/it]196it [09:13,  3.04s/it]197it [09:16,  3.02s/it]198it [09:20,  3.27s/it]199it [09:24,  3.48s/it]200it [09:27,  3.42s/it]201it [09:31,  3.43s/it]202it [09:34,  3.38s/it]203it [09:37,  3.41s/it]204it [09:41,  3.32s/it]205it [09:44,  3.37s/it]206it [09:47,  3.34s/it]207it [09:51,  3.44s/it]208it [09:54,  3.44s/it]209it [09:58,  3.45s/it]210it [10:02,  3.61s/it]211it [10:06,  3.70s/it]212it [10:09,  3.56s/it]213it [10:12,  3.43s/it]214it [10:15,  3.12s/it]215it [10:19,  3.40s/it]216it [10:22,  3.45s/it]217it [10:26,  3.48s/it]218it [10:30,  3.59s/it]219it [10:33,  3.58s/it]220it [10:36,  3.47s/it]221it [10:39,  3.33s/it]222it [10:43,  3.29s/it]223it [10:46,  3.35s/it]224it [10:51,  3.69s/it]225it [10:54,  3.50s/it]226it [10:56,  3.32s/it]227it [10:59,  3.19s/it]228it [11:02,  3.09s/it]229it [11:05,  2.87s/it]230it [11:08,  3.00s/it]231it [11:11,  3.13s/it]232it [11:15,  3.37s/it]233it [11:18,  3.32s/it]234it [11:22,  3.25s/it]235it [11:26,  3.68s/it]236it [11:29,  3.55s/it]237it [11:33,  3.43s/it]238it [11:36,  3.56s/it]239it [11:41,  3.73s/it]240it [11:44,  3.66s/it]241it [11:48,  3.73s/it]242it [11:52,  3.69s/it]243it [11:55,  3.60s/it]244it [11:58,  3.56s/it]245it [12:02,  3.56s/it]246it [12:05,  3.36s/it]247it [12:08,  3.28s/it]248it [12:11,  3.14s/it]249it [12:14,  3.27s/it]250it [12:18,  3.31s/it]251it [12:22,  3.52s/it]252it [12:26,  3.59s/it]253it [12:29,  3.55s/it]254it [12:33,  3.65s/it]255it [12:37,  3.72s/it]256it [12:41,  3.76s/it]257it [12:44,  3.78s/it]258it [12:48,  3.70s/it]259it [12:52,  3.92s/it]260it [12:56,  3.74s/it]261it [12:59,  3.72s/it]262it [13:03,  3.74s/it]263it [13:07,  3.64s/it]264it [13:11,  3.81s/it]265it [13:15,  3.86s/it]266it [13:18,  3.82s/it]267it [13:23,  4.12s/it]268it [13:27,  4.00s/it]269it [13:31,  3.87s/it]270it [13:34,  3.78s/it]271it [13:38,  3.73s/it]272it [13:42,  3.94s/it]273it [13:46,  3.84s/it]274it [13:50,  3.96s/it]274it [13:50,  3.03s/it]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]  3%|▎         | 1/34 [00:09<05:06,  9.30s/it]  6%|▌         | 2/34 [00:09<02:07,  3.98s/it]  9%|▉         | 3/34 [00:17<03:04,  5.96s/it] 12%|█▏        | 4/34 [00:18<01:51,  3.70s/it] 15%|█▍        | 5/34 [00:18<01:10,  2.44s/it] 18%|█▊        | 6/34 [00:18<00:48,  1.73s/it] 21%|██        | 7/34 [00:18<00:33,  1.24s/it] 24%|██▎       | 8/34 [00:19<00:25,  1.03it/s] 26%|██▋       | 9/34 [00:19<00:18,  1.36it/s] 29%|██▉       | 10/34 [00:19<00:13,  1.75it/s] 32%|███▏      | 11/34 [00:19<00:10,  2.17it/s] 35%|███▌      | 12/34 [00:20<00:08,  2.61it/s] 38%|███▊      | 13/34 [00:20<00:07,  2.99it/s] 41%|████      | 14/34 [00:20<00:05,  3.38it/s] 44%|████▍     | 15/34 [00:20<00:05,  3.68it/s] 47%|████▋     | 16/34 [00:20<00:04,  3.94it/s] 50%|█████     | 17/34 [00:21<00:04,  4.14it/s] 53%|█████▎    | 18/34 [00:21<00:03,  4.32it/s] 56%|█████▌    | 19/34 [00:21<00:03,  4.47it/s] 59%|█████▉    | 20/34 [00:21<00:03,  4.38it/s] 62%|██████▏   | 21/34 [00:22<00:03,  3.67it/s] 65%|██████▍   | 22/34 [00:22<00:03,  3.23it/s] 68%|██████▊   | 23/34 [00:22<00:03,  3.08it/s] 71%|███████   | 24/34 [00:23<00:03,  2.91it/s] 74%|███████▎  | 25/34 [00:23<00:03,  2.90it/s] 76%|███████▋  | 26/34 [00:24<00:02,  2.82it/s] 79%|███████▉  | 27/34 [00:24<00:02,  2.75it/s] 82%|████████▏ | 28/34 [00:24<00:02,  2.73it/s] 85%|████████▌ | 29/34 [00:25<00:01,  2.74it/s] 88%|████████▊ | 30/34 [00:25<00:01,  2.65it/s] 91%|█████████ | 31/34 [00:25<00:01,  2.66it/s] 94%|█████████▍| 32/34 [00:26<00:00,  2.68it/s] 97%|█████████▋| 33/34 [00:26<00:00,  2.66it/s]100%|██████████| 34/34 [00:27<00:00,  2.81it/s]100%|██████████| 34/34 [00:27<00:00,  1.24it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967992782593
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:25,  4.41s/it]  6%|▌         | 2/34 [00:04<01:02,  1.95s/it]  9%|▉         | 3/34 [00:04<00:35,  1.15s/it] 12%|█▏        | 4/34 [00:05<00:23,  1.28it/s] 15%|█▍        | 5/34 [00:05<00:16,  1.74it/s] 18%|█▊        | 6/34 [00:05<00:12,  2.21it/s] 21%|██        | 7/34 [00:05<00:10,  2.47it/s] 24%|██▎       | 8/34 [00:06<00:09,  2.72it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.15it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.53it/s] 32%|███▏      | 11/34 [00:06<00:05,  3.86it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.11it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.27it/s] 41%|████      | 14/34 [00:07<00:04,  4.05it/s] 44%|████▍     | 15/34 [00:07<00:05,  3.63it/s] 47%|████▋     | 16/34 [00:08<00:05,  3.13it/s] 50%|█████     | 17/34 [00:08<00:05,  2.86it/s] 53%|█████▎    | 18/34 [00:08<00:05,  2.74it/s] 56%|█████▌    | 19/34 [00:09<00:05,  2.59it/s] 59%|█████▉    | 20/34 [00:09<00:05,  2.49it/s] 62%|██████▏   | 21/34 [00:10<00:05,  2.46it/s] 65%|██████▍   | 22/34 [00:10<00:04,  2.45it/s] 68%|██████▊   | 23/34 [00:11<00:04,  2.39it/s] 71%|███████   | 24/34 [00:11<00:04,  2.39it/s] 74%|███████▎  | 25/34 [00:11<00:03,  2.42it/s] 76%|███████▋  | 26/34 [00:12<00:03,  2.42it/s] 79%|███████▉  | 27/34 [00:12<00:02,  2.44it/s] 82%|████████▏ | 28/34 [00:13<00:02,  2.45it/s] 85%|████████▌ | 29/34 [00:13<00:02,  2.41it/s] 88%|████████▊ | 30/34 [00:13<00:01,  2.43it/s] 91%|█████████ | 31/34 [00:14<00:01,  2.42it/s] 94%|█████████▍| 32/34 [00:14<00:00,  2.41it/s] 97%|█████████▋| 33/34 [00:15<00:00,  2.42it/s]100%|██████████| 34/34 [00:15<00:00,  2.82it/s]100%|██████████| 34/34 [00:15<00:00,  2.18it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6132594347000122


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36968231201172
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 43.01864801864802
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.71461443147713
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.547938820121164


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.32594299316406
=========================          END          =========================
0it [00:00, ?it/s]1it [00:03,  3.82s/it]2it [00:07,  3.84s/it]3it [00:12,  4.10s/it]4it [00:15,  3.83s/it]5it [00:18,  3.53s/it]6it [00:22,  3.58s/it]7it [00:25,  3.43s/it]8it [00:28,  3.27s/it]9it [00:31,  3.35s/it]10it [00:34,  3.29s/it]11it [00:38,  3.41s/it]12it [00:42,  3.48s/it]13it [00:46,  3.59s/it]14it [00:50,  3.83s/it]15it [00:54,  3.81s/it]16it [00:57,  3.79s/it]17it [01:01,  3.77s/it]18it [01:06,  3.93s/it]19it [01:10,  3.98s/it]20it [01:13,  3.90s/it]21it [01:17,  3.85s/it]22it [01:21,  3.90s/it]23it [01:25,  3.95s/it]24it [01:29,  3.84s/it]25it [01:33,  3.92s/it]26it [01:37,  3.88s/it]27it [01:40,  3.62s/it]28it [01:43,  3.70s/it]29it [01:47,  3.54s/it]30it [01:50,  3.42s/it]31it [01:53,  3.37s/it]32it [01:57,  3.43s/it]33it [02:00,  3.42s/it]34it [02:04,  3.63s/it]35it [02:08,  3.76s/it]36it [02:12,  3.84s/it]37it [02:15,  3.65s/it]38it [02:21,  4.13s/it]39it [02:24,  4.02s/it]40it [02:29,  4.07s/it]41it [02:32,  3.91s/it]42it [02:36,  3.74s/it]43it [02:39,  3.67s/it]44it [02:43,  3.62s/it]45it [02:46,  3.52s/it]46it [02:50,  3.58s/it]47it [02:53,  3.53s/it]48it [02:56,  3.49s/it]49it [02:59,  3.38s/it]50it [03:03,  3.43s/it]51it [03:07,  3.51s/it]52it [03:11,  3.66s/it]53it [03:15,  3.83s/it]54it [03:19,  3.82s/it]55it [03:22,  3.70s/it]56it [03:25,  3.53s/it]57it [03:29,  3.48s/it]58it [03:32,  3.47s/it]59it [03:36,  3.46s/it]60it [03:39,  3.49s/it]61it [03:42,  3.42s/it]62it [03:46,  3.41s/it]63it [03:50,  3.56s/it]64it [03:54,  3.71s/it]65it [03:58,  3.83s/it]66it [04:02,  3.82s/it]67it [04:06,  3.89s/it]68it [04:09,  3.65s/it]69it [04:12,  3.58s/it]70it [04:16,  3.52s/it]71it [04:19,  3.55s/it]72it [04:22,  3.43s/it]73it [04:26,  3.41s/it]74it [04:29,  3.37s/it]75it [04:33,  3.57s/it]76it [04:36,  3.54s/it]77it [04:40,  3.65s/it]78it [04:44,  3.80s/it]79it [04:47,  3.53s/it]80it [04:51,  3.55s/it]81it [04:54,  3.38s/it]82it [04:58,  3.54s/it]83it [05:02,  3.64s/it]84it [05:05,  3.45s/it]85it [05:09,  3.63s/it]86it [05:12,  3.47s/it]87it [05:15,  3.41s/it]88it [05:19,  3.50s/it]89it [05:22,  3.41s/it]90it [05:25,  3.27s/it]91it [05:29,  3.41s/it]92it [05:33,  3.56s/it]93it [05:36,  3.40s/it]94it [05:39,  3.22s/it]95it [05:43,  3.51s/it]96it [05:46,  3.38s/it]97it [05:50,  3.60s/it]98it [05:53,  3.31s/it]99it [05:57,  3.51s/it]100it [05:59,  3.16s/it]101it [06:02,  3.09s/it]102it [06:05,  3.20s/it]103it [06:09,  3.24s/it]104it [06:12,  3.25s/it]105it [06:14,  3.02s/it]106it [06:18,  3.19s/it]107it [06:22,  3.47s/it]108it [06:26,  3.64s/it]109it [06:29,  3.41s/it]110it [06:32,  3.42s/it]111it [06:35,  3.18s/it]112it [06:38,  3.03s/it]113it [06:40,  2.93s/it]114it [06:43,  2.82s/it]115it [06:46,  3.02s/it]116it [06:50,  3.03s/it]117it [06:52,  3.00s/it]118it [06:56,  3.10s/it]119it [07:00,  3.42s/it]120it [07:04,  3.59s/it]121it [07:07,  3.56s/it]122it [07:11,  3.46s/it]123it [07:14,  3.32s/it]124it [07:16,  3.14s/it]125it [07:19,  3.06s/it]126it [07:22,  3.09s/it]127it [07:25,  3.01s/it]128it [07:28,  2.90s/it]129it [07:31,  2.83s/it]130it [07:34,  3.08s/it]131it [07:37,  3.08s/it]132it [07:40,  3.01s/it]133it [07:44,  3.24s/it]134it [07:47,  3.19s/it]135it [07:50,  3.12s/it]136it [07:53,  2.97s/it]137it [07:55,  2.82s/it]138it [07:58,  2.73s/it]139it [08:00,  2.73s/it]140it [08:03,  2.82s/it]141it [08:06,  2.84s/it]142it [08:10,  3.16s/it]143it [08:14,  3.25s/it]144it [08:16,  3.10s/it]145it [08:20,  3.17s/it]146it [08:23,  3.24s/it]147it [08:26,  3.03s/it]148it [08:28,  2.89s/it]149it [08:31,  2.92s/it]150it [08:34,  3.00s/it]151it [08:38,  3.11s/it]152it [08:42,  3.60s/it]153it [08:46,  3.67s/it]154it [08:50,  3.59s/it]155it [08:53,  3.45s/it]156it [08:56,  3.38s/it]157it [08:59,  3.31s/it]158it [09:02,  3.23s/it]159it [09:05,  3.09s/it]160it [09:08,  3.00s/it]161it [09:11,  3.11s/it]162it [09:14,  3.08s/it]163it [09:16,  2.84s/it]164it [09:19,  2.88s/it]165it [09:22,  2.88s/it]166it [09:26,  3.12s/it]167it [09:29,  3.09s/it]168it [09:32,  3.03s/it]169it [09:35,  3.09s/it]170it [09:38,  3.09s/it]171it [09:41,  2.98s/it]172it [09:43,  2.86s/it]173it [09:46,  2.76s/it]174it [09:49,  2.75s/it]175it [09:52,  2.80s/it]176it [09:55,  2.94s/it]177it [09:58,  2.95s/it]178it [10:01,  3.13s/it]179it [10:04,  2.87s/it]180it [10:06,  2.82s/it]181it [10:09,  2.72s/it]182it [10:11,  2.67s/it]183it [10:14,  2.56s/it]184it [10:17,  2.67s/it]185it [10:20,  2.92s/it]186it [10:24,  3.19s/it]187it [10:28,  3.31s/it]188it [10:30,  3.10s/it]189it [10:33,  3.01s/it]190it [10:36,  3.07s/it]191it [10:39,  3.11s/it]192it [10:42,  3.03s/it]193it [10:45,  2.94s/it]194it [10:48,  2.84s/it]195it [10:51,  2.96s/it]196it [10:54,  3.00s/it]197it [10:56,  2.87s/it]198it [10:59,  2.74s/it]199it [11:01,  2.62s/it]200it [11:04,  2.57s/it]201it [11:07,  2.86s/it]202it [11:10,  2.84s/it]203it [11:12,  2.71s/it]204it [11:15,  2.71s/it]205it [11:18,  2.81s/it]206it [11:21,  2.76s/it]207it [11:23,  2.58s/it]208it [11:25,  2.55s/it]209it [11:30,  3.03s/it]210it [11:34,  3.34s/it]211it [11:36,  2.96s/it]212it [11:38,  2.71s/it]213it [11:41,  2.79s/it]214it [11:43,  2.70s/it]215it [11:45,  2.53s/it]216it [11:48,  2.41s/it]217it [11:50,  2.43s/it]218it [11:52,  2.42s/it]219it [11:55,  2.45s/it]220it [11:58,  2.54s/it]221it [12:00,  2.42s/it]222it [12:02,  2.46s/it]223it [12:06,  2.80s/it]224it [12:09,  2.93s/it]225it [12:11,  2.60s/it]226it [12:13,  2.39s/it]227it [12:15,  2.27s/it]228it [12:17,  2.16s/it]229it [12:19,  2.03s/it]230it [12:20,  1.98s/it]231it [12:23,  2.15s/it]232it [12:25,  2.13s/it]233it [12:27,  2.11s/it]234it [12:29,  2.05s/it]235it [12:32,  2.39s/it]236it [12:34,  2.30s/it]237it [12:36,  2.14s/it]238it [12:38,  2.20s/it]239it [12:41,  2.28s/it]240it [12:43,  2.28s/it]241it [12:45,  2.09s/it]242it [12:47,  2.16s/it]243it [12:49,  2.07s/it]244it [12:51,  2.01s/it]245it [12:53,  2.08s/it]246it [12:55,  2.13s/it]247it [12:57,  2.07s/it]248it [13:00,  2.16s/it]249it [13:02,  2.28s/it]250it [13:04,  2.22s/it]251it [13:06,  2.14s/it]252it [13:09,  2.17s/it]253it [13:10,  2.04s/it]254it [13:12,  2.04s/it]255it [13:14,  2.01s/it]256it [13:16,  1.97s/it]257it [13:18,  1.94s/it]258it [13:20,  1.89s/it]259it [13:23,  2.23s/it]260it [13:25,  2.17s/it]261it [13:27,  2.12s/it]262it [13:29,  2.09s/it]263it [13:31,  2.17s/it]264it [13:33,  2.13s/it]265it [13:35,  2.11s/it]266it [13:37,  2.05s/it]267it [13:40,  2.21s/it]268it [13:42,  2.17s/it]269it [13:45,  2.33s/it]270it [13:46,  2.21s/it]271it [13:49,  2.25s/it]272it [13:51,  2.09s/it]273it [13:52,  1.95s/it]274it [13:54,  1.94s/it]274it [13:54,  3.05s/it]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:37,  4.76s/it]  6%|▌         | 2/34 [00:05<01:11,  2.22s/it]  9%|▉         | 3/34 [00:05<00:40,  1.30s/it] 12%|█▏        | 4/34 [00:05<00:26,  1.15it/s] 15%|█▍        | 5/34 [00:05<00:18,  1.58it/s] 18%|█▊        | 6/34 [00:06<00:13,  2.05it/s] 21%|██        | 7/34 [00:06<00:10,  2.53it/s] 24%|██▎       | 8/34 [00:06<00:08,  2.98it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.38it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.73it/s] 32%|███▏      | 11/34 [00:07<00:05,  4.01it/s] 35%|███▌      | 12/34 [00:07<00:05,  4.24it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.39it/s] 41%|████      | 14/34 [00:07<00:04,  4.52it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.62it/s] 47%|████▋     | 16/34 [00:08<00:03,  4.69it/s] 50%|█████     | 17/34 [00:08<00:03,  4.69it/s] 53%|█████▎    | 18/34 [00:08<00:03,  4.73it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.76it/s] 59%|█████▉    | 20/34 [00:08<00:02,  4.78it/s] 62%|██████▏   | 21/34 [00:09<00:02,  4.80it/s] 65%|██████▍   | 22/34 [00:09<00:02,  4.81it/s] 68%|██████▊   | 23/34 [00:09<00:02,  4.83it/s] 71%|███████   | 24/34 [00:09<00:02,  4.73it/s] 74%|███████▎  | 25/34 [00:09<00:01,  4.77it/s] 76%|███████▋  | 26/34 [00:10<00:01,  4.79it/s] 79%|███████▉  | 27/34 [00:10<00:01,  4.81it/s] 82%|████████▏ | 28/34 [00:10<00:01,  4.83it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.79it/s] 88%|████████▊ | 30/34 [00:11<00:00,  4.81it/s] 91%|█████████ | 31/34 [00:11<00:00,  4.83it/s] 94%|█████████▍| 32/34 [00:11<00:00,  4.84it/s] 97%|█████████▋| 33/34 [00:11<00:00,  4.84it/s]100%|██████████| 34/34 [00:11<00:00,  5.63it/s]100%|██████████| 34/34 [00:11<00:00,  2.85it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967992782593
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:15,  4.12s/it]  6%|▌         | 2/34 [00:04<00:59,  1.85s/it]  9%|▉         | 3/34 [00:04<00:34,  1.10s/it] 12%|█▏        | 4/34 [00:04<00:23,  1.29it/s] 15%|█▍        | 5/34 [00:05<00:16,  1.76it/s] 18%|█▊        | 6/34 [00:05<00:12,  2.25it/s] 21%|██        | 7/34 [00:05<00:09,  2.73it/s] 24%|██▎       | 8/34 [00:05<00:08,  3.17it/s] 26%|██▋       | 9/34 [00:05<00:07,  3.55it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.87it/s] 32%|███▏      | 11/34 [00:06<00:05,  4.11it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.31it/s] 38%|███▊      | 13/34 [00:06<00:04,  4.46it/s] 41%|████      | 14/34 [00:06<00:04,  4.55it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.64it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.67it/s] 50%|█████     | 17/34 [00:07<00:03,  4.72it/s] 53%|█████▎    | 18/34 [00:07<00:03,  4.76it/s] 56%|█████▌    | 19/34 [00:07<00:03,  4.79it/s] 59%|█████▉    | 20/34 [00:08<00:02,  4.81it/s] 62%|██████▏   | 21/34 [00:08<00:02,  4.83it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.83it/s] 68%|██████▊   | 23/34 [00:08<00:02,  4.84it/s] 71%|███████   | 24/34 [00:08<00:02,  4.85it/s] 74%|███████▎  | 25/34 [00:09<00:01,  4.83it/s] 76%|███████▋  | 26/34 [00:09<00:01,  4.84it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.85it/s] 82%|████████▏ | 28/34 [00:09<00:01,  4.81it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.83it/s] 88%|████████▊ | 30/34 [00:10<00:00,  4.83it/s] 91%|█████████ | 31/34 [00:10<00:00,  4.84it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.84it/s] 97%|█████████▋| 33/34 [00:10<00:00,  4.83it/s]100%|██████████| 34/34 [00:10<00:00,  5.64it/s]100%|██████████| 34/34 [00:11<00:00,  3.05it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.614534854888916


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36968231201172
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 42.878787878787875
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.54287901355907
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.50841784523573


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.45348358154297
=========================          END          =========================
0it [00:00, ?it/s]1it [00:03,  3.91s/it]2it [00:07,  3.98s/it]3it [00:12,  4.31s/it]4it [00:17,  4.35s/it]5it [00:21,  4.23s/it]6it [00:24,  4.03s/it]7it [00:28,  4.09s/it]8it [00:33,  4.11s/it]9it [00:37,  4.19s/it]10it [00:41,  4.18s/it]11it [00:45,  4.21s/it]12it [00:49,  4.08s/it]13it [00:53,  3.93s/it]14it [00:57,  3.90s/it]15it [01:01,  4.14s/it]16it [01:05,  4.12s/it]17it [01:09,  4.05s/it]18it [01:13,  4.10s/it]19it [01:17,  4.02s/it]20it [01:21,  4.04s/it]21it [01:26,  4.20s/it]22it [01:30,  4.10s/it]23it [01:34,  4.18s/it]24it [01:38,  4.08s/it]25it [01:41,  3.84s/it]26it [01:46,  4.22s/it]27it [01:50,  4.07s/it]28it [01:55,  4.24s/it]29it [01:59,  4.13s/it]30it [02:02,  4.05s/it]31it [02:06,  4.03s/it]32it [02:10,  3.92s/it]33it [02:14,  3.88s/it]34it [02:18,  3.88s/it]35it [02:22,  3.92s/it]36it [02:25,  3.77s/it]37it [02:29,  3.73s/it]38it [02:33,  3.77s/it]39it [02:37,  3.91s/it]40it [02:41,  4.02s/it]41it [02:46,  4.15s/it]42it [02:50,  4.25s/it]43it [02:54,  4.27s/it]44it [02:59,  4.35s/it]45it [03:03,  4.33s/it]46it [03:08,  4.37s/it]47it [03:12,  4.45s/it]48it [03:17,  4.52s/it]49it [03:21,  4.49s/it]50it [03:25,  4.34s/it]51it [03:30,  4.25s/it]52it [03:33,  4.05s/it]53it [03:37,  4.14s/it]54it [03:42,  4.24s/it]55it [03:46,  4.14s/it]56it [03:50,  4.19s/it]57it [03:54,  4.16s/it]58it [03:59,  4.25s/it]59it [04:03,  4.25s/it]60it [04:07,  4.19s/it]61it [04:11,  4.23s/it]62it [04:15,  4.01s/it]63it [04:18,  3.81s/it]64it [04:22,  3.71s/it]65it [04:26,  3.86s/it]66it [04:29,  3.76s/it]67it [04:34,  3.94s/it]68it [04:37,  3.83s/it]69it [04:41,  3.89s/it]70it [04:46,  4.05s/it]71it [04:49,  3.95s/it]72it [04:53,  3.93s/it]73it [04:57,  3.96s/it]74it [05:01,  3.76s/it]75it [05:04,  3.73s/it]76it [05:08,  3.74s/it]77it [05:12,  3.68s/it]78it [05:15,  3.69s/it]79it [05:20,  3.87s/it]80it [05:24,  3.98s/it]81it [05:28,  4.14s/it]82it [05:33,  4.16s/it]83it [05:37,  4.38s/it]84it [05:41,  4.16s/it]85it [05:46,  4.28s/it]86it [05:50,  4.28s/it]87it [05:54,  4.25s/it]88it [05:58,  4.19s/it]89it [06:02,  4.12s/it]90it [06:06,  4.11s/it]91it [06:11,  4.21s/it]92it [06:16,  4.47s/it]93it [06:20,  4.47s/it]94it [06:24,  4.31s/it]95it [06:28,  4.04s/it]96it [06:31,  3.81s/it]97it [06:35,  3.85s/it]98it [06:39,  3.81s/it]99it [06:44,  4.26s/it]100it [06:48,  4.11s/it]101it [06:52,  4.13s/it]102it [06:56,  4.26s/it]103it [07:00,  4.13s/it]104it [07:04,  3.94s/it]105it [07:08,  3.94s/it]106it [07:11,  3.83s/it]107it [07:15,  3.68s/it]108it [07:18,  3.65s/it]109it [07:22,  3.69s/it]110it [07:26,  3.71s/it]111it [07:29,  3.67s/it]112it [07:33,  3.85s/it]113it [07:37,  3.77s/it]114it [07:41,  3.75s/it]115it [07:45,  3.89s/it]116it [07:48,  3.76s/it]117it [07:52,  3.70s/it]118it [07:56,  3.86s/it]119it [08:00,  3.93s/it]120it [08:05,  4.04s/it]121it [08:09,  4.21s/it]122it [08:13,  4.15s/it]123it [08:17,  4.17s/it]124it [08:22,  4.39s/it]125it [08:26,  4.27s/it]126it [08:31,  4.24s/it]127it [08:35,  4.20s/it]128it [08:39,  4.21s/it]129it [08:43,  4.21s/it]130it [08:47,  4.24s/it]131it [08:52,  4.29s/it]132it [08:56,  4.30s/it]133it [09:00,  4.29s/it]134it [09:05,  4.45s/it]135it [09:10,  4.41s/it]136it [09:14,  4.39s/it]137it [09:18,  4.37s/it]138it [09:23,  4.36s/it]139it [09:27,  4.43s/it]140it [09:32,  4.49s/it]141it [09:35,  4.12s/it]142it [09:39,  4.15s/it]143it [09:44,  4.20s/it]144it [09:47,  3.82s/it]145it [09:50,  3.76s/it]146it [09:54,  3.81s/it]147it [09:57,  3.61s/it]148it [10:01,  3.69s/it]149it [10:04,  3.56s/it]150it [10:09,  3.77s/it]151it [10:12,  3.69s/it]152it [10:15,  3.59s/it]153it [10:19,  3.56s/it]154it [10:22,  3.56s/it]155it [10:26,  3.58s/it]156it [10:30,  3.75s/it]157it [10:34,  3.77s/it]158it [10:38,  3.85s/it]159it [10:42,  4.01s/it]160it [10:47,  4.05s/it]161it [10:51,  4.22s/it]162it [10:56,  4.31s/it]163it [11:00,  4.33s/it]164it [11:05,  4.37s/it]165it [11:10,  4.63s/it]166it [11:15,  4.65s/it]167it [11:19,  4.55s/it]168it [11:23,  4.56s/it]169it [11:28,  4.53s/it]170it [11:32,  4.44s/it]171it [11:37,  4.55s/it]172it [11:42,  4.55s/it]173it [11:46,  4.50s/it]174it [11:51,  4.56s/it]175it [11:55,  4.59s/it]176it [12:00,  4.58s/it]177it [12:04,  4.61s/it]178it [12:09,  4.57s/it]179it [12:13,  4.46s/it]180it [12:18,  4.45s/it]181it [12:23,  4.68s/it]182it [12:27,  4.64s/it]183it [12:32,  4.63s/it]184it [12:36,  4.54s/it]185it [12:41,  4.51s/it]186it [12:45,  4.51s/it]187it [12:51,  4.80s/it]188it [12:55,  4.69s/it]189it [12:59,  4.54s/it]190it [13:04,  4.49s/it]191it [13:09,  4.63s/it]192it [13:13,  4.65s/it]193it [13:19,  4.94s/it]194it [13:24,  4.95s/it]195it [13:29,  4.89s/it]196it [13:33,  4.80s/it]197it [13:38,  4.77s/it]198it [13:42,  4.58s/it]199it [13:47,  4.58s/it]200it [13:51,  4.50s/it]201it [13:55,  4.48s/it]202it [13:59,  4.34s/it]203it [14:04,  4.40s/it]204it [14:08,  4.43s/it]205it [14:13,  4.47s/it]206it [14:17,  4.39s/it]207it [14:22,  4.37s/it]208it [14:26,  4.24s/it]209it [14:30,  4.46s/it]210it [14:35,  4.56s/it]211it [14:40,  4.47s/it]212it [14:43,  4.32s/it]213it [14:48,  4.28s/it]214it [14:52,  4.16s/it]215it [14:55,  4.05s/it]216it [14:59,  4.02s/it]217it [15:04,  4.17s/it]218it [15:08,  4.20s/it]219it [15:12,  4.22s/it]220it [15:17,  4.30s/it]221it [15:21,  4.34s/it]222it [15:25,  4.23s/it]223it [15:29,  4.11s/it]224it [15:33,  4.12s/it]225it [15:38,  4.19s/it]226it [15:42,  4.23s/it]227it [15:47,  4.42s/it]228it [15:51,  4.38s/it]229it [15:55,  4.37s/it]230it [15:59,  4.20s/it]231it [16:03,  4.15s/it]232it [16:06,  3.78s/it]233it [16:10,  3.69s/it]234it [16:13,  3.59s/it]235it [16:18,  3.94s/it]236it [16:21,  3.69s/it]237it [16:25,  3.72s/it]238it [16:28,  3.67s/it]239it [16:32,  3.78s/it]240it [16:36,  3.86s/it]241it [16:40,  3.95s/it]242it [16:44,  3.89s/it]243it [16:48,  3.76s/it]244it [16:51,  3.68s/it]245it [16:55,  3.75s/it]246it [17:00,  4.10s/it]247it [17:04,  4.02s/it]248it [17:08,  4.14s/it]249it [17:12,  4.18s/it]250it [17:17,  4.27s/it]251it [17:22,  4.36s/it]252it [17:25,  4.18s/it]253it [17:30,  4.23s/it]254it [17:34,  4.35s/it]255it [17:39,  4.32s/it]256it [17:43,  4.34s/it]257it [17:48,  4.45s/it]258it [17:52,  4.37s/it]259it [17:57,  4.61s/it]260it [18:01,  4.33s/it]261it [18:05,  4.24s/it]262it [18:10,  4.47s/it]263it [18:14,  4.38s/it]264it [18:19,  4.47s/it]265it [18:23,  4.56s/it]266it [18:28,  4.50s/it]267it [18:33,  4.61s/it]268it [18:36,  4.36s/it]269it [18:41,  4.37s/it]270it [18:46,  4.58s/it]271it [18:50,  4.59s/it]272it [18:56,  4.76s/it]273it [19:00,  4.76s/it]274it [19:05,  4.76s/it]274it [19:05,  4.18s/it]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:39,  4.83s/it]  6%|▌         | 2/34 [00:07<02:02,  3.84s/it]  9%|▉         | 3/34 [00:08<01:08,  2.20s/it] 12%|█▏        | 4/34 [00:08<00:42,  1.42s/it] 15%|█▍        | 5/34 [00:08<00:28,  1.01it/s] 18%|█▊        | 6/34 [00:08<00:20,  1.38it/s] 21%|██        | 7/34 [00:09<00:14,  1.81it/s] 24%|██▎       | 8/34 [00:09<00:11,  2.24it/s] 26%|██▋       | 9/34 [00:09<00:09,  2.66it/s] 29%|██▉       | 10/34 [00:09<00:07,  3.00it/s] 32%|███▏      | 11/34 [00:09<00:06,  3.31it/s] 35%|███▌      | 12/34 [00:10<00:06,  3.58it/s] 38%|███▊      | 13/34 [00:10<00:05,  3.82it/s] 41%|████      | 14/34 [00:10<00:05,  3.97it/s] 44%|████▍     | 15/34 [00:10<00:04,  3.99it/s] 47%|████▋     | 16/34 [00:11<00:04,  4.13it/s] 50%|█████     | 17/34 [00:11<00:03,  4.30it/s] 53%|█████▎    | 18/34 [00:11<00:03,  4.43it/s] 56%|█████▌    | 19/34 [00:11<00:03,  4.55it/s] 59%|█████▉    | 20/34 [00:11<00:03,  4.60it/s] 62%|██████▏   | 21/34 [00:12<00:02,  4.59it/s] 65%|██████▍   | 22/34 [00:12<00:02,  4.60it/s] 68%|██████▊   | 23/34 [00:12<00:02,  4.61it/s] 71%|███████   | 24/34 [00:12<00:02,  4.66it/s] 74%|███████▎  | 25/34 [00:13<00:01,  4.70it/s] 76%|███████▋  | 26/34 [00:13<00:01,  4.74it/s] 79%|███████▉  | 27/34 [00:13<00:01,  4.55it/s] 82%|████████▏ | 28/34 [00:13<00:01,  4.61it/s] 85%|████████▌ | 29/34 [00:13<00:01,  4.68it/s] 88%|████████▊ | 30/34 [00:14<00:00,  4.70it/s] 91%|█████████ | 31/34 [00:14<00:00,  4.68it/s] 94%|█████████▍| 32/34 [00:14<00:00,  4.49it/s] 97%|█████████▋| 33/34 [00:14<00:00,  4.42it/s]100%|██████████| 34/34 [00:14<00:00,  4.94it/s]100%|██████████| 34/34 [00:15<00:00,  2.19it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967396736145
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:05<03:08,  5.70s/it]  6%|▌         | 2/34 [00:06<01:25,  2.67s/it]  9%|▉         | 3/34 [00:06<00:47,  1.55s/it] 12%|█▏        | 4/34 [00:06<00:30,  1.02s/it] 15%|█▍        | 5/34 [00:06<00:21,  1.37it/s] 18%|█▊        | 6/34 [00:07<00:15,  1.80it/s] 21%|██        | 7/34 [00:07<00:12,  2.24it/s] 24%|██▎       | 8/34 [00:07<00:09,  2.66it/s] 26%|██▋       | 9/34 [00:07<00:08,  3.07it/s] 29%|██▉       | 10/34 [00:07<00:07,  3.36it/s] 32%|███▏      | 11/34 [00:08<00:06,  3.67it/s] 35%|███▌      | 12/34 [00:08<00:05,  3.92it/s] 38%|███▊      | 13/34 [00:08<00:05,  4.11it/s] 41%|████      | 14/34 [00:08<00:04,  4.23it/s] 44%|████▍     | 15/34 [00:09<00:04,  4.36it/s] 47%|████▋     | 16/34 [00:09<00:04,  4.30it/s] 50%|█████     | 17/34 [00:09<00:03,  4.31it/s] 53%|█████▎    | 18/34 [00:09<00:03,  4.44it/s] 56%|█████▌    | 19/34 [00:09<00:03,  4.51it/s] 59%|█████▉    | 20/34 [00:10<00:03,  4.46it/s] 62%|██████▏   | 21/34 [00:10<00:02,  4.54it/s] 65%|██████▍   | 22/34 [00:10<00:02,  4.47it/s] 68%|██████▊   | 23/34 [00:10<00:02,  4.54it/s] 71%|███████   | 24/34 [00:11<00:02,  4.52it/s] 74%|███████▎  | 25/34 [00:11<00:01,  4.54it/s] 76%|███████▋  | 26/34 [00:11<00:01,  4.57it/s] 79%|███████▉  | 27/34 [00:11<00:01,  4.64it/s] 82%|████████▏ | 28/34 [00:11<00:01,  4.58it/s] 85%|████████▌ | 29/34 [00:12<00:01,  4.45it/s] 88%|████████▊ | 30/34 [00:12<00:00,  4.42it/s] 91%|█████████ | 31/34 [00:12<00:00,  4.46it/s] 94%|█████████▍| 32/34 [00:12<00:00,  4.57it/s] 97%|█████████▋| 33/34 [00:13<00:00,  4.65it/s]100%|██████████| 34/34 [00:13<00:00,  5.43it/s]100%|██████████| 34/34 [00:13<00:00,  2.52it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6158567070960999


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36967468261719
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 42.85547785547786
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.61518150410069
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.74793127256009


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.585670471191406
=========================          END          =========================
0it [00:00, ?it/s]1it [00:04,  4.55s/it]2it [00:09,  4.76s/it]3it [00:14,  4.83s/it]4it [00:18,  4.62s/it]5it [00:23,  4.79s/it]6it [00:28,  4.80s/it]7it [00:33,  4.77s/it]8it [00:37,  4.75s/it]9it [00:42,  4.76s/it]10it [00:47,  4.82s/it]11it [00:52,  4.80s/it]12it [00:57,  4.96s/it]13it [01:03,  5.14s/it]14it [01:07,  4.94s/it]15it [01:12,  4.99s/it]16it [01:17,  4.89s/it]17it [01:21,  4.71s/it]18it [01:26,  4.72s/it]19it [01:30,  4.57s/it]20it [01:35,  4.56s/it]21it [01:40,  4.62s/it]22it [01:44,  4.59s/it]23it [01:49,  4.58s/it]24it [01:53,  4.60s/it]25it [01:57,  4.42s/it]26it [02:02,  4.39s/it]27it [02:06,  4.44s/it]28it [02:11,  4.45s/it]29it [02:15,  4.28s/it]30it [02:18,  4.01s/it]31it [02:22,  4.13s/it]32it [02:27,  4.18s/it]33it [02:31,  4.12s/it]34it [02:35,  4.09s/it]35it [02:39,  4.12s/it]36it [02:43,  4.00s/it]37it [02:46,  3.95s/it]38it [02:51,  4.03s/it]39it [02:54,  3.75s/it]40it [02:57,  3.73s/it]41it [03:01,  3.64s/it]42it [03:05,  3.74s/it]43it [03:09,  3.89s/it]44it [03:12,  3.68s/it]45it [03:14,  3.23s/it]46it [03:17,  3.01s/it]47it [03:21,  3.24s/it]48it [03:24,  3.27s/it]49it [03:29,  3.65s/it]50it [03:32,  3.72s/it]51it [03:36,  3.72s/it]52it [03:40,  3.68s/it]53it [03:43,  3.61s/it]54it [03:47,  3.56s/it]55it [03:50,  3.59s/it]56it [03:54,  3.75s/it]57it [03:59,  4.06s/it]58it [04:04,  4.21s/it]59it [04:08,  4.17s/it]60it [04:12,  4.01s/it]61it [04:17,  4.32s/it]62it [04:22,  4.70s/it]63it [04:27,  4.79s/it]64it [04:32,  4.91s/it]65it [04:37,  4.89s/it]66it [04:42,  4.81s/it]67it [04:47,  4.99s/it]68it [04:52,  5.02s/it]69it [04:57,  5.03s/it]70it [05:02,  5.00s/it]71it [05:07,  4.90s/it]72it [05:12,  4.94s/it]73it [05:17,  5.07s/it]74it [05:22,  5.04s/it]75it [05:28,  5.09s/it]76it [05:33,  5.15s/it]77it [05:38,  5.15s/it]78it [05:43,  5.23s/it]79it [05:48,  5.20s/it]80it [05:54,  5.23s/it]81it [05:59,  5.15s/it]82it [06:04,  5.12s/it]83it [06:09,  5.15s/it]84it [06:14,  5.11s/it]85it [06:19,  5.15s/it]86it [06:25,  5.19s/it]87it [06:30,  5.24s/it]88it [06:35,  5.22s/it]89it [06:40,  5.00s/it]90it [06:44,  4.86s/it]91it [06:49,  4.73s/it]92it [06:53,  4.49s/it]93it [06:57,  4.58s/it]94it [07:02,  4.64s/it]95it [07:07,  4.66s/it]96it [07:11,  4.59s/it]97it [07:16,  4.58s/it]98it [07:20,  4.50s/it]99it [07:26,  4.94s/it]100it [07:31,  4.88s/it]101it [07:35,  4.74s/it]102it [07:40,  4.66s/it]103it [07:44,  4.64s/it]104it [07:49,  4.62s/it]105it [07:54,  4.83s/it]106it [07:59,  4.90s/it]107it [08:05,  5.10s/it]108it [08:10,  5.16s/it]109it [08:15,  5.16s/it]110it [08:21,  5.35s/it]111it [08:26,  5.28s/it]112it [08:31,  5.20s/it]113it [08:36,  5.17s/it]114it [08:42,  5.21s/it]115it [08:47,  5.27s/it]116it [08:52,  5.27s/it]117it [08:57,  5.22s/it]118it [09:02,  5.10s/it]119it [09:07,  5.07s/it]120it [09:12,  5.11s/it]121it [09:17,  5.02s/it]122it [09:23,  5.11s/it]123it [09:28,  5.13s/it]124it [09:33,  5.21s/it]125it [09:39,  5.35s/it]126it [09:44,  5.24s/it]127it [09:49,  5.38s/it]128it [09:55,  5.46s/it]129it [10:00,  5.32s/it]130it [10:06,  5.47s/it]131it [10:11,  5.45s/it]132it [10:17,  5.45s/it]133it [10:21,  5.21s/it]134it [10:25,  4.63s/it]135it [10:29,  4.53s/it]136it [10:33,  4.36s/it]137it [10:38,  4.46s/it]138it [10:43,  4.59s/it]139it [10:47,  4.49s/it]140it [10:51,  4.38s/it]141it [10:56,  4.45s/it]142it [11:00,  4.51s/it]143it [11:04,  4.42s/it]144it [11:08,  4.24s/it]145it [11:12,  4.21s/it]146it [11:18,  4.51s/it]147it [11:23,  4.72s/it]148it [11:28,  4.87s/it]149it [11:33,  4.89s/it]150it [11:38,  5.00s/it]151it [11:43,  5.03s/it]152it [11:48,  5.01s/it]153it [11:54,  5.13s/it]154it [11:59,  5.17s/it]155it [12:04,  5.21s/it]156it [12:09,  5.18s/it]157it [12:15,  5.39s/it]158it [12:20,  5.28s/it]159it [12:25,  5.22s/it]160it [12:31,  5.31s/it]161it [12:36,  5.25s/it]162it [12:41,  5.17s/it]163it [12:46,  5.13s/it]164it [12:51,  5.08s/it]165it [12:56,  5.18s/it]166it [13:02,  5.27s/it]167it [13:07,  5.32s/it]168it [13:13,  5.31s/it]169it [13:18,  5.27s/it]170it [13:23,  5.32s/it]171it [13:28,  5.32s/it]172it [13:34,  5.34s/it]173it [13:40,  5.50s/it]174it [13:45,  5.37s/it]175it [13:49,  5.16s/it]176it [13:54,  4.93s/it]177it [13:58,  4.81s/it]178it [14:02,  4.54s/it]179it [14:07,  4.45s/it]180it [14:11,  4.54s/it]181it [14:16,  4.49s/it]182it [14:20,  4.30s/it]183it [14:23,  4.01s/it]184it [14:26,  3.61s/it]185it [14:29,  3.47s/it]186it [14:32,  3.36s/it]187it [14:37,  3.94s/it]188it [14:41,  3.93s/it]189it [14:45,  3.96s/it]190it [14:50,  4.22s/it]191it [14:54,  4.07s/it]192it [14:57,  3.98s/it]193it [15:01,  3.85s/it]194it [15:05,  3.84s/it]195it [15:08,  3.81s/it]196it [15:13,  3.98s/it]197it [15:17,  3.93s/it]198it [15:21,  3.97s/it]199it [15:25,  4.14s/it]200it [15:29,  4.15s/it]201it [15:34,  4.22s/it]202it [15:39,  4.55s/it]203it [15:44,  4.80s/it]204it [15:50,  4.88s/it]205it [15:53,  4.58s/it]206it [15:58,  4.42s/it]207it [16:01,  4.29s/it]208it [16:06,  4.40s/it]209it [16:10,  4.29s/it]210it [16:14,  4.21s/it]211it [16:18,  4.05s/it]212it [16:22,  4.09s/it]213it [16:26,  4.01s/it]214it [16:30,  4.14s/it]215it [16:35,  4.18s/it]216it [16:39,  4.23s/it]217it [16:43,  4.30s/it]218it [16:48,  4.33s/it]219it [16:52,  4.37s/it]220it [16:57,  4.49s/it]221it [17:02,  4.73s/it]222it [17:07,  4.77s/it]223it [17:12,  4.79s/it]224it [17:17,  4.84s/it]225it [17:22,  4.91s/it]226it [17:27,  4.78s/it]227it [17:32,  4.86s/it]228it [17:36,  4.83s/it]229it [17:41,  4.80s/it]230it [17:46,  4.87s/it]231it [17:51,  4.85s/it]232it [17:56,  4.87s/it]233it [18:00,  4.81s/it]234it [18:05,  4.80s/it]235it [18:11,  5.03s/it]236it [18:16,  4.97s/it]237it [18:20,  4.89s/it]238it [18:26,  4.99s/it]239it [18:30,  4.93s/it]240it [18:35,  4.91s/it]241it [18:40,  4.85s/it]242it [18:45,  4.79s/it]243it [18:50,  4.90s/it]244it [18:55,  4.97s/it]245it [19:00,  4.97s/it]246it [19:05,  4.95s/it]247it [19:09,  4.86s/it]248it [19:14,  4.79s/it]249it [19:19,  4.76s/it]250it [19:23,  4.76s/it]251it [19:29,  4.87s/it]252it [19:33,  4.88s/it]253it [19:39,  5.00s/it]254it [19:44,  5.06s/it]255it [19:49,  4.92s/it]256it [19:53,  4.80s/it]257it [19:58,  4.78s/it]258it [20:02,  4.60s/it]259it [20:07,  4.68s/it]260it [20:11,  4.37s/it]261it [20:15,  4.28s/it]262it [20:18,  4.02s/it]263it [20:22,  3.98s/it]264it [20:26,  3.97s/it]265it [20:30,  4.06s/it]266it [20:34,  4.05s/it]267it [20:39,  4.30s/it]268it [20:43,  4.15s/it]269it [20:46,  3.75s/it]270it [20:49,  3.52s/it]271it [20:52,  3.35s/it]272it [20:54,  3.18s/it]273it [20:58,  3.23s/it]274it [21:01,  3.35s/it]274it [21:01,  4.61s/it]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:41,  4.90s/it]  6%|▌         | 2/34 [00:05<01:09,  2.16s/it]  9%|▉         | 3/34 [00:05<00:40,  1.30s/it] 12%|█▏        | 4/34 [00:05<00:29,  1.00it/s] 15%|█▍        | 5/34 [00:06<00:21,  1.35it/s] 18%|█▊        | 6/34 [00:06<00:16,  1.73it/s] 21%|██        | 7/34 [00:06<00:12,  2.18it/s] 24%|██▎       | 8/34 [00:06<00:09,  2.63it/s] 26%|██▋       | 9/34 [00:07<00:08,  3.04it/s] 29%|██▉       | 10/34 [00:07<00:07,  3.42it/s] 32%|███▏      | 11/34 [00:07<00:06,  3.75it/s] 35%|███▌      | 12/34 [00:07<00:05,  4.01it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.24it/s] 41%|████      | 14/34 [00:08<00:04,  4.36it/s] 44%|████▍     | 15/34 [00:08<00:04,  4.48it/s] 47%|████▋     | 16/34 [00:08<00:03,  4.56it/s] 50%|█████     | 17/34 [00:08<00:03,  4.64it/s] 53%|█████▎    | 18/34 [00:09<00:03,  4.70it/s] 56%|█████▌    | 19/34 [00:09<00:03,  4.75it/s] 59%|█████▉    | 20/34 [00:09<00:02,  4.78it/s] 62%|██████▏   | 21/34 [00:09<00:02,  4.64it/s] 65%|██████▍   | 22/34 [00:10<00:03,  3.87it/s] 68%|██████▊   | 23/34 [00:10<00:03,  3.30it/s] 71%|███████   | 24/34 [00:10<00:03,  3.02it/s] 74%|███████▎  | 25/34 [00:11<00:03,  2.85it/s] 76%|███████▋  | 26/34 [00:11<00:02,  2.72it/s] 79%|███████▉  | 27/34 [00:12<00:02,  2.62it/s] 82%|████████▏ | 28/34 [00:12<00:02,  2.57it/s] 85%|████████▌ | 29/34 [00:12<00:01,  2.53it/s] 88%|████████▊ | 30/34 [00:13<00:01,  2.51it/s] 91%|█████████ | 31/34 [00:13<00:01,  2.49it/s] 94%|█████████▍| 32/34 [00:14<00:00,  2.48it/s] 97%|█████████▋| 33/34 [00:14<00:00,  2.45it/s]100%|██████████| 34/34 [00:14<00:00,  2.84it/s]100%|██████████| 34/34 [00:14<00:00,  2.28it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967396736145
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:34,  4.69s/it]  6%|▌         | 2/34 [00:05<01:19,  2.48s/it]  9%|▉         | 3/34 [00:05<00:44,  1.44s/it] 12%|█▏        | 4/34 [00:06<00:28,  1.05it/s] 15%|█▍        | 5/34 [00:06<00:19,  1.46it/s] 18%|█▊        | 6/34 [00:06<00:14,  1.90it/s] 21%|██        | 7/34 [00:06<00:11,  2.37it/s] 24%|██▎       | 8/34 [00:06<00:09,  2.81it/s] 26%|██▋       | 9/34 [00:07<00:07,  3.23it/s] 29%|██▉       | 10/34 [00:07<00:06,  3.59it/s] 32%|███▏      | 11/34 [00:07<00:05,  3.87it/s] 35%|███▌      | 12/34 [00:07<00:05,  4.09it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.23it/s] 41%|████      | 14/34 [00:08<00:04,  4.38it/s] 44%|████▍     | 15/34 [00:08<00:04,  4.25it/s] 47%|████▋     | 16/34 [00:08<00:04,  4.23it/s] 50%|█████     | 17/34 [00:08<00:04,  4.16it/s] 53%|█████▎    | 18/34 [00:09<00:03,  4.29it/s] 56%|█████▌    | 19/34 [00:09<00:03,  4.43it/s] 59%|█████▉    | 20/34 [00:09<00:03,  4.47it/s] 62%|██████▏   | 21/34 [00:09<00:02,  4.43it/s] 65%|██████▍   | 22/34 [00:09<00:02,  4.39it/s] 68%|██████▊   | 23/34 [00:10<00:02,  4.45it/s] 71%|███████   | 24/34 [00:10<00:02,  3.82it/s] 74%|███████▎  | 25/34 [00:11<00:02,  3.16it/s] 76%|███████▋  | 26/34 [00:11<00:02,  3.48it/s] 79%|███████▉  | 27/34 [00:11<00:01,  3.77it/s] 82%|████████▏ | 28/34 [00:11<00:01,  4.02it/s] 85%|████████▌ | 29/34 [00:11<00:01,  4.22it/s] 88%|████████▊ | 30/34 [00:12<00:00,  4.39it/s] 91%|█████████ | 31/34 [00:12<00:00,  4.46it/s] 94%|█████████▍| 32/34 [00:12<00:00,  4.56it/s] 97%|█████████▋| 33/34 [00:12<00:00,  4.61it/s]100%|██████████| 34/34 [00:12<00:00,  5.33it/s]100%|██████████| 34/34 [00:13<00:00,  2.60it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6164112687110901


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36967468261719
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 43.17016317016317
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.83149304478619
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.571425394471035


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.64112854003906
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.67s/it]2it [00:06,  3.08s/it]3it [00:09,  3.37s/it]4it [00:12,  3.23s/it]5it [00:15,  3.11s/it]6it [00:18,  2.92s/it]7it [00:20,  2.64s/it]8it [00:23,  2.69s/it]9it [00:26,  2.84s/it]10it [00:28,  2.79s/it]11it [00:32,  2.99s/it]12it [00:35,  2.98s/it]13it [00:37,  2.77s/it]14it [00:40,  2.87s/it]15it [00:43,  2.87s/it]16it [00:46,  2.88s/it]17it [00:49,  2.82s/it]18it [00:51,  2.76s/it]19it [00:54,  2.81s/it]20it [00:58,  3.06s/it]21it [01:01,  3.16s/it]22it [01:05,  3.26s/it]23it [01:08,  3.34s/it]24it [01:12,  3.37s/it]25it [01:15,  3.34s/it]26it [01:18,  3.31s/it]27it [01:22,  3.34s/it]28it [01:25,  3.24s/it]29it [01:28,  3.39s/it]30it [01:32,  3.58s/it]31it [01:37,  3.75s/it]32it [01:40,  3.68s/it]33it [01:44,  3.62s/it]34it [01:48,  3.73s/it]35it [01:51,  3.73s/it]36it [01:55,  3.69s/it]37it [01:59,  3.79s/it]38it [02:03,  3.84s/it]39it [02:07,  3.85s/it]40it [02:10,  3.82s/it]41it [02:13,  3.47s/it]42it [02:16,  3.17s/it]43it [02:18,  3.05s/it]44it [02:22,  3.19s/it]45it [02:25,  3.05s/it]46it [02:28,  3.14s/it]47it [02:31,  3.17s/it]48it [02:34,  3.18s/it]49it [02:38,  3.20s/it]50it [02:40,  3.06s/it]51it [02:44,  3.08s/it]52it [02:47,  3.12s/it]53it [02:50,  3.03s/it]54it [02:53,  3.19s/it]55it [02:56,  3.22s/it]56it [03:00,  3.27s/it]57it [03:03,  3.37s/it]58it [03:06,  3.16s/it]59it [03:09,  2.98s/it]60it [03:11,  2.74s/it]61it [03:13,  2.73s/it]62it [03:16,  2.74s/it]63it [03:19,  2.71s/it]64it [03:21,  2.65s/it]65it [03:24,  2.49s/it]66it [03:26,  2.43s/it]67it [03:29,  2.64s/it]68it [03:31,  2.57s/it]69it [03:34,  2.67s/it]70it [03:37,  2.74s/it]71it [03:39,  2.61s/it]72it [03:42,  2.54s/it]73it [03:44,  2.44s/it]74it [03:46,  2.43s/it]75it [03:49,  2.45s/it]76it [03:52,  2.58s/it]77it [03:55,  2.64s/it]78it [03:57,  2.65s/it]79it [04:00,  2.63s/it]80it [04:03,  2.70s/it]81it [04:05,  2.70s/it]82it [04:08,  2.56s/it]83it [04:10,  2.56s/it]84it [04:13,  2.52s/it]85it [04:16,  2.69s/it]86it [04:18,  2.64s/it]87it [04:21,  2.54s/it]88it [04:23,  2.59s/it]89it [04:26,  2.60s/it]90it [04:28,  2.53s/it]91it [04:31,  2.55s/it]92it [04:34,  2.68s/it]93it [04:36,  2.64s/it]94it [04:39,  2.61s/it]95it [04:41,  2.50s/it]96it [04:44,  2.54s/it]97it [04:46,  2.46s/it]98it [04:48,  2.38s/it]99it [04:52,  2.65s/it]100it [04:54,  2.58s/it]101it [04:56,  2.42s/it]102it [04:58,  2.31s/it]103it [05:00,  2.20s/it]104it [05:02,  2.13s/it]105it [05:05,  2.29s/it]106it [05:07,  2.23s/it]107it [05:09,  2.18s/it]108it [05:11,  2.22s/it]109it [05:13,  2.25s/it]110it [05:16,  2.28s/it]111it [05:18,  2.31s/it]112it [05:20,  2.30s/it]113it [05:23,  2.35s/it]114it [05:26,  2.50s/it]115it [05:28,  2.49s/it]116it [05:31,  2.44s/it]117it [05:33,  2.38s/it]118it [05:35,  2.35s/it]119it [05:37,  2.36s/it]120it [05:39,  2.18s/it]121it [05:41,  1.93s/it]122it [05:42,  1.78s/it]123it [05:44,  1.76s/it]124it [05:45,  1.70s/it]125it [05:47,  1.78s/it]126it [05:49,  1.76s/it]127it [05:50,  1.69s/it]128it [05:52,  1.56s/it]129it [05:52,  1.29s/it]130it [05:53,  1.03it/s]131it [05:53,  1.23it/s]132it [05:54,  1.20it/s]133it [05:55,  1.27it/s]134it [05:56,  1.11it/s]135it [05:57,  1.07s/it]136it [05:58,  1.09s/it]137it [06:00,  1.20s/it]138it [06:01,  1.26s/it]139it [06:02,  1.11s/it]140it [06:03,  1.17s/it]141it [06:05,  1.33s/it]142it [06:07,  1.42s/it]143it [06:08,  1.50s/it]144it [06:10,  1.60s/it]145it [06:12,  1.59s/it]146it [06:13,  1.59s/it]147it [06:14,  1.47s/it]148it [06:16,  1.41s/it]149it [06:17,  1.39s/it]150it [06:18,  1.37s/it]151it [06:19,  1.23s/it]152it [06:20,  1.03s/it]153it [06:21,  1.17s/it]154it [06:24,  1.56s/it]155it [06:26,  1.65s/it]156it [06:27,  1.54s/it]157it [06:29,  1.64s/it]158it [06:30,  1.60s/it]159it [06:32,  1.55s/it]160it [06:33,  1.46s/it]161it [06:34,  1.35s/it]162it [06:36,  1.42s/it]163it [06:37,  1.39s/it]164it [06:39,  1.48s/it]165it [06:41,  1.63s/it]166it [06:43,  1.71s/it]167it [06:44,  1.65s/it]168it [06:45,  1.56s/it]169it [06:47,  1.64s/it]170it [06:49,  1.73s/it]171it [06:51,  1.79s/it]172it [06:53,  1.85s/it]173it [06:55,  1.77s/it]174it [06:56,  1.73s/it]175it [06:58,  1.58s/it]176it [06:59,  1.54s/it]177it [07:01,  1.60s/it]178it [07:02,  1.51s/it]179it [07:04,  1.72s/it]180it [07:06,  1.64s/it]181it [07:07,  1.62s/it]182it [07:09,  1.61s/it]183it [07:10,  1.55s/it]184it [07:12,  1.71s/it]185it [07:14,  1.69s/it]186it [07:16,  1.72s/it]187it [07:19,  2.12s/it]188it [07:21,  2.15s/it]189it [07:23,  2.19s/it]190it [07:25,  2.06s/it]191it [07:27,  2.12s/it]192it [07:30,  2.23s/it]193it [07:32,  2.15s/it]194it [07:34,  2.12s/it]195it [07:36,  2.10s/it]196it [07:38,  2.21s/it]197it [07:41,  2.19s/it]198it [07:43,  2.17s/it]199it [07:45,  2.18s/it]200it [07:47,  2.15s/it]201it [07:49,  2.11s/it]202it [07:51,  2.09s/it]203it [07:54,  2.22s/it]204it [07:56,  2.27s/it]205it [07:59,  2.43s/it]206it [08:01,  2.51s/it]207it [08:04,  2.43s/it]208it [08:06,  2.43s/it]209it [08:08,  2.20s/it]210it [08:10,  2.17s/it]211it [08:12,  2.20s/it]212it [08:14,  2.14s/it]213it [08:16,  2.13s/it]214it [08:18,  2.14s/it]215it [08:21,  2.27s/it]216it [08:23,  2.19s/it]217it [08:25,  2.15s/it]218it [08:27,  2.21s/it]219it [08:30,  2.34s/it]220it [08:32,  2.33s/it]221it [08:35,  2.39s/it]222it [08:38,  2.54s/it]223it [08:40,  2.44s/it]224it [08:44,  2.81s/it]225it [08:46,  2.81s/it]226it [08:49,  2.71s/it]227it [08:51,  2.57s/it]228it [08:54,  2.61s/it]229it [08:56,  2.54s/it]230it [08:59,  2.56s/it]231it [09:02,  2.72s/it]232it [09:04,  2.61s/it]233it [09:07,  2.64s/it]234it [09:10,  2.63s/it]235it [09:13,  2.74s/it]236it [09:15,  2.52s/it]237it [09:17,  2.56s/it]238it [09:19,  2.42s/it]239it [09:21,  2.32s/it]240it [09:24,  2.40s/it]241it [09:26,  2.33s/it]242it [09:29,  2.42s/it]243it [09:31,  2.25s/it]244it [09:33,  2.37s/it]245it [09:35,  2.28s/it]246it [09:38,  2.24s/it]247it [09:40,  2.31s/it]248it [09:42,  2.34s/it]249it [09:45,  2.32s/it]250it [09:47,  2.31s/it]251it [09:50,  2.37s/it]252it [09:54,  2.99s/it]253it [09:56,  2.63s/it]254it [10:00,  2.97s/it]255it [10:02,  2.80s/it]256it [10:05,  2.78s/it]257it [10:07,  2.67s/it]258it [10:10,  2.66s/it]259it [10:13,  2.72s/it]260it [10:16,  2.97s/it]261it [10:18,  2.79s/it]262it [10:23,  3.32s/it]263it [10:27,  3.60s/it]264it [10:31,  3.73s/it]265it [10:34,  3.27s/it]266it [10:36,  2.96s/it]267it [10:38,  2.87s/it]268it [10:41,  2.68s/it]269it [10:43,  2.58s/it]270it [10:45,  2.48s/it]271it [10:47,  2.36s/it]272it [10:49,  2.29s/it]273it [10:52,  2.28s/it]274it [10:54,  2.20s/it]274it [10:54,  2.39s/it]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:43,  4.95s/it]  6%|▌         | 2/34 [00:05<01:10,  2.20s/it]  9%|▉         | 3/34 [00:05<00:40,  1.29s/it] 12%|█▏        | 4/34 [00:05<00:25,  1.16it/s] 15%|█▍        | 5/34 [00:05<00:18,  1.58it/s] 18%|█▊        | 6/34 [00:06<00:13,  2.06it/s] 21%|██        | 7/34 [00:06<00:10,  2.53it/s] 24%|██▎       | 8/34 [00:06<00:08,  2.96it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.37it/s] 29%|██▉       | 10/34 [00:07<00:07,  3.32it/s] 32%|███▏      | 11/34 [00:07<00:06,  3.68it/s] 35%|███▌      | 12/34 [00:07<00:05,  3.97it/s] 38%|███▊      | 13/34 [00:07<00:05,  4.14it/s] 41%|████      | 14/34 [00:07<00:04,  4.34it/s] 44%|████▍     | 15/34 [00:08<00:04,  4.47it/s] 47%|████▋     | 16/34 [00:08<00:03,  4.57it/s] 50%|█████     | 17/34 [00:08<00:03,  4.66it/s] 53%|█████▎    | 18/34 [00:08<00:03,  4.72it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.72it/s] 59%|█████▉    | 20/34 [00:09<00:02,  4.72it/s] 62%|██████▏   | 21/34 [00:09<00:02,  4.73it/s] 65%|██████▍   | 22/34 [00:09<00:02,  4.77it/s] 68%|██████▊   | 23/34 [00:09<00:02,  4.80it/s] 71%|███████   | 24/34 [00:09<00:02,  4.80it/s] 74%|███████▎  | 25/34 [00:10<00:01,  4.82it/s] 76%|███████▋  | 26/34 [00:10<00:01,  4.83it/s] 79%|███████▉  | 27/34 [00:10<00:01,  4.84it/s] 82%|████████▏ | 28/34 [00:10<00:01,  4.81it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.77it/s] 88%|████████▊ | 30/34 [00:11<00:00,  4.80it/s] 91%|█████████ | 31/34 [00:11<00:00,  4.81it/s] 94%|█████████▍| 32/34 [00:11<00:00,  4.80it/s] 97%|█████████▋| 33/34 [00:11<00:00,  4.82it/s]100%|██████████| 34/34 [00:11<00:00,  5.21it/s]100%|██████████| 34/34 [00:12<00:00,  2.76it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967396736145
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:05<03:08,  5.70s/it]  6%|▌         | 2/34 [00:05<01:19,  2.47s/it]  9%|▉         | 3/34 [00:06<00:44,  1.44s/it] 12%|█▏        | 4/34 [00:06<00:28,  1.05it/s] 15%|█▍        | 5/34 [00:06<00:19,  1.45it/s] 18%|█▊        | 6/34 [00:06<00:14,  1.90it/s] 21%|██        | 7/34 [00:06<00:11,  2.37it/s] 24%|██▎       | 8/34 [00:07<00:09,  2.82it/s] 26%|██▋       | 9/34 [00:07<00:07,  3.22it/s] 29%|██▉       | 10/34 [00:07<00:06,  3.59it/s] 32%|███▏      | 11/34 [00:07<00:05,  3.90it/s] 35%|███▌      | 12/34 [00:08<00:05,  4.14it/s] 38%|███▊      | 13/34 [00:08<00:04,  4.32it/s] 41%|████      | 14/34 [00:08<00:04,  4.47it/s] 44%|████▍     | 15/34 [00:08<00:04,  4.56it/s] 47%|████▋     | 16/34 [00:08<00:03,  4.61it/s] 50%|█████     | 17/34 [00:09<00:03,  4.63it/s] 53%|█████▎    | 18/34 [00:09<00:03,  4.67it/s] 56%|█████▌    | 19/34 [00:09<00:03,  4.72it/s] 59%|█████▉    | 20/34 [00:09<00:02,  4.73it/s] 62%|██████▏   | 21/34 [00:09<00:02,  4.76it/s] 65%|██████▍   | 22/34 [00:10<00:02,  4.78it/s] 68%|██████▊   | 23/34 [00:10<00:02,  4.79it/s] 71%|███████   | 24/34 [00:10<00:02,  4.79it/s] 74%|███████▎  | 25/34 [00:10<00:01,  4.81it/s] 76%|███████▋  | 26/34 [00:10<00:01,  4.83it/s] 79%|███████▉  | 27/34 [00:11<00:01,  4.84it/s] 82%|████████▏ | 28/34 [00:11<00:01,  4.82it/s] 85%|████████▌ | 29/34 [00:11<00:01,  4.83it/s] 88%|████████▊ | 30/34 [00:11<00:00,  4.84it/s] 91%|█████████ | 31/34 [00:11<00:00,  4.82it/s] 94%|█████████▍| 32/34 [00:12<00:00,  4.83it/s] 97%|█████████▋| 33/34 [00:12<00:00,  4.83it/s]100%|██████████| 34/34 [00:12<00:00,  5.64it/s]100%|██████████| 34/34 [00:12<00:00,  2.65it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6149675846099854


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36967468261719
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 43.00699300699301
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.75844794252701
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.622540408446426


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.49675750732422
=========================          END          =========================
0it [00:00, ?it/s]1it [00:00,  1.10it/s]2it [00:02,  1.59s/it]3it [00:05,  1.82s/it]4it [00:08,  2.28s/it]5it [00:11,  2.70s/it]6it [00:14,  2.84s/it]7it [00:17,  2.94s/it]8it [00:20,  2.81s/it]9it [00:22,  2.77s/it]10it [00:25,  2.71s/it]11it [00:28,  2.93s/it]12it [00:31,  2.84s/it]13it [00:34,  2.81s/it]14it [00:36,  2.64s/it]15it [00:38,  2.53s/it]16it [00:41,  2.45s/it]17it [00:43,  2.34s/it]18it [00:44,  2.11s/it]19it [00:47,  2.22s/it]20it [00:49,  2.19s/it]21it [00:51,  2.15s/it]22it [00:53,  2.13s/it]23it [00:55,  2.21s/it]24it [00:57,  2.14s/it]25it [00:59,  2.10s/it]26it [01:02,  2.12s/it]27it [01:04,  2.11s/it]28it [01:06,  2.13s/it]29it [01:08,  2.14s/it]30it [01:10,  2.13s/it]31it [01:12,  2.18s/it]32it [01:15,  2.21s/it]33it [01:17,  2.16s/it]34it [01:19,  2.18s/it]35it [01:22,  2.33s/it]36it [01:24,  2.27s/it]37it [01:26,  2.14s/it]38it [01:28,  2.11s/it]39it [01:30,  2.16s/it]40it [01:32,  2.20s/it]41it [01:34,  2.20s/it]42it [01:37,  2.22s/it]43it [01:39,  2.24s/it]44it [01:41,  2.23s/it]45it [01:44,  2.34s/it]46it [01:47,  2.48s/it]47it [01:49,  2.49s/it]48it [01:52,  2.63s/it]49it [01:54,  2.54s/it]50it [01:57,  2.58s/it]51it [01:59,  2.54s/it]52it [02:02,  2.43s/it]53it [02:04,  2.36s/it]54it [02:06,  2.33s/it]55it [02:08,  2.31s/it]56it [02:11,  2.29s/it]57it [02:13,  2.32s/it]58it [02:16,  2.41s/it]59it [02:18,  2.26s/it]60it [02:20,  2.18s/it]61it [02:22,  2.19s/it]62it [02:24,  2.17s/it]63it [02:27,  2.45s/it]64it [02:29,  2.36s/it]65it [02:31,  2.19s/it]66it [02:33,  2.17s/it]67it [02:35,  2.25s/it]68it [02:38,  2.18s/it]69it [02:40,  2.23s/it]70it [02:42,  2.18s/it]71it [02:44,  2.05s/it]72it [02:46,  2.02s/it]73it [02:48,  2.05s/it]74it [02:50,  2.10s/it]75it [02:52,  2.05s/it]76it [02:54,  2.05s/it]77it [02:56,  2.06s/it]78it [02:58,  2.08s/it]79it [03:00,  2.06s/it]80it [03:02,  2.06s/it]81it [03:05,  2.13s/it]82it [03:06,  2.08s/it]83it [03:08,  1.83s/it]84it [03:09,  1.75s/it]85it [03:11,  1.73s/it]86it [03:13,  1.80s/it]87it [03:15,  1.78s/it]88it [03:17,  1.86s/it]89it [03:18,  1.74s/it]90it [03:20,  1.76s/it]91it [03:22,  1.83s/it]92it [03:24,  1.86s/it]93it [03:26,  1.85s/it]94it [03:27,  1.77s/it]95it [03:29,  1.78s/it]96it [03:31,  1.72s/it]97it [03:32,  1.69s/it]98it [03:34,  1.80s/it]99it [03:37,  2.04s/it]100it [03:39,  2.01s/it]101it [03:41,  1.92s/it]102it [03:42,  1.81s/it]103it [03:44,  1.86s/it]104it [03:46,  1.87s/it]105it [03:48,  1.85s/it]106it [03:49,  1.77s/it]107it [03:51,  1.77s/it]108it [03:53,  1.73s/it]109it [03:55,  1.73s/it]110it [03:56,  1.71s/it]111it [03:58,  1.70s/it]112it [04:00,  1.68s/it]113it [04:01,  1.66s/it]114it [04:03,  1.60s/it]115it [04:04,  1.57s/it]116it [04:06,  1.58s/it]117it [04:08,  1.72s/it]118it [04:10,  1.87s/it]119it [04:13,  2.10s/it]120it [04:14,  2.02s/it]121it [04:16,  1.97s/it]122it [04:19,  2.04s/it]123it [04:21,  2.10s/it]124it [04:23,  2.11s/it]125it [04:25,  2.09s/it]126it [04:27,  2.06s/it]127it [04:29,  2.12s/it]128it [04:31,  2.08s/it]129it [04:33,  2.10s/it]130it [04:36,  2.13s/it]131it [04:38,  2.21s/it]132it [04:40,  2.21s/it]133it [04:42,  2.18s/it]134it [04:44,  2.16s/it]135it [04:47,  2.17s/it]136it [04:49,  2.11s/it]137it [04:51,  2.10s/it]138it [04:53,  2.08s/it]139it [04:54,  2.01s/it]140it [04:56,  2.01s/it]141it [04:59,  2.10s/it]142it [05:01,  2.11s/it]143it [05:03,  2.09s/it]144it [05:05,  2.08s/it]145it [05:07,  2.02s/it]146it [05:09,  2.07s/it]147it [05:11,  2.03s/it]148it [05:13,  1.88s/it]149it [05:14,  1.67s/it]150it [05:15,  1.64s/it]151it [05:17,  1.79s/it]152it [05:20,  1.94s/it]153it [05:22,  2.07s/it]154it [05:24,  2.13s/it]155it [05:26,  2.01s/it]156it [05:28,  2.03s/it]157it [05:30,  1.91s/it]158it [05:32,  1.87s/it]159it [05:33,  1.72s/it]160it [05:34,  1.64s/it]161it [05:36,  1.69s/it]162it [05:38,  1.71s/it]163it [05:40,  1.68s/it]164it [05:42,  1.74s/it]165it [05:43,  1.81s/it]166it [05:45,  1.78s/it]167it [05:47,  1.80s/it]168it [05:49,  1.86s/it]169it [05:51,  1.79s/it]170it [05:52,  1.69s/it]171it [05:54,  1.67s/it]172it [05:55,  1.62s/it]173it [05:57,  1.60s/it]174it [05:59,  1.73s/it]175it [06:01,  1.79s/it]176it [06:03,  1.84s/it]177it [06:05,  1.85s/it]178it [06:06,  1.75s/it]179it [06:08,  1.76s/it]180it [06:10,  1.73s/it]181it [06:11,  1.74s/it]182it [06:13,  1.69s/it]183it [06:15,  1.67s/it]184it [06:16,  1.68s/it]185it [06:18,  1.66s/it]186it [06:19,  1.60s/it]187it [06:22,  1.83s/it]188it [06:23,  1.77s/it]189it [06:25,  1.70s/it]190it [06:27,  1.76s/it]191it [06:29,  1.90s/it]192it [06:31,  2.02s/it]193it [06:33,  2.01s/it]194it [06:35,  1.99s/it]195it [06:37,  1.97s/it]196it [06:39,  1.90s/it]197it [06:40,  1.82s/it]198it [06:42,  1.80s/it]199it [06:44,  1.73s/it]200it [06:45,  1.66s/it]201it [06:47,  1.69s/it]202it [06:49,  1.65s/it]203it [06:50,  1.70s/it]204it [06:52,  1.73s/it]205it [06:54,  1.80s/it]206it [06:57,  1.96s/it]207it [06:59,  1.98s/it]208it [07:01,  2.24s/it]209it [07:04,  2.34s/it]210it [07:07,  2.48s/it]211it [07:08,  2.26s/it]212it [07:10,  2.17s/it]213it [07:13,  2.17s/it]214it [07:15,  2.16s/it]215it [07:17,  2.14s/it]216it [07:19,  2.01s/it]217it [07:20,  1.91s/it]218it [07:22,  1.94s/it]219it [07:25,  2.04s/it]220it [07:27,  2.14s/it]221it [07:29,  2.19s/it]222it [07:31,  2.05s/it]223it [07:33,  1.94s/it]224it [07:34,  1.80s/it]225it [07:36,  1.95s/it]226it [07:38,  1.98s/it]227it [07:41,  2.01s/it]228it [07:43,  2.03s/it]229it [07:44,  1.98s/it]230it [07:46,  1.97s/it]231it [07:48,  1.91s/it]232it [07:50,  1.94s/it]233it [07:52,  1.85s/it]234it [07:54,  1.82s/it]235it [07:56,  2.04s/it]236it [07:58,  2.02s/it]237it [08:00,  2.03s/it]238it [08:02,  2.06s/it]239it [08:04,  2.07s/it]240it [08:06,  2.07s/it]241it [08:08,  1.87s/it]242it [08:09,  1.74s/it]243it [08:11,  1.68s/it]244it [08:12,  1.64s/it]245it [08:14,  1.62s/it]246it [08:15,  1.58s/it]247it [08:17,  1.61s/it]248it [08:19,  1.61s/it]249it [08:20,  1.62s/it]250it [08:22,  1.56s/it]251it [08:23,  1.54s/it]252it [08:25,  1.60s/it]253it [08:27,  1.57s/it]254it [08:28,  1.52s/it]255it [08:30,  1.57s/it]256it [08:31,  1.59s/it]257it [08:33,  1.53s/it]258it [08:34,  1.54s/it]259it [08:36,  1.70s/it]260it [08:38,  1.66s/it]261it [08:39,  1.63s/it]262it [08:41,  1.58s/it]263it [08:42,  1.50s/it]264it [08:44,  1.52s/it]265it [08:45,  1.53s/it]266it [08:47,  1.53s/it]267it [08:49,  1.64s/it]268it [08:50,  1.59s/it]269it [08:52,  1.57s/it]270it [08:53,  1.57s/it]271it [08:55,  1.59s/it]272it [08:56,  1.57s/it]273it [08:58,  1.57s/it]274it [09:00,  1.58s/it]274it [09:00,  1.97s/it]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:05<02:49,  5.14s/it]  6%|▌         | 2/34 [00:05<01:11,  2.24s/it]  9%|▉         | 3/34 [00:05<00:40,  1.31s/it] 12%|█▏        | 4/34 [00:05<00:26,  1.14it/s] 15%|█▍        | 5/34 [00:05<00:18,  1.57it/s] 18%|█▊        | 6/34 [00:06<00:13,  2.04it/s] 21%|██        | 7/34 [00:06<00:10,  2.50it/s] 24%|██▎       | 8/34 [00:06<00:08,  2.94it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.31it/s] 29%|██▉       | 10/34 [00:07<00:06,  3.58it/s] 32%|███▏      | 11/34 [00:07<00:05,  3.89it/s] 35%|███▌      | 12/34 [00:07<00:05,  4.09it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.25it/s] 41%|████      | 14/34 [00:07<00:04,  4.40it/s] 44%|████▍     | 15/34 [00:08<00:04,  4.51it/s] 47%|████▋     | 16/34 [00:08<00:03,  4.61it/s] 50%|█████     | 17/34 [00:08<00:03,  4.69it/s] 53%|█████▎    | 18/34 [00:08<00:03,  4.74it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.73it/s] 59%|█████▉    | 20/34 [00:09<00:02,  4.77it/s] 62%|██████▏   | 21/34 [00:09<00:02,  4.80it/s] 65%|██████▍   | 22/34 [00:09<00:02,  4.82it/s] 68%|██████▊   | 23/34 [00:09<00:02,  4.84it/s] 71%|███████   | 24/34 [00:09<00:02,  4.84it/s] 74%|███████▎  | 25/34 [00:10<00:01,  4.84it/s] 76%|███████▋  | 26/34 [00:10<00:01,  4.83it/s] 79%|███████▉  | 27/34 [00:10<00:01,  4.83it/s] 82%|████████▏ | 28/34 [00:10<00:01,  4.84it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.85it/s] 88%|████████▊ | 30/34 [00:11<00:00,  4.84it/s] 91%|█████████ | 31/34 [00:11<00:00,  4.81it/s] 94%|█████████▍| 32/34 [00:11<00:00,  4.83it/s] 97%|█████████▋| 33/34 [00:11<00:00,  4.77it/s]100%|██████████| 34/34 [00:11<00:00,  5.54it/s]100%|██████████| 34/34 [00:12<00:00,  2.79it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967396736145
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:34,  4.67s/it]  6%|▌         | 2/34 [00:05<01:19,  2.47s/it]  9%|▉         | 3/34 [00:05<00:44,  1.44s/it] 12%|█▏        | 4/34 [00:06<00:28,  1.05it/s] 15%|█▍        | 5/34 [00:06<00:19,  1.46it/s] 18%|█▊        | 6/34 [00:06<00:14,  1.91it/s] 21%|██        | 7/34 [00:06<00:11,  2.37it/s] 24%|██▎       | 8/34 [00:06<00:09,  2.84it/s] 26%|██▋       | 9/34 [00:07<00:07,  3.24it/s] 29%|██▉       | 10/34 [00:07<00:06,  3.60it/s] 32%|███▏      | 11/34 [00:07<00:05,  3.89it/s] 35%|███▌      | 12/34 [00:07<00:05,  4.10it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.30it/s] 41%|████      | 14/34 [00:08<00:04,  4.46it/s] 44%|████▍     | 15/34 [00:08<00:04,  4.57it/s] 47%|████▋     | 16/34 [00:08<00:03,  4.66it/s] 50%|█████     | 17/34 [00:08<00:03,  4.72it/s] 53%|█████▎    | 18/34 [00:08<00:03,  4.75it/s] 56%|█████▌    | 19/34 [00:09<00:03,  4.78it/s] 59%|█████▉    | 20/34 [00:09<00:02,  4.81it/s] 62%|██████▏   | 21/34 [00:09<00:02,  4.82it/s] 65%|██████▍   | 22/34 [00:09<00:02,  4.77it/s] 68%|██████▊   | 23/34 [00:09<00:02,  4.78it/s] 71%|███████   | 24/34 [00:10<00:02,  4.80it/s] 74%|███████▎  | 25/34 [00:10<00:01,  4.79it/s] 76%|███████▋  | 26/34 [00:10<00:01,  4.81it/s] 79%|███████▉  | 27/34 [00:10<00:01,  4.83it/s] 82%|████████▏ | 28/34 [00:11<00:01,  4.81it/s] 85%|████████▌ | 29/34 [00:11<00:01,  4.82it/s] 88%|████████▊ | 30/34 [00:11<00:00,  4.83it/s] 91%|█████████ | 31/34 [00:11<00:00,  4.81it/s] 94%|█████████▍| 32/34 [00:11<00:00,  4.82it/s] 97%|█████████▋| 33/34 [00:12<00:00,  4.83it/s]100%|██████████| 34/34 [00:12<00:00,  5.64it/s]100%|██████████| 34/34 [00:12<00:00,  2.74it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6165563464164734


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36967468261719
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 42.62237762237762
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.63640443423903
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.423362471627332


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.655635833740234
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.04s/it]2it [00:02,  1.08s/it]3it [00:03,  1.13s/it]4it [00:04,  1.05s/it]5it [00:05,  1.02s/it]6it [00:06,  1.03s/it]7it [00:07,  1.01it/s]8it [00:07,  1.15it/s]9it [00:08,  1.13it/s]10it [00:09,  1.12it/s]11it [00:11,  1.09s/it]12it [00:12,  1.05s/it]13it [00:13,  1.01s/it]14it [00:13,  1.01it/s]15it [00:14,  1.01it/s]16it [00:16,  1.00s/it]17it [00:17,  1.01s/it]18it [00:18,  1.00it/s]19it [00:18,  1.02it/s]20it [00:20,  1.01s/it]21it [00:20,  1.01it/s]22it [00:21,  1.04it/s]23it [00:22,  1.05it/s]24it [00:23,  1.08it/s]25it [00:24,  1.06it/s]26it [00:25,  1.07it/s]27it [00:26,  1.00it/s]28it [00:27,  1.04s/it]29it [00:28,  1.05s/it]30it [00:29,  1.02s/it]31it [00:30,  1.00it/s]32it [00:31,  1.01s/it]33it [00:32,  1.03it/s]34it [00:33,  1.06it/s]35it [00:35,  1.09s/it]36it [00:36,  1.13s/it]37it [00:37,  1.14s/it]38it [00:38,  1.15s/it]39it [00:39,  1.15s/it]40it [00:40,  1.07s/it]41it [00:41,  1.01s/it]42it [00:42,  1.03s/it]43it [00:43,  1.05it/s]44it [00:44,  1.09it/s]45it [00:45,  1.13it/s]46it [00:45,  1.12it/s]47it [00:46,  1.16it/s]48it [00:47,  1.15it/s]49it [00:48,  1.07it/s]50it [00:49,  1.04it/s]51it [00:50,  1.01s/it]52it [00:51,  1.03it/s]53it [00:52,  1.05it/s]54it [00:53,  1.00s/it]55it [00:54,  1.02it/s]56it [00:55,  1.00it/s]57it [00:57,  1.21s/it]58it [00:59,  1.33s/it]59it [01:00,  1.41s/it]60it [01:02,  1.43s/it]61it [01:03,  1.46s/it]62it [01:05,  1.49s/it]63it [01:06,  1.55s/it]64it [01:08,  1.55s/it]65it [01:10,  1.56s/it]66it [01:11,  1.52s/it]67it [01:12,  1.53s/it]68it [01:14,  1.49s/it]69it [01:15,  1.48s/it]70it [01:17,  1.49s/it]71it [01:18,  1.46s/it]72it [01:20,  1.51s/it]73it [01:21,  1.50s/it]74it [01:23,  1.50s/it]75it [01:24,  1.52s/it]76it [01:26,  1.52s/it]77it [01:28,  1.55s/it]78it [01:29,  1.56s/it]79it [01:31,  1.51s/it]80it [01:32,  1.52s/it]81it [01:34,  1.54s/it]82it [01:35,  1.53s/it]83it [01:37,  1.53s/it]84it [01:39,  1.66s/it]85it [01:41,  1.96s/it]86it [01:43,  1.91s/it]87it [01:45,  1.86s/it]88it [01:47,  1.88s/it]89it [01:49,  1.87s/it]90it [01:51,  1.93s/it]91it [01:52,  1.85s/it]92it [01:54,  1.77s/it]93it [01:56,  1.76s/it]94it [01:57,  1.73s/it]95it [01:59,  1.68s/it]96it [02:00,  1.51s/it]97it [02:02,  1.55s/it]98it [02:03,  1.57s/it]99it [02:06,  1.92s/it]100it [02:08,  1.81s/it]101it [02:09,  1.79s/it]102it [02:11,  1.74s/it]103it [02:13,  1.70s/it]104it [02:14,  1.65s/it]105it [02:16,  1.66s/it]106it [02:17,  1.66s/it]107it [02:19,  1.63s/it]108it [02:21,  1.64s/it]109it [02:22,  1.63s/it]110it [02:24,  1.67s/it]111it [02:25,  1.58s/it]112it [02:27,  1.59s/it]113it [02:29,  1.59s/it]114it [02:30,  1.60s/it]115it [02:32,  1.60s/it]116it [02:34,  1.64s/it]117it [02:35,  1.56s/it]118it [02:36,  1.32s/it]119it [02:37,  1.23s/it]120it [02:38,  1.18s/it]121it [02:39,  1.13s/it]122it [02:40,  1.10s/it]123it [02:41,  1.06s/it]124it [02:42,  1.06s/it]125it [02:43,  1.09s/it]126it [02:44,  1.10s/it]127it [02:45,  1.08s/it]128it [02:46,  1.07s/it]129it [02:47,  1.03s/it]130it [02:48,  1.09s/it]131it [02:49,  1.03s/it]132it [02:50,  1.03s/it]133it [02:52,  1.25s/it]134it [02:54,  1.46s/it]135it [02:56,  1.58s/it]136it [02:58,  1.62s/it]137it [02:59,  1.63s/it]138it [03:01,  1.58s/it]139it [03:02,  1.53s/it]140it [03:03,  1.45s/it]141it [03:04,  1.27s/it]142it [03:05,  1.02it/s]143it [03:05,  1.09it/s]144it [03:07,  1.02s/it]145it [03:08,  1.13s/it]146it [03:09,  1.22s/it]147it [03:11,  1.20s/it]148it [03:11,  1.13s/it]149it [03:13,  1.22s/it]150it [03:14,  1.23s/it]151it [03:15,  1.26s/it]152it [03:17,  1.32s/it]153it [03:18,  1.29s/it]154it [03:19,  1.27s/it]155it [03:20,  1.16s/it]156it [03:21,  1.16s/it]157it [03:22,  1.11s/it]158it [03:23,  1.06s/it]159it [03:24,  1.01s/it]160it [03:25,  1.00s/it]161it [03:27,  1.08s/it]162it [03:28,  1.25s/it]163it [03:30,  1.36s/it]164it [03:32,  1.52s/it]165it [03:33,  1.43s/it]166it [03:35,  1.52s/it]167it [03:36,  1.61s/it]168it [03:38,  1.66s/it]169it [03:40,  1.65s/it]170it [03:42,  1.69s/it]171it [03:43,  1.67s/it]172it [03:45,  1.64s/it]173it [03:47,  1.72s/it]174it [03:49,  1.91s/it]175it [03:51,  1.87s/it]176it [03:53,  1.87s/it]177it [03:55,  1.84s/it]178it [03:56,  1.77s/it]179it [03:58,  1.70s/it]180it [03:59,  1.73s/it]181it [04:01,  1.64s/it]182it [04:03,  1.65s/it]183it [04:04,  1.71s/it]184it [04:06,  1.72s/it]185it [04:08,  1.72s/it]186it [04:10,  1.73s/it]187it [04:12,  1.85s/it]188it [04:13,  1.80s/it]189it [04:15,  1.76s/it]190it [04:17,  1.72s/it]191it [04:18,  1.64s/it]192it [04:20,  1.61s/it]193it [04:21,  1.61s/it]194it [04:23,  1.60s/it]195it [04:24,  1.58s/it]196it [04:26,  1.48s/it]197it [04:27,  1.38s/it]198it [04:28,  1.37s/it]199it [04:30,  1.40s/it]200it [04:31,  1.50s/it]201it [04:33,  1.60s/it]202it [04:35,  1.65s/it]203it [04:36,  1.48s/it]204it [04:37,  1.41s/it]205it [04:39,  1.38s/it]206it [04:40,  1.32s/it]207it [04:41,  1.33s/it]208it [04:43,  1.45s/it]209it [04:45,  1.67s/it]210it [04:47,  1.80s/it]211it [04:49,  1.88s/it]212it [04:51,  1.84s/it]213it [04:53,  1.86s/it]214it [04:54,  1.70s/it]215it [04:55,  1.57s/it]216it [04:57,  1.50s/it]217it [04:58,  1.47s/it]218it [04:59,  1.37s/it]219it [05:00,  1.30s/it]220it [05:01,  1.17s/it]221it [05:02,  1.13s/it]222it [05:04,  1.12s/it]223it [05:05,  1.11s/it]224it [05:06,  1.14s/it]225it [05:07,  1.27s/it]226it [05:09,  1.25s/it]227it [05:10,  1.26s/it]228it [05:11,  1.27s/it]229it [05:12,  1.21s/it]230it [05:13,  1.17s/it]231it [05:14,  1.13s/it]232it [05:16,  1.15s/it]233it [05:17,  1.17s/it]234it [05:18,  1.15s/it]235it [05:20,  1.36s/it]236it [05:21,  1.28s/it]237it [05:22,  1.20s/it]238it [05:23,  1.21s/it]239it [05:24,  1.16s/it]240it [05:25,  1.11s/it]241it [05:26,  1.11s/it]242it [05:27,  1.11s/it]243it [05:29,  1.18s/it]244it [05:30,  1.16s/it]245it [05:31,  1.14s/it]246it [05:32,  1.13s/it]247it [05:33,  1.13s/it]248it [05:34,  1.12s/it]249it [05:35,  1.11s/it]250it [05:36,  1.08s/it]251it [05:37,  1.07s/it]252it [05:38,  1.03s/it]253it [05:39,  1.04s/it]254it [05:40,  1.01s/it]255it [05:41,  1.00it/s]256it [05:42,  1.02s/it]257it [05:43,  1.03s/it]258it [05:44,  1.05s/it]259it [05:46,  1.20s/it]260it [05:47,  1.15s/it]261it [05:48,  1.11s/it]262it [05:49,  1.10s/it]263it [05:50,  1.12s/it]264it [05:51,  1.13s/it]265it [05:53,  1.16s/it]266it [05:54,  1.12s/it]267it [05:55,  1.14s/it]268it [05:56,  1.14s/it]269it [05:57,  1.05s/it]270it [05:57,  1.27it/s]271it [05:57,  1.65it/s]272it [05:58,  1.99it/s]273it [05:58,  2.41it/s]274it [05:58,  2.89it/s]274it [05:58,  1.31s/it]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:12,  4.01s/it]  6%|▌         | 2/34 [00:04<01:07,  2.12s/it]  9%|▉         | 3/34 [00:05<00:38,  1.25s/it] 12%|█▏        | 4/34 [00:05<00:25,  1.20it/s] 15%|█▍        | 5/34 [00:05<00:17,  1.64it/s] 18%|█▊        | 6/34 [00:05<00:13,  2.12it/s] 21%|██        | 7/34 [00:05<00:10,  2.61it/s] 24%|██▎       | 8/34 [00:06<00:08,  3.06it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.47it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.81it/s] 32%|███▏      | 11/34 [00:06<00:05,  4.09it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.31it/s] 38%|███▊      | 13/34 [00:07<00:04,  4.47it/s] 41%|████      | 14/34 [00:07<00:04,  4.59it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.68it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.73it/s] 50%|█████     | 17/34 [00:07<00:03,  4.78it/s] 53%|█████▎    | 18/34 [00:08<00:03,  4.81it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.83it/s] 59%|█████▉    | 20/34 [00:08<00:02,  4.85it/s] 62%|██████▏   | 21/34 [00:08<00:02,  4.86it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.87it/s] 68%|██████▊   | 23/34 [00:09<00:02,  4.88it/s] 71%|███████   | 24/34 [00:09<00:02,  4.88it/s] 74%|███████▎  | 25/34 [00:09<00:01,  4.88it/s] 76%|███████▋  | 26/34 [00:09<00:01,  4.88it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.88it/s] 82%|████████▏ | 28/34 [00:10<00:01,  4.89it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.88it/s] 88%|████████▊ | 30/34 [00:10<00:00,  4.88it/s] 91%|█████████ | 31/34 [00:10<00:00,  4.88it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.89it/s] 97%|█████████▋| 33/34 [00:11<00:00,  4.88it/s]100%|██████████| 34/34 [00:11<00:00,  5.66it/s]100%|██████████| 34/34 [00:11<00:00,  2.97it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967992782593
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:12,  4.02s/it]  6%|▌         | 2/34 [00:04<00:56,  1.78s/it]  9%|▉         | 3/34 [00:04<00:32,  1.06s/it] 12%|█▏        | 4/34 [00:04<00:21,  1.38it/s] 15%|█▍        | 5/34 [00:04<00:15,  1.86it/s] 18%|█▊        | 6/34 [00:05<00:11,  2.36it/s] 21%|██        | 7/34 [00:05<00:09,  2.84it/s] 24%|██▎       | 8/34 [00:05<00:07,  3.28it/s] 26%|██▋       | 9/34 [00:05<00:06,  3.65it/s] 29%|██▉       | 10/34 [00:05<00:06,  3.96it/s] 32%|███▏      | 11/34 [00:06<00:05,  4.20it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.38it/s] 38%|███▊      | 13/34 [00:06<00:04,  4.52it/s] 41%|████      | 14/34 [00:06<00:04,  4.62it/s] 44%|████▍     | 15/34 [00:06<00:04,  4.70it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.75it/s] 50%|█████     | 17/34 [00:07<00:03,  4.79it/s] 53%|█████▎    | 18/34 [00:07<00:03,  4.81it/s] 56%|█████▌    | 19/34 [00:07<00:03,  4.83it/s] 59%|█████▉    | 20/34 [00:07<00:02,  4.85it/s] 62%|██████▏   | 21/34 [00:08<00:02,  4.85it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.86it/s] 68%|██████▊   | 23/34 [00:08<00:02,  4.86it/s] 71%|███████   | 24/34 [00:08<00:02,  4.87it/s] 74%|███████▎  | 25/34 [00:08<00:01,  4.87it/s] 76%|███████▋  | 26/34 [00:09<00:01,  4.87it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.87it/s] 82%|████████▏ | 28/34 [00:09<00:01,  4.87it/s] 85%|████████▌ | 29/34 [00:09<00:01,  4.87it/s] 88%|████████▊ | 30/34 [00:09<00:00,  4.87it/s] 91%|█████████ | 31/34 [00:10<00:00,  4.87it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.87it/s] 97%|█████████▋| 33/34 [00:10<00:00,  4.87it/s]100%|██████████| 34/34 [00:10<00:00,  5.68it/s]100%|██████████| 34/34 [00:10<00:00,  3.12it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6143144965171814


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36968231201172
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 42.599067599067595
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.51508873445752
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.435048699363712


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.43144989013672
=========================          END          =========================
0it [00:00, ?it/s]1it [00:00,  1.95it/s]2it [00:00,  2.92it/s]3it [00:01,  2.80it/s]4it [00:01,  3.14it/s]5it [00:01,  3.81it/s]6it [00:01,  3.92it/s]7it [00:02,  3.50it/s]8it [00:02,  3.57it/s]9it [00:02,  3.95it/s]10it [00:02,  4.35it/s]11it [00:02,  4.32it/s]13it [00:03,  5.82it/s]14it [00:03,  5.32it/s]15it [00:03,  6.04it/s]16it [00:03,  5.64it/s]17it [00:03,  5.82it/s]18it [00:04,  6.54it/s]19it [00:04,  7.25it/s]20it [00:04,  5.66it/s]21it [00:04,  5.76it/s]22it [00:04,  5.35it/s]23it [00:04,  5.82it/s]24it [00:05,  5.74it/s]25it [00:05,  5.97it/s]26it [00:05,  6.49it/s]27it [00:05,  6.58it/s]28it [00:05,  5.66it/s]29it [00:05,  6.07it/s]30it [00:06,  4.76it/s]32it [00:06,  6.13it/s]33it [00:06,  6.34it/s]35it [00:06,  6.61it/s]36it [00:07,  6.40it/s]37it [00:07,  3.03it/s]38it [00:08,  3.15it/s]39it [00:08,  2.50it/s]40it [00:08,  2.89it/s]41it [00:09,  3.33it/s]42it [00:09,  3.92it/s]43it [00:09,  4.64it/s]45it [00:09,  4.99it/s]46it [00:09,  5.34it/s]47it [00:10,  4.52it/s]48it [00:10,  5.27it/s]49it [00:10,  5.59it/s]50it [00:10,  4.31it/s]51it [00:11,  4.70it/s]52it [00:11,  4.96it/s]53it [00:11,  5.44it/s]54it [00:11,  4.63it/s]55it [00:11,  4.12it/s]56it [00:12,  4.20it/s]57it [00:12,  2.84it/s]58it [00:12,  3.31it/s]59it [00:13,  3.71it/s]61it [00:13,  4.62it/s]62it [00:13,  4.80it/s]63it [00:14,  4.12it/s]65it [00:14,  4.26it/s]66it [00:14,  4.64it/s]67it [00:15,  3.63it/s]68it [00:15,  4.00it/s]69it [00:15,  3.99it/s]70it [00:15,  4.46it/s]71it [00:15,  5.20it/s]72it [00:15,  5.36it/s]73it [00:16,  5.70it/s]74it [00:16,  5.83it/s]75it [00:16,  5.64it/s]76it [00:16,  6.20it/s]78it [00:16,  7.85it/s]80it [00:17,  6.98it/s]81it [00:17,  6.23it/s]82it [00:17,  6.09it/s]83it [00:17,  5.71it/s]84it [00:17,  5.90it/s]85it [00:17,  6.08it/s]86it [00:18,  5.91it/s]88it [00:18,  6.68it/s]89it [00:18,  6.85it/s]90it [00:18,  5.48it/s]91it [00:19,  4.26it/s]92it [00:19,  3.42it/s]93it [00:19,  3.47it/s]94it [00:20,  3.96it/s]95it [00:20,  3.79it/s]96it [00:20,  3.78it/s]97it [00:20,  4.10it/s]98it [00:21,  4.03it/s]99it [00:21,  2.71it/s]100it [00:21,  3.24it/s]101it [00:22,  3.05it/s]102it [00:22,  3.28it/s]103it [00:22,  3.85it/s]104it [00:22,  4.39it/s]105it [00:22,  5.25it/s]106it [00:23,  5.64it/s]107it [00:23,  6.07it/s]108it [00:23,  5.93it/s]109it [00:23,  6.22it/s]110it [00:23,  6.02it/s]111it [00:23,  5.72it/s]112it [00:24,  5.72it/s]113it [00:24,  6.54it/s]115it [00:24,  7.15it/s]116it [00:24,  7.16it/s]118it [00:24,  8.17it/s]120it [00:25,  8.28it/s]121it [00:25,  7.94it/s]122it [00:25,  7.72it/s]123it [00:25,  7.92it/s]124it [00:25,  6.30it/s]125it [00:25,  6.46it/s]126it [00:25,  6.64it/s]127it [00:26,  6.33it/s]128it [00:26,  7.00it/s]129it [00:26,  7.54it/s]130it [00:26,  7.23it/s]131it [00:26,  6.41it/s]132it [00:26,  6.74it/s]133it [00:27,  5.64it/s]134it [00:27,  2.58it/s]136it [00:28,  2.94it/s]137it [00:28,  2.95it/s]138it [00:29,  2.52it/s]139it [00:29,  2.43it/s]140it [00:30,  2.88it/s]141it [00:30,  3.48it/s]142it [00:30,  3.76it/s]143it [00:30,  3.87it/s]145it [00:30,  5.05it/s]146it [00:31,  5.31it/s]147it [00:31,  5.66it/s]148it [00:31,  5.51it/s]149it [00:31,  4.28it/s]151it [00:32,  5.48it/s]153it [00:32,  5.87it/s]154it [00:32,  3.77it/s]155it [00:33,  3.58it/s]157it [00:33,  4.44it/s]158it [00:33,  4.91it/s]159it [00:33,  4.34it/s]160it [00:34,  4.20it/s]161it [00:34,  4.30it/s]162it [00:34,  4.42it/s]163it [00:34,  4.98it/s]164it [00:34,  5.42it/s]165it [00:35,  6.00it/s]166it [00:35,  5.72it/s]167it [00:35,  4.97it/s]168it [00:35,  4.75it/s]169it [00:36,  4.53it/s]170it [00:36,  4.55it/s]171it [00:36,  5.27it/s]172it [00:36,  5.67it/s]173it [00:36,  5.31it/s]174it [00:36,  5.81it/s]175it [00:37,  6.08it/s]176it [00:37,  6.11it/s]177it [00:37,  6.27it/s]178it [00:37,  6.17it/s]179it [00:37,  6.88it/s]180it [00:37,  6.81it/s]181it [00:37,  6.66it/s]183it [00:38,  7.99it/s]185it [00:38,  7.63it/s]186it [00:38,  7.80it/s]187it [00:38,  4.72it/s]188it [00:39,  5.22it/s]189it [00:39,  5.04it/s]190it [00:39,  5.74it/s]191it [00:39,  6.26it/s]192it [00:39,  6.47it/s]193it [00:39,  6.49it/s]194it [00:39,  6.78it/s]195it [00:40,  6.57it/s]196it [00:40,  4.24it/s]197it [00:40,  4.91it/s]198it [00:40,  5.75it/s]199it [00:41,  5.09it/s]200it [00:41,  5.17it/s]201it [00:41,  5.29it/s]203it [00:41,  6.95it/s]204it [00:41,  6.61it/s]205it [00:41,  6.39it/s]206it [00:42,  6.71it/s]207it [00:42,  5.73it/s]208it [00:42,  5.56it/s]209it [00:42,  6.16it/s]210it [00:42,  5.57it/s]211it [00:43,  5.07it/s]212it [00:43,  5.61it/s]213it [00:43,  5.88it/s]214it [00:43,  6.24it/s]215it [00:43,  5.70it/s]216it [00:43,  6.19it/s]217it [00:44,  5.49it/s]218it [00:44,  5.87it/s]219it [00:44,  5.41it/s]220it [00:44,  4.06it/s]221it [00:45,  3.39it/s]222it [00:46,  1.90it/s]223it [00:47,  1.18it/s]224it [00:49,  1.11s/it]225it [00:51,  1.35s/it]226it [00:51,  1.08s/it]227it [00:52,  1.17it/s]228it [00:53,  1.07it/s]229it [00:54,  1.03s/it]230it [00:56,  1.14s/it]231it [00:57,  1.11s/it]232it [00:58,  1.17s/it]233it [00:59,  1.20s/it]234it [01:00,  1.18s/it]235it [01:01,  1.02it/s]236it [01:01,  1.38it/s]237it [01:01,  1.83it/s]238it [01:01,  2.28it/s]239it [01:02,  2.69it/s]240it [01:02,  2.99it/s]241it [01:02,  3.53it/s]242it [01:02,  3.58it/s]243it [01:02,  4.01it/s]244it [01:03,  4.44it/s]245it [01:03,  5.20it/s]246it [01:03,  5.54it/s]247it [01:03,  5.13it/s]248it [01:03,  5.09it/s]249it [01:03,  4.76it/s]250it [01:04,  4.74it/s]251it [01:04,  4.87it/s]252it [01:04,  5.12it/s]253it [01:04,  5.01it/s]254it [01:04,  5.35it/s]255it [01:05,  6.10it/s]256it [01:05,  5.32it/s]257it [01:05,  5.32it/s]258it [01:05,  5.41it/s]259it [01:06,  2.74it/s]260it [01:06,  3.10it/s]261it [01:07,  2.93it/s]262it [01:07,  3.04it/s]263it [01:07,  3.08it/s]264it [01:08,  2.93it/s]265it [01:08,  2.84it/s]266it [01:08,  2.95it/s]267it [01:09,  2.71it/s]268it [01:09,  3.01it/s]269it [01:09,  3.40it/s]270it [01:09,  3.21it/s]271it [01:10,  3.30it/s]272it [01:10,  3.55it/s]273it [01:10,  3.52it/s]274it [01:10,  4.04it/s]274it [01:10,  3.86it/s]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:24,  4.38s/it]  6%|▌         | 2/34 [00:04<01:01,  1.92s/it]  9%|▉         | 3/34 [00:04<00:35,  1.14s/it] 12%|█▏        | 4/34 [00:04<00:23,  1.30it/s] 15%|█▍        | 5/34 [00:05<00:16,  1.76it/s] 18%|█▊        | 6/34 [00:05<00:12,  2.25it/s] 21%|██        | 7/34 [00:05<00:09,  2.73it/s] 24%|██▎       | 8/34 [00:05<00:08,  3.18it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.57it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.89it/s] 32%|███▏      | 11/34 [00:06<00:05,  4.15it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.35it/s] 38%|███▊      | 13/34 [00:06<00:04,  4.50it/s] 41%|████      | 14/34 [00:07<00:04,  4.61it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.69it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.75it/s] 50%|█████     | 17/34 [00:07<00:03,  4.78it/s] 53%|█████▎    | 18/34 [00:07<00:03,  4.81it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.83it/s] 59%|█████▉    | 20/34 [00:08<00:02,  4.84it/s] 62%|██████▏   | 21/34 [00:08<00:02,  4.83it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.84it/s] 68%|██████▊   | 23/34 [00:08<00:02,  4.85it/s] 71%|███████   | 24/34 [00:09<00:02,  4.85it/s] 74%|███████▎  | 25/34 [00:09<00:01,  4.86it/s] 76%|███████▋  | 26/34 [00:09<00:01,  4.86it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.86it/s] 82%|████████▏ | 28/34 [00:09<00:01,  4.86it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.87it/s] 88%|████████▊ | 30/34 [00:10<00:00,  4.87it/s] 91%|█████████ | 31/34 [00:10<00:00,  4.87it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.87it/s] 97%|█████████▋| 33/34 [00:10<00:00,  4.87it/s]100%|██████████| 34/34 [00:11<00:00,  5.66it/s]100%|██████████| 34/34 [00:11<00:00,  3.02it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967396736145
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:04<02:22,  4.31s/it]  6%|▌         | 2/34 [00:04<01:00,  1.89s/it]  9%|▉         | 3/34 [00:04<00:34,  1.12s/it] 12%|█▏        | 4/34 [00:04<00:22,  1.32it/s] 15%|█▍        | 5/34 [00:05<00:16,  1.78it/s] 18%|█▊        | 6/34 [00:05<00:12,  2.27it/s] 21%|██        | 7/34 [00:05<00:09,  2.75it/s] 24%|██▎       | 8/34 [00:05<00:08,  3.19it/s] 26%|██▋       | 9/34 [00:05<00:06,  3.58it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.89it/s] 32%|███▏      | 11/34 [00:06<00:05,  4.15it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.30it/s] 38%|███▊      | 13/34 [00:06<00:04,  4.44it/s] 41%|████      | 14/34 [00:06<00:04,  4.54it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.63it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.70it/s] 50%|█████     | 17/34 [00:07<00:03,  4.75it/s] 53%|█████▎    | 18/34 [00:07<00:03,  4.78it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.79it/s] 59%|█████▉    | 20/34 [00:08<00:02,  4.81it/s] 62%|██████▏   | 21/34 [00:08<00:02,  4.82it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.83it/s] 68%|██████▊   | 23/34 [00:08<00:02,  4.84it/s] 71%|███████   | 24/34 [00:09<00:02,  4.84it/s] 74%|███████▎  | 25/34 [00:09<00:01,  4.85it/s] 76%|███████▋  | 26/34 [00:09<00:01,  4.85it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.86it/s] 82%|████████▏ | 28/34 [00:09<00:01,  4.86it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.86it/s] 88%|████████▊ | 30/34 [00:10<00:00,  4.86it/s] 91%|█████████ | 31/34 [00:10<00:00,  4.85it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.86it/s] 97%|█████████▋| 33/34 [00:10<00:00,  4.85it/s]100%|██████████| 34/34 [00:11<00:00,  5.66it/s]100%|██████████| 34/34 [00:11<00:00,  3.03it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6119526028633118


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36967468261719
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 42.925407925407924
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.65595626049102
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.387912374628097


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.19525909423828
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.03s/it]2it [00:02,  1.01s/it]3it [00:02,  1.07it/s]4it [00:03,  1.00it/s]5it [00:05,  1.29s/it]6it [00:06,  1.26s/it]7it [00:08,  1.19s/it]8it [00:09,  1.13s/it]9it [00:10,  1.09s/it]10it [00:10,  1.03s/it]11it [00:11,  1.04s/it]12it [00:12,  1.03s/it]13it [00:13,  1.00s/it]14it [00:14,  1.04it/s]15it [00:15,  1.04it/s]16it [00:16,  1.02it/s]17it [00:17,  1.03it/s]18it [00:18,  1.02s/it]19it [00:20,  1.06s/it]20it [00:20,  1.03s/it]21it [00:21,  1.00it/s]22it [00:22,  1.05it/s]23it [00:23,  1.01it/s]24it [00:24,  1.02s/it]25it [00:25,  1.02s/it]26it [00:26,  1.01it/s]27it [00:27,  1.02s/it]28it [00:28,  1.03it/s]29it [00:29,  1.05it/s]30it [00:30,  1.04it/s]31it [00:31,  1.09it/s]32it [00:32,  1.10it/s]33it [00:33,  1.15it/s]34it [00:34,  1.15it/s]35it [00:34,  1.13it/s]36it [00:35,  1.10it/s]37it [00:36,  1.13it/s]38it [00:37,  1.12it/s]39it [00:38,  1.11it/s]40it [00:39,  1.10it/s]41it [00:40,  1.10it/s]42it [00:41,  1.05it/s]43it [00:42,  1.14it/s]44it [00:43,  1.09it/s]45it [00:44,  1.06it/s]46it [00:45,  1.06it/s]47it [00:46,  1.05it/s]48it [00:46,  1.15it/s]49it [00:47,  1.08it/s]50it [00:48,  1.06it/s]51it [00:49,  1.06it/s]52it [00:50,  1.09it/s]53it [00:51,  1.13it/s]54it [00:52,  1.10it/s]55it [00:53,  1.04it/s]56it [00:54,  1.02s/it]57it [00:55,  1.03it/s]58it [00:56,  1.03it/s]59it [00:57,  1.10it/s]60it [00:58,  1.11it/s]61it [00:59,  1.12it/s]62it [00:59,  1.14it/s]63it [01:00,  1.17it/s]64it [01:01,  1.10it/s]65it [01:02,  1.11it/s]66it [01:03,  1.15it/s]67it [01:03,  1.36it/s]68it [01:03,  1.82it/s]69it [01:04,  2.35it/s]70it [01:04,  3.01it/s]71it [01:04,  3.65it/s]72it [01:04,  4.10it/s]73it [01:04,  4.57it/s]74it [01:04,  5.13it/s]75it [01:04,  5.70it/s]76it [01:05,  5.83it/s]77it [01:05,  5.61it/s]79it [01:05,  6.94it/s]80it [01:05,  6.80it/s]81it [01:05,  7.17it/s]83it [01:05,  7.67it/s]84it [01:06,  7.43it/s]85it [01:06,  6.95it/s]86it [01:06,  7.14it/s]87it [01:06,  7.11it/s]88it [01:06,  7.54it/s]89it [01:06,  7.45it/s]90it [01:06,  8.02it/s]92it [01:07,  7.92it/s]93it [01:07,  7.37it/s]94it [01:07,  7.61it/s]95it [01:07,  7.21it/s]96it [01:07,  6.77it/s]97it [01:08,  6.03it/s]98it [01:08,  6.25it/s]99it [01:08,  3.53it/s]100it [01:08,  3.85it/s]101it [01:09,  4.34it/s]102it [01:09,  4.14it/s]103it [01:09,  4.63it/s]104it [01:09,  4.44it/s]105it [01:09,  4.82it/s]106it [01:10,  4.59it/s]107it [01:10,  5.19it/s]108it [01:10,  5.92it/s]109it [01:10,  6.52it/s]110it [01:10,  6.01it/s]111it [01:10,  6.71it/s]113it [01:11,  5.85it/s]114it [01:11,  5.24it/s]115it [01:11,  5.39it/s]116it [01:11,  4.47it/s]117it [01:12,  4.69it/s]118it [01:12,  5.06it/s]119it [01:12,  4.93it/s]120it [01:13,  3.63it/s]121it [01:13,  3.78it/s]122it [01:13,  3.43it/s]123it [01:13,  4.17it/s]124it [01:13,  4.02it/s]126it [01:14,  4.59it/s]127it [01:14,  4.78it/s]129it [01:14,  6.10it/s]131it [01:14,  6.81it/s]133it [01:15,  7.45it/s]134it [01:15,  7.70it/s]135it [01:15,  7.37it/s]136it [01:15,  7.09it/s]137it [01:15,  7.29it/s]138it [01:16,  5.57it/s]139it [01:16,  5.22it/s]140it [01:16,  5.71it/s]142it [01:16,  7.09it/s]143it [01:16,  7.55it/s]144it [01:16,  7.44it/s]145it [01:16,  7.64it/s]147it [01:17,  8.54it/s]148it [01:17,  8.68it/s]149it [01:17,  8.23it/s]151it [01:17,  9.34it/s]152it [01:17,  9.12it/s]153it [01:17,  9.23it/s]154it [01:18,  5.69it/s]155it [01:18,  6.15it/s]156it [01:18,  6.43it/s]157it [01:18,  5.71it/s]159it [01:18,  6.65it/s]160it [01:19,  5.97it/s]161it [01:19,  6.19it/s]162it [01:19,  4.03it/s]163it [01:20,  2.80it/s]164it [01:20,  2.69it/s]165it [01:21,  2.25it/s]166it [01:21,  2.77it/s]167it [01:21,  3.45it/s]168it [01:21,  4.03it/s]169it [01:22,  4.40it/s]170it [01:22,  5.18it/s]171it [01:22,  5.61it/s]172it [01:22,  6.13it/s]173it [01:22,  5.91it/s]174it [01:22,  5.83it/s]175it [01:22,  6.10it/s]176it [01:23,  5.66it/s]177it [01:23,  5.67it/s]178it [01:23,  6.09it/s]179it [01:23,  6.04it/s]180it [01:23,  6.05it/s]181it [01:23,  5.82it/s]182it [01:24,  4.99it/s]183it [01:24,  5.79it/s]184it [01:24,  6.16it/s]185it [01:24,  5.11it/s]186it [01:25,  4.50it/s]187it [01:25,  3.25it/s]189it [01:25,  4.70it/s]191it [01:25,  6.38it/s]193it [01:26,  7.97it/s]195it [01:26,  9.49it/s]197it [01:26,  9.23it/s]199it [01:26, 10.36it/s]201it [01:26, 11.12it/s]203it [01:26, 11.66it/s]205it [01:26, 12.19it/s]207it [01:27, 13.06it/s]209it [01:27, 13.38it/s]211it [01:27, 12.51it/s]213it [01:27, 12.81it/s]215it [01:27, 13.60it/s]217it [01:27, 13.80it/s]219it [01:27, 13.54it/s]221it [01:28, 13.69it/s]223it [01:28, 14.62it/s]225it [01:28, 15.11it/s]227it [01:28, 15.90it/s]229it [01:28, 16.07it/s]231it [01:28, 16.77it/s]233it [01:28, 17.27it/s]235it [01:29, 10.03it/s]237it [01:29, 11.47it/s]239it [01:29, 12.33it/s]241it [01:29, 13.28it/s]243it [01:29, 13.59it/s]245it [01:29, 13.24it/s]247it [01:30, 13.52it/s]249it [01:30, 13.63it/s]251it [01:30, 13.55it/s]253it [01:30, 14.34it/s]255it [01:30, 14.97it/s]257it [01:30, 15.86it/s]259it [01:31, 10.18it/s]261it [01:31, 11.22it/s]263it [01:31, 11.84it/s]265it [01:31, 12.47it/s]267it [01:31, 11.32it/s]269it [01:31, 12.45it/s]271it [01:31, 13.55it/s]273it [01:32, 14.58it/s]274it [01:32,  2.98it/s]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:03<01:55,  3.49s/it]  6%|▌         | 2/34 [00:04<01:06,  2.08s/it]  9%|▉         | 3/34 [00:04<00:38,  1.23s/it] 12%|█▏        | 4/34 [00:05<00:24,  1.21it/s] 15%|█▍        | 5/34 [00:05<00:17,  1.66it/s] 18%|█▊        | 6/34 [00:05<00:13,  2.14it/s] 21%|██        | 7/34 [00:05<00:10,  2.62it/s] 24%|██▎       | 8/34 [00:05<00:08,  3.07it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.47it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.81it/s] 32%|███▏      | 11/34 [00:06<00:05,  4.09it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.29it/s] 38%|███▊      | 13/34 [00:06<00:04,  4.45it/s] 41%|████      | 14/34 [00:07<00:04,  4.57it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.66it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.72it/s] 50%|█████     | 17/34 [00:07<00:03,  4.77it/s] 53%|█████▎    | 18/34 [00:07<00:03,  4.80it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.82it/s] 59%|█████▉    | 20/34 [00:08<00:02,  4.83it/s] 62%|██████▏   | 21/34 [00:08<00:02,  4.85it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.86it/s] 68%|██████▊   | 23/34 [00:08<00:02,  4.86it/s] 71%|███████   | 24/34 [00:09<00:02,  4.86it/s] 74%|███████▎  | 25/34 [00:09<00:01,  4.86it/s] 76%|███████▋  | 26/34 [00:09<00:01,  4.86it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.86it/s] 82%|████████▏ | 28/34 [00:09<00:01,  4.86it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.87it/s] 88%|████████▊ | 30/34 [00:10<00:00,  4.87it/s] 91%|█████████ | 31/34 [00:10<00:00,  4.87it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.87it/s] 97%|█████████▋| 33/34 [00:10<00:00,  4.88it/s]100%|██████████| 34/34 [00:11<00:00,  5.66it/s]100%|██████████| 34/34 [00:11<00:00,  3.02it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967396736145
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:03<02:08,  3.89s/it]  6%|▌         | 2/34 [00:04<01:05,  2.06s/it]  9%|▉         | 3/34 [00:04<00:37,  1.21s/it] 12%|█▏        | 4/34 [00:05<00:24,  1.23it/s] 15%|█▍        | 5/34 [00:05<00:17,  1.68it/s] 18%|█▊        | 6/34 [00:05<00:12,  2.16it/s] 21%|██        | 7/34 [00:05<00:10,  2.64it/s] 24%|██▎       | 8/34 [00:05<00:08,  3.09it/s] 26%|██▋       | 9/34 [00:06<00:07,  3.48it/s] 29%|██▉       | 10/34 [00:06<00:06,  3.82it/s] 32%|███▏      | 11/34 [00:06<00:05,  4.09it/s] 35%|███▌      | 12/34 [00:06<00:05,  4.30it/s] 38%|███▊      | 13/34 [00:06<00:04,  4.46it/s] 41%|████      | 14/34 [00:07<00:04,  4.57it/s] 44%|████▍     | 15/34 [00:07<00:04,  4.66it/s] 47%|████▋     | 16/34 [00:07<00:03,  4.73it/s] 50%|█████     | 17/34 [00:07<00:03,  4.77it/s] 53%|█████▎    | 18/34 [00:07<00:03,  4.80it/s] 56%|█████▌    | 19/34 [00:08<00:03,  4.82it/s] 59%|█████▉    | 20/34 [00:08<00:02,  4.84it/s] 62%|██████▏   | 21/34 [00:08<00:02,  4.83it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.84it/s] 68%|██████▊   | 23/34 [00:08<00:02,  4.83it/s] 71%|███████   | 24/34 [00:09<00:02,  4.84it/s] 74%|███████▎  | 25/34 [00:09<00:01,  4.84it/s] 76%|███████▋  | 26/34 [00:09<00:01,  4.84it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.85it/s] 82%|████████▏ | 28/34 [00:10<00:01,  4.86it/s] 85%|████████▌ | 29/34 [00:10<00:01,  4.86it/s] 88%|████████▊ | 30/34 [00:10<00:00,  4.86it/s] 91%|█████████ | 31/34 [00:10<00:00,  4.86it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.86it/s] 97%|█████████▋| 33/34 [00:11<00:00,  4.85it/s]100%|██████████| 34/34 [00:11<00:00,  5.65it/s]100%|██████████| 34/34 [00:11<00:00,  3.00it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6143482327461243


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36967468261719
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 42.61072261072261
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.51164688036236
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.574560145410025


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.43482208251953
=========================          END          =========================
0it [00:00, ?it/s]1it [00:00,  2.95it/s]2it [00:00,  2.91it/s]3it [00:00,  3.89it/s]4it [00:01,  3.81it/s]5it [00:01,  4.34it/s]6it [00:02,  2.32it/s]7it [00:02,  2.05it/s]8it [00:03,  1.90it/s]9it [00:03,  1.88it/s]10it [00:04,  2.10it/s]11it [00:04,  2.65it/s]12it [00:04,  3.15it/s]13it [00:04,  2.98it/s]14it [00:05,  2.46it/s]15it [00:06,  2.28it/s]16it [00:06,  2.34it/s]17it [00:06,  2.14it/s]18it [00:07,  2.25it/s]19it [00:07,  2.57it/s]20it [00:07,  2.67it/s]21it [00:08,  3.17it/s]22it [00:08,  3.41it/s]23it [00:08,  3.91it/s]24it [00:08,  4.23it/s]25it [00:08,  4.64it/s]26it [00:09,  5.19it/s]27it [00:09,  5.25it/s]28it [00:09,  4.86it/s]29it [00:09,  5.20it/s]30it [00:09,  4.95it/s]31it [00:09,  5.68it/s]33it [00:10,  5.72it/s]34it [00:10,  5.69it/s]35it [00:10,  5.82it/s]36it [00:10,  6.23it/s]37it [00:11,  5.61it/s]38it [00:11,  5.02it/s]39it [00:11,  5.50it/s]40it [00:11,  4.76it/s]41it [00:12,  3.98it/s]42it [00:12,  4.04it/s]43it [00:12,  4.34it/s]44it [00:12,  4.91it/s]45it [00:12,  4.31it/s]46it [00:13,  4.78it/s]47it [00:13,  5.02it/s]48it [00:13,  5.34it/s]49it [00:13,  5.06it/s]50it [00:13,  4.93it/s]51it [00:14,  4.64it/s]52it [00:14,  5.15it/s]53it [00:14,  4.09it/s]54it [00:14,  4.25it/s]55it [00:14,  4.44it/s]56it [00:15,  4.34it/s]57it [00:15,  4.42it/s]58it [00:15,  5.07it/s]60it [00:15,  5.69it/s]61it [00:16,  5.59it/s]62it [00:16,  5.14it/s]63it [00:16,  4.75it/s]64it [00:16,  4.73it/s]65it [00:17,  4.52it/s]66it [00:17,  5.36it/s]67it [00:17,  4.00it/s]68it [00:17,  4.55it/s]69it [00:17,  4.92it/s]70it [00:18,  4.50it/s]71it [00:18,  3.99it/s]72it [00:18,  4.16it/s]73it [00:18,  4.65it/s]74it [00:18,  4.86it/s]75it [00:19,  4.71it/s]76it [00:19,  4.41it/s]77it [00:19,  3.72it/s]78it [00:20,  3.96it/s]79it [00:20,  3.76it/s]80it [00:20,  3.85it/s]81it [00:20,  4.33it/s]83it [00:21,  3.55it/s]84it [00:21,  3.02it/s]85it [00:22,  2.79it/s]86it [00:22,  2.74it/s]87it [00:23,  2.34it/s]88it [00:23,  2.05it/s]89it [00:24,  2.06it/s]90it [00:25,  1.87it/s]91it [00:25,  1.90it/s]92it [00:26,  1.89it/s]93it [00:26,  2.37it/s]94it [00:26,  2.84it/s]95it [00:27,  2.38it/s]96it [00:27,  2.87it/s]97it [00:27,  2.75it/s]98it [00:27,  3.19it/s]99it [00:28,  2.30it/s]100it [00:28,  2.77it/s]101it [00:28,  3.25it/s]102it [00:29,  3.93it/s]103it [00:29,  4.02it/s]104it [00:29,  4.71it/s]105it [00:29,  4.41it/s]106it [00:29,  5.26it/s]107it [00:29,  5.71it/s]108it [00:30,  4.19it/s]109it [00:30,  4.32it/s]110it [00:30,  4.09it/s]111it [00:31,  4.34it/s]112it [00:31,  4.38it/s]113it [00:31,  4.90it/s]114it [00:31,  4.87it/s]115it [00:31,  5.11it/s]116it [00:31,  5.95it/s]117it [00:32,  5.37it/s]118it [00:32,  4.99it/s]119it [00:32,  5.68it/s]120it [00:32,  5.56it/s]121it [00:32,  5.82it/s]122it [00:32,  5.96it/s]123it [00:33,  6.17it/s]124it [00:33,  5.42it/s]125it [00:33,  5.87it/s]126it [00:33,  6.15it/s]127it [00:33,  5.69it/s]128it [00:34,  5.50it/s]129it [00:34,  6.00it/s]130it [00:34,  5.74it/s]131it [00:34,  5.95it/s]132it [00:34,  4.78it/s]133it [00:35,  4.77it/s]134it [00:35,  5.02it/s]135it [00:35,  4.36it/s]136it [00:35,  5.11it/s]137it [00:35,  4.78it/s]138it [00:36,  4.30it/s]139it [00:36,  4.54it/s]140it [00:36,  4.83it/s]141it [00:36,  5.35it/s]142it [00:36,  4.92it/s]143it [00:37,  4.83it/s]144it [00:37,  5.28it/s]145it [00:37,  5.44it/s]146it [00:37,  4.48it/s]148it [00:38,  4.73it/s]149it [00:38,  5.03it/s]150it [00:38,  4.10it/s]151it [00:39,  3.41it/s]152it [00:39,  4.02it/s]153it [00:39,  4.77it/s]154it [00:39,  3.62it/s]155it [00:40,  3.28it/s]156it [00:40,  3.56it/s]157it [00:40,  4.05it/s]158it [00:40,  4.35it/s]159it [00:40,  4.38it/s]160it [00:41,  3.85it/s]161it [00:41,  4.06it/s]162it [00:41,  4.45it/s]163it [00:41,  5.08it/s]164it [00:42,  4.86it/s]165it [00:42,  4.83it/s]166it [00:42,  4.64it/s]167it [00:42,  5.40it/s]168it [00:42,  5.05it/s]169it [00:43,  5.10it/s]170it [00:43,  4.97it/s]171it [00:43,  5.04it/s]172it [00:43,  5.43it/s]173it [00:43,  5.47it/s]174it [00:43,  5.25it/s]175it [00:44,  5.37it/s]176it [00:44,  5.06it/s]177it [00:44,  4.52it/s]178it [00:44,  4.29it/s]179it [00:45,  4.51it/s]180it [00:45,  4.75it/s]181it [00:45,  5.16it/s]182it [00:45,  4.10it/s]183it [00:46,  3.02it/s]184it [00:46,  3.09it/s]185it [00:46,  3.37it/s]186it [00:47,  3.53it/s]187it [00:47,  2.35it/s]188it [00:48,  2.58it/s]189it [00:48,  2.46it/s]190it [00:48,  2.76it/s]191it [00:49,  2.96it/s]192it [00:49,  2.68it/s]193it [00:50,  2.25it/s]194it [00:50,  2.31it/s]195it [00:50,  2.56it/s]196it [00:51,  2.36it/s]197it [00:51,  2.37it/s]198it [00:51,  3.00it/s]199it [00:52,  2.97it/s]200it [00:52,  3.31it/s]201it [00:52,  3.48it/s]202it [00:53,  3.70it/s]203it [00:53,  3.08it/s]204it [00:53,  3.31it/s]205it [00:54,  2.89it/s]206it [00:54,  2.47it/s]207it [00:55,  2.21it/s]208it [00:55,  1.95it/s]209it [00:56,  1.92it/s]210it [00:57,  1.77it/s]211it [00:57,  1.73it/s]212it [00:58,  1.72it/s]213it [00:58,  2.01it/s]214it [00:58,  2.58it/s]215it [00:59,  2.30it/s]216it [00:59,  2.05it/s]217it [01:00,  2.10it/s]218it [01:00,  1.92it/s]219it [01:01,  2.17it/s]220it [01:01,  2.22it/s]221it [01:01,  2.57it/s]223it [01:02,  4.13it/s]224it [01:02,  4.80it/s]226it [01:02,  6.31it/s]228it [01:02,  7.69it/s]230it [01:02,  9.02it/s]232it [01:02,  9.92it/s]234it [01:03, 10.81it/s]236it [01:03,  6.76it/s]238it [01:03,  7.97it/s]240it [01:03,  9.05it/s]242it [01:04,  9.41it/s]244it [01:04, 11.00it/s]246it [01:04, 12.30it/s]248it [01:04, 13.28it/s]250it [01:04, 13.05it/s]252it [01:04, 12.92it/s]254it [01:04, 12.99it/s]256it [01:05, 13.90it/s]258it [01:05, 13.64it/s]260it [01:05,  8.65it/s]262it [01:05, 10.07it/s]264it [01:05, 10.37it/s]266it [01:06, 10.39it/s]268it [01:06,  9.52it/s]270it [01:06, 10.84it/s]272it [01:06, 12.03it/s]274it [01:06, 13.48it/s]274it [01:06,  4.11it/s]
Number of selected candidates = 108
---> Each Classifier' shapes
	 GT_classifier = 120
	 ViLang_guessed = 108
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:03<01:53,  3.44s/it]  6%|▌         | 2/34 [00:03<00:49,  1.54s/it]  9%|▉         | 3/34 [00:03<00:29,  1.06it/s] 12%|█▏        | 4/34 [00:04<00:19,  1.54it/s] 15%|█▍        | 5/34 [00:04<00:14,  2.04it/s] 18%|█▊        | 6/34 [00:04<00:11,  2.54it/s] 21%|██        | 7/34 [00:04<00:08,  3.01it/s] 24%|██▎       | 8/34 [00:04<00:07,  3.43it/s] 26%|██▋       | 9/34 [00:05<00:06,  3.78it/s] 29%|██▉       | 10/34 [00:05<00:05,  4.06it/s] 32%|███▏      | 11/34 [00:05<00:05,  4.28it/s] 35%|███▌      | 12/34 [00:05<00:04,  4.44it/s] 38%|███▊      | 13/34 [00:05<00:04,  4.56it/s] 41%|████      | 14/34 [00:06<00:04,  4.65it/s] 44%|████▍     | 15/34 [00:06<00:04,  4.72it/s] 47%|████▋     | 16/34 [00:06<00:03,  4.76it/s] 50%|█████     | 17/34 [00:06<00:03,  4.80it/s] 53%|█████▎    | 18/34 [00:06<00:03,  4.82it/s] 56%|█████▌    | 19/34 [00:07<00:03,  4.84it/s] 59%|█████▉    | 20/34 [00:07<00:02,  4.85it/s] 62%|██████▏   | 21/34 [00:07<00:02,  4.86it/s] 65%|██████▍   | 22/34 [00:07<00:02,  4.86it/s] 68%|██████▊   | 23/34 [00:07<00:02,  4.86it/s] 71%|███████   | 24/34 [00:08<00:02,  4.86it/s] 74%|███████▎  | 25/34 [00:08<00:01,  4.86it/s] 76%|███████▋  | 26/34 [00:08<00:01,  4.86it/s] 79%|███████▉  | 27/34 [00:08<00:01,  4.86it/s] 82%|████████▏ | 28/34 [00:09<00:01,  4.86it/s] 85%|████████▌ | 29/34 [00:09<00:01,  4.86it/s] 88%|████████▊ | 30/34 [00:09<00:00,  4.87it/s] 91%|█████████ | 31/34 [00:09<00:00,  4.87it/s] 94%|█████████▍| 32/34 [00:09<00:00,  4.86it/s] 97%|█████████▋| 33/34 [00:10<00:00,  4.86it/s]100%|██████████| 34/34 [00:10<00:00,  5.60it/s]100%|██████████| 34/34 [00:10<00:00,  3.29it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.7536967992782593
---> Evaluating
  0%|          | 0/34 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  3%|▎         | 1/34 [00:03<01:48,  3.28s/it]  6%|▌         | 2/34 [00:03<00:55,  1.73s/it]  9%|▉         | 3/34 [00:04<00:32,  1.04s/it] 12%|█▏        | 4/34 [00:04<00:21,  1.41it/s] 15%|█▍        | 5/34 [00:04<00:15,  1.90it/s] 18%|█▊        | 6/34 [00:04<00:11,  2.40it/s] 21%|██        | 7/34 [00:04<00:09,  2.87it/s] 24%|██▎       | 8/34 [00:05<00:07,  3.30it/s] 26%|██▋       | 9/34 [00:05<00:06,  3.67it/s] 29%|██▉       | 10/34 [00:05<00:06,  3.98it/s] 32%|███▏      | 11/34 [00:05<00:05,  4.22it/s] 35%|███▌      | 12/34 [00:05<00:05,  4.39it/s] 38%|███▊      | 13/34 [00:06<00:04,  4.52it/s] 41%|████      | 14/34 [00:06<00:04,  4.62it/s] 44%|████▍     | 15/34 [00:06<00:04,  4.69it/s] 47%|████▋     | 16/34 [00:06<00:03,  4.74it/s] 50%|█████     | 17/34 [00:07<00:03,  4.78it/s] 53%|█████▎    | 18/34 [00:07<00:03,  4.80it/s] 56%|█████▌    | 19/34 [00:07<00:03,  4.81it/s] 59%|█████▉    | 20/34 [00:07<00:02,  4.82it/s] 62%|██████▏   | 21/34 [00:07<00:02,  4.84it/s] 65%|██████▍   | 22/34 [00:08<00:02,  4.84it/s] 68%|██████▊   | 23/34 [00:08<00:02,  4.85it/s] 71%|███████   | 24/34 [00:08<00:02,  4.85it/s] 74%|███████▎  | 25/34 [00:08<00:01,  4.84it/s] 76%|███████▋  | 26/34 [00:08<00:01,  4.85it/s] 79%|███████▉  | 27/34 [00:09<00:01,  4.86it/s] 82%|████████▏ | 28/34 [00:09<00:01,  4.86it/s] 85%|████████▌ | 29/34 [00:09<00:01,  4.87it/s] 88%|████████▊ | 30/34 [00:09<00:00,  4.87it/s] 91%|█████████ | 31/34 [00:09<00:00,  4.87it/s] 94%|█████████▍| 32/34 [00:10<00:00,  4.87it/s] 97%|█████████▋| 33/34 [00:10<00:00,  4.87it/s]100%|██████████| 34/34 [00:10<00:00,  5.68it/s]100%|██████████| 34/34 [00:10<00:00,  3.21it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8580, 768])
gt_feats.shape torch.Size([8580, 768])
Semantic similarity score = 0.6146780252456665


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 56.841491841491845
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 72.58855407797449
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 41.80943733544862


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 75.36968231201172
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 42.599067599067595
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 66.35021985480938
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 30.963753800828137


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 61.467803955078125
=========================          END          =========================


========================= ViLang Final Results of 10 runs, w/ random imgs per class=========================


[Clustering]
Clustering ACC: 42.828671328671334
Semantic ACC:   61.46879959106445
=========================          END          =========================
