Namespace(config_file_env='./configs/env_machine.yml', config_file_expt='./configs/expts/flower102_all.yml', alpha=0.7, N_tta=10, num_per_category='random', num_runs=10)
Number of GPUs: 1
Device ID: 0 Device Name: NVIDIA RTX A6000
['Yellow Daffodils', 'White Petal Ring Flower', 'Water Droplet Rose', 'Pink Gradient Petal Flower', 'Violet Fern Lily', 'Purple Daisy', 'Purple Bellflower', 'White Rosette', 'Smooth-textured Green-leaf Orchid', 'White and Blue Bud Flower', 'Ringed Petal Pink Flower', 'Pink and White Striped Orchid', 'Orange Lily', 'Asymmetrical Ringed Petal Flower', 'Orange and Yellow Daisy', 'Passionflower', 'Striped Petal Orchid', 'Fern Leaf Pink Flower', 'Small Purple Dot Flower', 'Asymmetrical Ring-flower', 'Asymmetrical Fern-Like Petal Purple and White Flower', 'Thistle', 'Fern-like Petals', 'Blue Bellflower', 'Green Centered Orchid', 'Intensely Colored Small Flower', 'Small Blue Petal Flower', 'Small Center Sphereflower', 'Autumn Marigold', 'Symmetrical White Flower with Yellow Center', 'Ruffled Pink and White Flower', 'Lily-Leafed Purple Spotted Flower', 'Bouquet Lily', 'Blue Sphere Flower', 'Pink and White Gradient Flower', 'Rosette Leaf Hibiscus', 'Pink Carnation', 'Small fern leaf flower', 'White Calla Lily', 'Pink orchid', 'Daisy', 'Iris', 'Wavy Leafed White and Blue Flower', 'Daffodil', 'Purple Ring Flower', 'Clustered White Flower', 'White-Centered Star Flower', 'Phalaenopsis Orchid', 'Ring petal flower', 'Rosette Orchid', 'White Poppy with Yellow Center', 'Iris sunset', 'Lily-of-the-Nile', 'Dahlia', 'Spotted Lily Flower', 'Asymmetrical Iris', 'Bee Flower', 'Small Pink Petal Flower', 'Rosy Tube Flower', 'Fern-like Blue Flower', 'Pink Lily', 'Pond Lily', 'Small Rosette Viola', 'Asymmetrical Bud Flower', 'Pink Trumpet Flower', 'Pink Azalea', 'White Bellflower', 'Striped Lily', 'Small Pink Center Flower', 'Purple Sweet Pea', 'Yellow Centered Pink Blossom', 'Lotus Flower', 'Pink Thistle', 'Ovate-leaved Pink Flower', 'Blue-Purple Ring Flower', 'Symmetrical Disc Flower', 'Asymmetrical White Center Flower', 'Red and yellow daisy', 'Lily-Heart Orchid', 'Zigzag Stem Daisy', 'Asymmetrical Purple Flowers', 'White-Centered Cluster Flower', 'Pink Bellflower', 'Pink-Yellow-Orange Flower', 'Small Orchid', 'Water Droplet Lily', 'Intense Pink Petal Flower', 'Purple Rosette Lily', 'White Centered Pink Flower', 'Asymmetrical yellow and red flower', 'White Flower', 'Yellow and White Flower', 'Pink Hibiscus', 'Ringed Disc Flower', 'White Pea Flower', 'Small Violet Lily', 'Small Petal Flower', 'Pinkish Purple Flower', 'Tube bud flower', 'Carnation', 'Ruffled Petal Viola', 'Ruffled Petal Purple Flower', 'Smooth Petal Lily', 'White-spotted foxglove', 'Zigzag Stem Rosette Flower', 'Pink and Purple Flower', 'Large Ruffled Flower', 'Geranium', 'Rosette Arranged Purple Spotted Flower', 'Protea flower', 'Small Green Center Flower', 'Red and White Flower', 'Symmetrical Petal Bud', 'Blue Lily', 'Iris Sabdariffa', 'Sunflower', 'Small Pink Bee Flower', 'Large Clematis', 'Fernleaf Lily', 'Rosette Daisy', 'Orange-Blue Petal Flower', 'Pink and white gradient flower', 'Symmetrical White Flower', 'Striped White Flower', 'Small Yellow Flower with Green Center', 'Blue Petal Ring', 'Small Yellow Rosette', 'Round Petal Sunflower', 'Rosette Leaf Orchid', 'Small Ruffled Petal Flower', 'Drooping Ring-Petaled Flowers', 'Medium-Sized Symmetrical Flower', 'Red Carnation', 'Large Yellow Centered Flower', 'Purple Coneflower', 'Rosette Fernflower', 'Gradient Pink Flower', 'White and Yellow Orchid', 'Lily', 'Rosette Blue Flower', 'White Starflower', 'Red poppy', 'Red Ringed Drooping Flower', 'Smooth Stem Rosette', 'Pink and White Rosette Flower', 'Small Tube Bud Flower', 'Ruffled Bluebell', 'Taraxacum officinale', 'Asymmetrical Petal Flower', 'Yellow Daffodil', 'Lily with Pink Center', 'Orange and Yellow Bloom', 'Petite Pink Blossom', 'Nelumbo nucifera', 'Pink Water Lily', 'Asymmetrical striped petal flower', 'Spiral Leaf Arrangement', 'Small Lily-Flowered Purple and White Flower', 'Pink Peony', 'Colorful disc-flowered plant', 'Field poppy', 'Rosette Flower', 'Sphere-shaped Purple Flower', 'Speedwell', 'Pink Lotus', 'Rosette Blue Daisy', 'Pink Rosette', 'Pink and White Sweet Peas', 'Orange Ringflower', 'Strelitzia', 'Asymmetrical Drooping Flower', 'Lily of the Valley', 'Small Yellow Flower', 'Yellow Bellflower', 'Bird of Paradise, Strelitzia, Crane Flower', 'Amaryllis', 'Pink and White Sweet Pea', 'Yellow Centered White Flower', 'White-Centered Purple Flower', 'Blue Scabious', 'Yellow Daisy', 'Green Centered Drooping Lily', 'Intensely Colored Purple and White Flower with Zigzag Stem', 'Purple and White Lily', 'Large Flower', 'Bird of Paradise', 'Yellow and orange iris', 'Orange Ray Flower', 'Black-eyed Susan, Daisy, Yellow Sunflower', 'Yellow-centered oblong flower', 'Hydrangea', 'Waterdrop Flower', 'Fern-like Pink Flower', 'Yellow Woodland Flowers', 'Spotted Lily', 'White-Yellow Center Flower', 'Small White Center Rosette Flower', 'Zigzag Petal Flower', 'Red Rosette', 'Small Rosette Flower', 'White and Purple Gradient Flower', 'Rosette Yellow Flower', 'Small Fern-like Pink and White Flower', 'Sweet Pea', 'Protea', 'Asymmetrical Yellow Flower', 'White-Ringed Pink', 'Fern Leafed Red Flower', 'Fernlike Petal Flower', 'Small Rounded Pink Petals', 'Pink and White Flower', 'Small Pink Ring Flower', 'Rocky Hillside Purple Flower', 'Narcissus', 'Ringed Petal Flower', 'Arum Lily', 'Fernleaf Pink', 'Fernleaf Petunia', 'Wavy Margined Pink Flower', 'Ringed Center Lily', 'Yellow Ray Flower', 'Nasturtium', 'Orchid', 'Small Gradient Red Flower', 'Purple Star Flower', 'Orange Petal Fernleaf', 'Striped Daisy', 'Medium-sized Ruffled Petal Flower', "Lady's slipper orchid", 'Fern-like Petal Flower', 'Large Petal Flower', 'Ruffled Petal Flower', 'Pink Rosette Flower', 'Zigzag Petal Orchid', 'Spiky White-Petaled Flower', 'White Striped Tulip', 'Orange-centered Crocus', 'Rhombus Petal', 'Green Stem', 'Red Poppy, Papaver rhoeas, Field Poppy', 'Intense Purple Petalring', 'Rosette flower', 'Speckled Purple Blossom', 'Ringed Pink Flower', 'Digitalis purpurea', 'Small Orchid with Yellow Center', 'White and Pink Orchid', 'Large Round Flower', 'Gladiolus', 'Large Asymmetrical Petal Flower', 'Purple Rhombus Flower', 'White Petal Fern', 'Common Dandelion', 'Slipper orchid', 'Rosette Yellow Petal', 'Yellow and red daffodil-shaped flower', 'Wild UK Flower', 'Lily with Yellow Center', 'Greenstem Pink', 'Orange Petal Flower', 'Ring-centered yellow flower', 'Tiger Lily', 'White and Yellow Ring Flower', 'Fern-like Sunflower', 'Orange and Blue Flower', 'Graduated Pink Petal Flower', 'Large Oblong Petal Flower', 'Pink to Purple Gradient Daisy', 'Small White Centered Purple Flower', 'Intense Red Ruffle', 'Yellow Dandelion', 'Ruffled petal iris', 'Symmetrical Ring Orchid', 'White Orchid', 'Peony-like Sweet Pea', 'Asymmetrical White Flower', 'Yellow Iris', 'Papaver rhoeas', 'Lily Bellflower', 'Yellow Gerbera', 'Sunburst Poppy', 'Pink Coneflower, Echinacea Sunburst, Large Rosette Flower', 'Symmetrical White and Purple Lily', 'Symmetrical Grass flower', 'Asymmetrical White Poppy', 'Zigzag Striped Petal Flower', 'Green Leaf Rosette Flower', 'Drooping Bellflower', 'White-Ringed Pink Flower', 'Blue and White Petal Flower', 'White center flower', 'Purple Artichoke Flower', 'Large Pink Flower', 'Citrus Blossom', 'Yellow-White Ring Flower', 'Symmetrical Petal Orchid', 'Rosette-leaved Daisy', 'Marigold', 'Bluebell', 'Small Flower', 'Ruffled Pink Petal Flower', 'White Petal Flower', 'Yellow-centered Meadow flower', 'Intensely colored daisy', 'Red Ringed Petal Flower', 'Pink Coneflower', 'Zigzag Stemmed Purple Flower', 'Purple Sphere Flower', 'Golden Sunflower', 'Dahlia, Sunburst Dahlia, Red Dahlia', 'Yellow-flowered drooping plant', 'Purple Pea Flower', 'Trumpet Lily', 'Rosette Pea Flower', 'Garden Rosette', 'Small Blue Rosette', 'Symmetrical Gladiolus', 'Orange and Yellow Gradient Hibiscus', 'Asymmetrical Daisy', 'Small Yellow Blossom', 'Yellow-centered Pink Flower', 'Lilium lancifolium', 'Pink White Ringflower', 'Zigzag Stem', 'White Garden Flower', 'Pink and Purple Ring Flower', 'Ruffled Petal Iris', 'Smooth Stemmed Ovate Flower', 'Orange Polka Dot Lily', "Rhododendron'sassafras Bush", 'Yellow-Centered Daisy', 'Small Blue Lily', 'UK Wildflower-Thistle', 'Intensely Colored Ring Flower', 'Intense Blue Petal Flower', 'Lilium tigrinum', 'Gerbera Daisy', 'Sand Flower', 'Pink Clematis', 'Paradise Flower', 'White and Yellow Viola', 'Spiky Fern-like Spiral Leaf Flower', 'Symmetrical Rosette Flower', 'Asteraceae', 'Asymmetrical Petal Ring', 'Pink Orchid', 'Pink Garden Blossom', 'Brown-centered Coneflower', 'Orchid with Yellow Center', 'Blue Rosette Flower', 'Foxglove', 'Symmetrical Ringed Petal Flower', 'Rosette Petal Flower', 'Large Bud Flower', 'Helianthus', 'Pink Gradient Fernflower', 'Purple Rosette Flower', 'Rosette of Leaves with Small Yellow Flower', 'Yellow drooping flower', 'Lily Orchid', 'Brown-Centered Daisy', 'Purple Crocus', 'Pink-White Orchid', 'Large Ruffled Orchid', 'Symmetrical Gradient Pink', 'Orange Petunia', 'Asymmetrical Fern Leaf Flower', 'Pink and White Center Orchid', 'Red Poinsettia', 'Ringed Rayflower', 'Purple Cyclamen', 'Spiral Leaf Blue Flower', 'Bud-shaped Orchid', 'Purple Spotted Tulip', 'Symmetrical Petal Blossom', 'Large Yellow Flower', 'Small Pink Flower', 'Bellflower', 'Orange Flower', 'Large-Sized Azalea', 'Purple Blue Ring Flower', 'Small Orange Flower', 'Pink-to-Red Gradient Flower', 'White Rosette Flower', 'Asymmetrical Flower', 'Large Center Flower', 'Ringed yellow daisy', 'Gradient Petal Ring Blooms', 'Star Patterned Lily', 'Rosette Thistle', 'Rosette Pink Flower', 'Orange-Yellow Gladiolus', 'Colorful Ovate Petals', 'Pink Spotted Lily', 'Yellow Lily', 'Curved Petal Flower', 'Pink-White Gradient Flower', 'White Anemone', 'Rosette Petal Orchid', 'Yellow Centered Flower', 'Intense Pink Rosette', 'Spiral Leaf Blossom', 'Lily-flowered Purple Flower', 'White Petal Center Flower', 'Fern-like Purple Flower', 'Yellow Iris, British Iris, United Kingdom Iris', 'Ruffled Violet', 'Symmetrical Petal Hibiscus', 'Asiatic Lotus', 'Pink and White Ring Blossom', 'magnolia grandiflora', 'Rosette Petal Sweet Peas', 'Pink Ruffle Petal Flower', 'Large Petal Hibiscus', 'Drooping Flower', 'Ringed Center Flower', 'Blue and White Petunia', 'Pink Cluster Flower', 'Asymmetrical Blue Flower', 'Disc Ring Petal Flower', 'Yellow-centered Orchid', 'Small Ring Center Sweet Peas', 'Rosette Leaf Flower', 'Red-Centered Daisy', 'Large Brown and Yellow Striped Flower', 'Pink Flower', 'White-Centered Pink Flower', 'Small Symmetrical Petal Flower', 'Ringed Orchid', 'Small Yellow-centered Pink Flower', 'Blue Forget-me-not', 'Purple Ringflower', 'Intense Orange Flower', 'Lavender Disc Lily', 'Large Pink Petal Blossom', 'Rosette Leaf White Poppy', 'Clematis', 'Textured Pink Flower', 'Small Ray Flower', 'Purple Spot Tulip', 'Spring Bulbs', 'Green Centered Ruffled Flower', 'Delphinium', 'Orange Daisy', 'Red/Yellow Petal Flower', 'Gradient Pink and Purple Flower', 'Green Leaf Trumpet', 'Intensely colored ruffled nasturtium', 'Ruffled Yellow Flower', 'Rosette-Leaved Wildflowers', 'Large Blue Petal Flower', 'Purple Lily Flower', 'Intense Purple and White Crocus', 'Medium-sized Rose', 'Small Water Droplet Flowers', 'Purple and Yellow Flower', 'Forget-me-not', 'Intense Purple Petal Lily', 'Pink Rose', 'Pink gradient flower', 'Globe Thistle', 'Ruffled Ray Flower', 'Asymmetrical Center Ring Flower', 'Ruffled Flower', 'Yellow Centered Daisy', 'Small Petal Daisy', 'Pink Ring Flower', 'Yellow-Centered Pink Flower', 'Large Iris with Yellow Center', 'Pink and Yellow Ring Flower', 'Star-Shaped Center Flower', 'Ringed Orange Flower', 'Large Petal Orchid', 'Golf Ball Flower', 'Intense Blue Dandelion', 'Small White Center Flower', 'Yellow-Centered Orchid', 'Single-Petal Pink Flower', 'Spot-Flowered Lily', 'Blue Starflower', 'Pink-Centered Orchid', 'White Daisy', 'Ringed Rosette', 'Drooping Yellow Center Flower', 'Small Gradient Petal', 'Yellow-Red Ring Flower']
0it [00:00, ?it/s]1it [00:02,  2.01s/it]2it [00:03,  1.88s/it]3it [00:05,  1.59s/it]4it [00:06,  1.46s/it]5it [00:08,  1.68s/it]6it [00:09,  1.65s/it]7it [00:12,  1.82s/it]8it [00:14,  1.84s/it]9it [00:15,  1.80s/it]10it [00:16,  1.59s/it]11it [00:17,  1.44s/it]12it [00:20,  1.67s/it]13it [00:21,  1.58s/it]14it [00:23,  1.61s/it]15it [00:24,  1.43s/it]16it [00:25,  1.48s/it]17it [00:27,  1.51s/it]18it [00:29,  1.63s/it]19it [00:31,  1.80s/it]20it [00:34,  2.12s/it]21it [00:36,  2.28s/it]22it [00:39,  2.34s/it]23it [00:41,  2.35s/it]24it [00:44,  2.37s/it]25it [00:46,  2.32s/it]26it [00:48,  2.36s/it]27it [00:51,  2.32s/it]28it [00:53,  2.22s/it]29it [00:55,  2.16s/it]30it [00:57,  2.31s/it]31it [01:00,  2.33s/it]32it [01:02,  2.39s/it]33it [01:04,  2.31s/it]34it [01:07,  2.33s/it]35it [01:09,  2.18s/it]36it [01:11,  2.23s/it]37it [01:13,  2.12s/it]38it [01:16,  2.57s/it]39it [01:20,  2.77s/it]40it [01:22,  2.70s/it]41it [01:25,  2.69s/it]42it [01:27,  2.63s/it]43it [01:30,  2.62s/it]44it [01:32,  2.56s/it]45it [01:36,  2.90s/it]46it [01:39,  2.91s/it]47it [01:42,  2.83s/it]48it [01:44,  2.83s/it]49it [01:48,  3.19s/it]50it [01:51,  3.08s/it]51it [01:54,  3.00s/it]52it [01:57,  3.00s/it]53it [02:00,  2.93s/it]54it [02:03,  2.94s/it]55it [02:06,  3.01s/it]56it [02:10,  3.20s/it]57it [02:13,  3.19s/it]58it [02:17,  3.37s/it]59it [02:20,  3.22s/it]60it [02:23,  3.32s/it]61it [02:25,  3.02s/it]62it [02:28,  2.94s/it]63it [02:31,  3.07s/it]64it [02:34,  2.90s/it]65it [02:37,  2.98s/it]66it [02:40,  2.79s/it]67it [02:42,  2.84s/it]68it [02:45,  2.78s/it]69it [02:48,  2.84s/it]70it [02:51,  2.94s/it]71it [02:54,  2.80s/it]72it [02:57,  2.91s/it]73it [02:59,  2.77s/it]74it [03:02,  2.77s/it]75it [03:05,  2.82s/it]76it [03:08,  2.76s/it]77it [03:10,  2.77s/it]78it [03:13,  2.85s/it]79it [03:16,  2.84s/it]80it [03:19,  2.83s/it]81it [03:21,  2.62s/it]82it [03:24,  2.73s/it]83it [03:27,  2.84s/it]84it [03:30,  2.75s/it]85it [03:33,  2.78s/it]86it [03:36,  2.80s/it]87it [03:38,  2.70s/it]88it [03:40,  2.44s/it]89it [03:42,  2.39s/it]90it [03:45,  2.63s/it]91it [03:48,  2.68s/it]92it [03:51,  2.72s/it]93it [03:54,  2.75s/it]94it [03:57,  2.89s/it]95it [04:00,  2.99s/it]96it [04:03,  2.89s/it]97it [04:06,  2.85s/it]98it [04:08,  2.86s/it]99it [04:12,  2.92s/it]100it [04:14,  2.88s/it]101it [04:17,  2.75s/it]102it [04:19,  2.64s/it]103it [04:22,  2.63s/it]104it [04:24,  2.44s/it]105it [04:26,  2.31s/it]106it [04:28,  2.37s/it]107it [04:31,  2.33s/it]108it [04:34,  2.55s/it]109it [04:36,  2.43s/it]110it [04:38,  2.45s/it]111it [04:41,  2.65s/it]112it [04:44,  2.77s/it]113it [04:47,  2.75s/it]114it [04:50,  2.77s/it]115it [04:52,  2.70s/it]116it [04:56,  2.85s/it]117it [04:58,  2.75s/it]118it [05:01,  2.81s/it]119it [05:04,  2.75s/it]120it [05:06,  2.68s/it]121it [05:09,  2.75s/it]122it [05:12,  2.64s/it]123it [05:14,  2.56s/it]124it [05:17,  2.66s/it]125it [05:20,  2.91s/it]126it [05:24,  3.10s/it]127it [05:26,  2.92s/it]128it [05:29,  2.90s/it]129it [05:32,  2.88s/it]130it [05:35,  2.88s/it]131it [05:38,  2.87s/it]132it [05:41,  2.92s/it]133it [05:43,  2.82s/it]134it [05:46,  2.86s/it]135it [05:50,  3.18s/it]136it [05:54,  3.28s/it]137it [05:57,  3.26s/it]138it [06:00,  3.28s/it]139it [06:04,  3.26s/it]140it [06:06,  3.09s/it]141it [06:09,  2.95s/it]142it [06:11,  2.85s/it]143it [06:15,  2.99s/it]144it [06:18,  2.93s/it]145it [06:21,  3.09s/it]146it [06:24,  3.15s/it]147it [06:27,  3.03s/it]148it [06:30,  2.96s/it]149it [06:33,  3.10s/it]150it [06:36,  3.07s/it]151it [06:40,  3.18s/it]152it [06:43,  3.09s/it]153it [06:46,  3.05s/it]154it [06:49,  3.14s/it]155it [06:53,  3.31s/it]156it [06:55,  3.06s/it]157it [06:59,  3.23s/it]158it [07:02,  3.17s/it]159it [07:05,  3.13s/it]160it [07:07,  2.92s/it]161it [07:10,  2.92s/it]162it [07:13,  2.87s/it]163it [07:16,  2.85s/it]164it [07:19,  2.87s/it]165it [07:21,  2.74s/it]166it [07:24,  2.69s/it]167it [07:26,  2.73s/it]168it [07:29,  2.64s/it]169it [07:31,  2.50s/it]170it [07:34,  2.61s/it]171it [07:37,  2.63s/it]172it [07:39,  2.59s/it]173it [07:42,  2.65s/it]174it [07:45,  2.70s/it]175it [07:48,  2.77s/it]176it [07:50,  2.73s/it]177it [07:53,  2.74s/it]178it [07:56,  2.78s/it]179it [07:59,  2.78s/it]180it [08:02,  2.84s/it]181it [08:04,  2.82s/it]182it [08:08,  2.90s/it]183it [08:10,  2.74s/it]184it [08:13,  2.76s/it]185it [08:15,  2.73s/it]186it [08:19,  2.90s/it]187it [08:22,  2.94s/it]188it [08:24,  2.88s/it]189it [08:28,  3.08s/it]190it [08:31,  3.03s/it]191it [08:34,  3.04s/it]192it [08:38,  3.20s/it]193it [08:40,  3.06s/it]194it [08:43,  3.04s/it]195it [08:46,  3.00s/it]196it [08:49,  3.01s/it]197it [08:52,  3.00s/it]198it [08:55,  2.97s/it]199it [08:58,  2.95s/it]200it [09:01,  3.04s/it]201it [09:04,  3.03s/it]202it [09:07,  2.95s/it]203it [09:10,  3.00s/it]204it [09:13,  2.99s/it]205it [09:16,  2.92s/it]206it [09:19,  2.89s/it]207it [09:22,  3.12s/it]208it [09:25,  3.03s/it]209it [09:28,  3.11s/it]210it [09:32,  3.39s/it]211it [09:35,  3.22s/it]212it [09:38,  3.17s/it]213it [09:41,  3.10s/it]214it [09:45,  3.18s/it]215it [09:48,  3.28s/it]216it [09:52,  3.50s/it]217it [09:55,  3.40s/it]218it [09:59,  3.53s/it]219it [10:02,  3.43s/it]220it [10:06,  3.52s/it]221it [10:09,  3.37s/it]222it [10:12,  3.32s/it]223it [10:15,  3.27s/it]224it [10:19,  3.37s/it]225it [10:23,  3.44s/it]226it [10:26,  3.30s/it]227it [10:29,  3.26s/it]228it [10:32,  3.26s/it]229it [10:35,  3.11s/it]229it [10:35,  2.77s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]  4%|▍         | 1/25 [00:39<15:49, 39.56s/it]  8%|▊         | 2/25 [00:39<06:20, 16.54s/it] 12%|█▏        | 3/25 [00:40<03:22,  9.18s/it] 16%|█▌        | 4/25 [00:40<01:58,  5.65s/it] 20%|██        | 5/25 [00:40<01:13,  3.70s/it] 24%|██▍       | 6/25 [00:41<00:48,  2.53s/it] 28%|██▊       | 7/25 [00:41<00:31,  1.78s/it] 32%|███▏      | 8/25 [00:41<00:21,  1.28s/it] 36%|███▌      | 9/25 [00:41<00:15,  1.05it/s] 40%|████      | 10/25 [00:42<00:10,  1.38it/s] 44%|████▍     | 11/25 [00:42<00:07,  1.76it/s] 48%|████▊     | 12/25 [00:43<00:08,  1.60it/s] 52%|█████▏    | 13/25 [00:43<00:05,  2.01it/s] 56%|█████▌    | 14/25 [00:43<00:04,  2.40it/s] 60%|██████    | 15/25 [00:43<00:03,  2.68it/s] 64%|██████▍   | 16/25 [00:43<00:03,  2.92it/s] 68%|██████▊   | 17/25 [01:03<00:48,  6.02s/it] 72%|███████▏  | 18/25 [01:03<00:29,  4.27s/it] 76%|███████▌  | 19/25 [01:04<00:19,  3.17s/it] 80%|████████  | 20/25 [01:04<00:11,  2.28s/it] 84%|████████▍ | 21/25 [01:04<00:06,  1.66s/it] 88%|████████▊ | 22/25 [01:04<00:03,  1.22s/it] 92%|█████████▏| 23/25 [01:04<00:01,  1.09it/s] 96%|█████████▌| 24/25 [01:05<00:00,  1.42it/s]100%|██████████| 25/25 [01:05<00:00,  2.62s/it]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288623809814
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:05<02:09,  5.40s/it]  8%|▊         | 2/25 [00:05<00:53,  2.35s/it] 12%|█▏        | 3/25 [00:05<00:30,  1.38s/it] 16%|█▌        | 4/25 [00:06<00:19,  1.09it/s] 20%|██        | 5/25 [00:06<00:15,  1.27it/s] 24%|██▍       | 6/25 [00:06<00:11,  1.69it/s] 28%|██▊       | 7/25 [00:07<00:08,  2.11it/s] 32%|███▏      | 8/25 [00:07<00:06,  2.56it/s] 36%|███▌      | 9/25 [00:07<00:05,  2.96it/s] 40%|████      | 10/25 [00:07<00:04,  3.31it/s] 44%|████▍     | 11/25 [00:07<00:03,  3.62it/s] 48%|████▊     | 12/25 [00:08<00:03,  3.85it/s] 52%|█████▏    | 13/25 [00:08<00:02,  4.08it/s] 56%|█████▌    | 14/25 [00:08<00:02,  4.29it/s] 60%|██████    | 15/25 [00:08<00:02,  4.45it/s] 64%|██████▍   | 16/25 [00:08<00:01,  4.53it/s] 68%|██████▊   | 17/25 [00:09<00:03,  2.19it/s] 72%|███████▏  | 18/25 [00:10<00:02,  2.61it/s] 76%|███████▌  | 19/25 [00:10<00:01,  3.03it/s] 80%|████████  | 20/25 [00:10<00:01,  3.42it/s] 84%|████████▍ | 21/25 [00:10<00:01,  3.60it/s] 88%|████████▊ | 22/25 [00:11<00:00,  3.82it/s] 92%|█████████▏| 23/25 [00:11<00:00,  4.06it/s] 96%|█████████▌| 24/25 [00:11<00:00,  4.27it/s]100%|██████████| 25/25 [00:11<00:00,  2.13it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.4752058982849121


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.8128890991211
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 58.07448365587901
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.83322173481235
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 52.2390322587627


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.520591735839844
=========================          END          =========================
0it [00:00, ?it/s]1it [00:04,  4.23s/it]2it [00:09,  4.61s/it]3it [00:13,  4.65s/it]4it [00:17,  4.45s/it]5it [00:22,  4.51s/it]6it [00:27,  4.60s/it]7it [00:32,  4.77s/it]8it [00:36,  4.53s/it]9it [00:40,  4.45s/it]10it [00:44,  4.37s/it]11it [00:49,  4.28s/it]12it [00:53,  4.27s/it]13it [00:57,  4.32s/it]14it [01:01,  4.17s/it]15it [01:05,  4.21s/it]16it [01:09,  4.20s/it]17it [01:14,  4.27s/it]18it [01:18,  4.27s/it]19it [01:23,  4.30s/it]20it [01:27,  4.34s/it]21it [01:32,  4.43s/it]22it [01:36,  4.34s/it]23it [01:40,  4.27s/it]24it [01:43,  4.00s/it]25it [01:47,  3.84s/it]26it [01:50,  3.63s/it]27it [01:54,  3.75s/it]28it [01:58,  3.74s/it]29it [02:01,  3.71s/it]30it [02:05,  3.65s/it]31it [02:08,  3.56s/it]32it [02:11,  3.51s/it]33it [02:15,  3.62s/it]34it [02:19,  3.63s/it]35it [02:23,  3.59s/it]36it [02:26,  3.50s/it]37it [02:29,  3.52s/it]38it [02:33,  3.53s/it]39it [02:36,  3.50s/it]40it [02:41,  3.70s/it]41it [02:44,  3.71s/it]42it [02:48,  3.71s/it]43it [02:52,  3.66s/it]44it [02:55,  3.60s/it]45it [02:59,  3.60s/it]46it [03:02,  3.56s/it]47it [03:05,  3.36s/it]48it [03:07,  3.08s/it]49it [03:10,  2.96s/it]50it [03:14,  3.37s/it]51it [03:19,  3.67s/it]52it [03:23,  3.71s/it]53it [03:26,  3.75s/it]54it [03:30,  3.63s/it]55it [03:34,  3.67s/it]56it [03:38,  3.93s/it]57it [03:42,  3.94s/it]58it [03:45,  3.74s/it]59it [03:49,  3.84s/it]60it [03:53,  3.88s/it]61it [03:57,  3.80s/it]62it [04:01,  3.85s/it]63it [04:05,  4.07s/it]64it [04:10,  4.13s/it]65it [04:14,  4.25s/it]66it [04:18,  4.20s/it]67it [04:22,  4.02s/it]68it [04:26,  4.10s/it]69it [04:31,  4.32s/it]70it [04:36,  4.42s/it]71it [04:40,  4.47s/it]72it [04:45,  4.44s/it]73it [04:49,  4.34s/it]74it [04:53,  4.16s/it]75it [04:57,  4.12s/it]76it [05:01,  4.18s/it]77it [05:05,  4.10s/it]78it [05:09,  4.02s/it]79it [05:13,  4.02s/it]80it [05:17,  4.02s/it]81it [05:21,  4.02s/it]82it [05:25,  4.11s/it]83it [05:29,  4.17s/it]84it [05:34,  4.23s/it]85it [05:38,  4.15s/it]86it [05:42,  4.19s/it]87it [05:46,  4.05s/it]88it [05:51,  4.33s/it]89it [05:55,  4.33s/it]90it [05:59,  4.32s/it]91it [06:04,  4.33s/it]92it [06:07,  4.19s/it]93it [06:12,  4.28s/it]94it [06:16,  4.20s/it]95it [06:20,  4.08s/it]96it [06:24,  4.01s/it]97it [06:28,  4.03s/it]98it [06:32,  4.10s/it]99it [06:36,  3.93s/it]100it [06:40,  3.97s/it]101it [06:44,  4.04s/it]102it [06:48,  3.96s/it]103it [06:52,  4.01s/it]104it [06:56,  4.03s/it]105it [07:00,  4.00s/it]106it [07:03,  3.92s/it]107it [07:08,  4.05s/it]108it [07:12,  4.18s/it]109it [07:16,  4.13s/it]110it [07:20,  4.13s/it]111it [07:25,  4.18s/it]112it [07:29,  4.09s/it]113it [07:33,  4.11s/it]114it [07:37,  4.13s/it]115it [07:41,  4.20s/it]116it [07:45,  4.10s/it]117it [07:49,  4.08s/it]118it [07:53,  4.04s/it]119it [07:57,  4.09s/it]120it [08:01,  3.99s/it]121it [08:05,  3.84s/it]122it [08:09,  3.87s/it]123it [08:12,  3.78s/it]124it [08:16,  3.93s/it]125it [08:20,  3.97s/it]126it [08:24,  3.90s/it]127it [08:28,  3.81s/it]128it [08:32,  3.86s/it]129it [08:36,  3.94s/it]130it [08:40,  3.91s/it]131it [08:44,  4.02s/it]132it [08:48,  3.90s/it]133it [08:51,  3.82s/it]134it [08:55,  3.81s/it]135it [08:59,  3.89s/it]136it [09:03,  3.87s/it]137it [09:07,  3.82s/it]138it [09:11,  4.08s/it]139it [09:15,  4.03s/it]140it [09:19,  3.85s/it]141it [09:22,  3.72s/it]142it [09:26,  3.78s/it]143it [09:30,  3.71s/it]144it [09:33,  3.66s/it]145it [09:36,  3.51s/it]146it [09:40,  3.46s/it]147it [09:43,  3.50s/it]148it [09:47,  3.59s/it]149it [09:51,  3.64s/it]150it [09:54,  3.55s/it]151it [09:58,  3.67s/it]152it [10:02,  3.66s/it]153it [10:05,  3.63s/it]154it [10:09,  3.54s/it]155it [10:12,  3.48s/it]156it [10:15,  3.47s/it]157it [10:20,  3.75s/it]158it [10:24,  3.88s/it]159it [10:28,  3.88s/it]160it [10:31,  3.76s/it]161it [10:35,  3.66s/it]162it [10:38,  3.63s/it]163it [10:42,  3.75s/it]164it [10:46,  3.77s/it]165it [10:50,  3.77s/it]166it [10:54,  3.96s/it]167it [10:58,  3.99s/it]168it [11:03,  4.12s/it]169it [11:07,  4.07s/it]170it [11:11,  3.99s/it]171it [11:14,  3.94s/it]172it [11:19,  4.10s/it]173it [11:22,  3.92s/it]174it [11:26,  3.97s/it]175it [11:31,  4.07s/it]176it [11:35,  4.07s/it]177it [11:40,  4.30s/it]178it [11:43,  4.09s/it]179it [11:47,  4.08s/it]180it [11:51,  4.05s/it]181it [11:55,  3.95s/it]182it [11:59,  3.93s/it]183it [12:03,  3.95s/it]184it [12:07,  4.00s/it]185it [12:11,  4.05s/it]186it [12:14,  3.84s/it]187it [12:18,  3.82s/it]188it [12:22,  3.90s/it]189it [12:26,  3.72s/it]190it [12:29,  3.55s/it]191it [12:32,  3.47s/it]192it [12:35,  3.40s/it]193it [12:38,  3.33s/it]194it [12:42,  3.35s/it]195it [12:45,  3.32s/it]196it [12:49,  3.52s/it]197it [12:53,  3.67s/it]198it [12:56,  3.46s/it]199it [13:00,  3.60s/it]200it [13:04,  3.73s/it]201it [13:07,  3.55s/it]202it [13:10,  3.43s/it]203it [13:13,  3.28s/it]204it [13:17,  3.40s/it]205it [13:20,  3.35s/it]206it [13:23,  3.23s/it]207it [13:26,  3.10s/it]208it [13:29,  3.18s/it]209it [13:33,  3.20s/it]210it [13:35,  3.04s/it]211it [13:39,  3.21s/it]212it [13:42,  3.19s/it]213it [13:45,  3.23s/it]214it [13:48,  3.15s/it]215it [13:52,  3.22s/it]216it [13:55,  3.23s/it]217it [13:58,  3.23s/it]218it [14:02,  3.49s/it]219it [14:06,  3.60s/it]220it [14:11,  3.88s/it]221it [14:14,  3.74s/it]222it [14:18,  3.74s/it]223it [14:21,  3.63s/it]224it [14:24,  3.49s/it]225it [14:27,  3.36s/it]226it [14:31,  3.48s/it]227it [14:35,  3.75s/it]228it [14:40,  3.93s/it]229it [14:44,  4.02s/it]229it [14:44,  3.86s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:09<03:39,  9.16s/it]  8%|▊         | 2/25 [00:09<01:36,  4.19s/it] 12%|█▏        | 3/25 [00:10<00:52,  2.38s/it] 16%|█▌        | 4/25 [00:10<00:32,  1.54s/it] 20%|██        | 5/25 [00:10<00:21,  1.06s/it] 24%|██▍       | 6/25 [00:10<00:14,  1.30it/s] 28%|██▊       | 7/25 [00:10<00:10,  1.70it/s] 32%|███▏      | 8/25 [00:11<00:08,  2.09it/s] 36%|███▌      | 9/25 [00:11<00:06,  2.50it/s] 40%|████      | 10/25 [00:11<00:05,  2.93it/s] 44%|████▍     | 11/25 [00:11<00:04,  3.33it/s] 48%|████▊     | 12/25 [00:12<00:03,  3.65it/s] 52%|█████▏    | 13/25 [00:12<00:03,  3.94it/s] 56%|█████▌    | 14/25 [00:12<00:02,  4.18it/s] 60%|██████    | 15/25 [00:12<00:02,  4.27it/s] 64%|██████▍   | 16/25 [00:12<00:02,  4.43it/s] 68%|██████▊   | 17/25 [00:13<00:01,  4.53it/s] 72%|███████▏  | 18/25 [00:15<00:05,  1.32it/s] 76%|███████▌  | 19/25 [00:15<00:03,  1.68it/s] 80%|████████  | 20/25 [00:15<00:02,  2.10it/s] 84%|████████▍ | 21/25 [00:15<00:01,  2.53it/s] 88%|████████▊ | 22/25 [00:15<00:01,  2.96it/s] 92%|█████████▏| 23/25 [00:16<00:00,  3.36it/s] 96%|█████████▌| 24/25 [00:16<00:00,  3.71it/s]100%|██████████| 25/25 [00:16<00:00,  1.48it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288623809814
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:07<02:57,  7.41s/it]  8%|▊         | 2/25 [00:07<01:13,  3.17s/it] 12%|█▏        | 3/25 [00:07<00:40,  1.82s/it] 16%|█▌        | 4/25 [00:09<00:33,  1.60s/it] 20%|██        | 5/25 [00:09<00:22,  1.11s/it] 24%|██▍       | 6/25 [00:09<00:15,  1.25it/s] 28%|██▊       | 7/25 [00:09<00:10,  1.65it/s] 32%|███▏      | 8/25 [00:09<00:08,  2.09it/s] 36%|███▌      | 9/25 [00:10<00:06,  2.54it/s] 40%|████      | 10/25 [00:10<00:05,  2.96it/s] 44%|████▍     | 11/25 [00:10<00:04,  3.35it/s] 48%|████▊     | 12/25 [00:10<00:03,  3.70it/s] 52%|█████▏    | 13/25 [00:10<00:03,  3.99it/s] 56%|█████▌    | 14/25 [00:11<00:02,  4.16it/s] 60%|██████    | 15/25 [00:11<00:02,  4.30it/s] 64%|██████▍   | 16/25 [00:11<00:02,  4.44it/s] 68%|██████▊   | 17/25 [00:11<00:01,  4.53it/s] 72%|███████▏  | 18/25 [00:12<00:01,  4.63it/s] 76%|███████▌  | 19/25 [00:12<00:01,  4.51it/s] 80%|████████  | 20/25 [00:12<00:01,  3.83it/s] 84%|████████▍ | 21/25 [00:12<00:01,  3.98it/s] 88%|████████▊ | 22/25 [00:13<00:00,  4.07it/s] 92%|█████████▏| 23/25 [00:13<00:00,  4.20it/s] 96%|█████████▌| 24/25 [00:13<00:00,  4.30it/s]100%|██████████| 25/25 [00:13<00:00,  1.79it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.47347936034202576


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.8128890991211
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 57.37518295657831
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.57941590171579
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 51.586535985885995


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.34793472290039
=========================          END          =========================
0it [00:00, ?it/s]1it [00:05,  5.34s/it]2it [00:10,  5.41s/it]3it [00:15,  5.20s/it]4it [00:20,  5.06s/it]5it [00:25,  4.94s/it]6it [00:29,  4.69s/it]7it [00:34,  4.72s/it]8it [00:39,  4.76s/it]9it [00:44,  4.83s/it]10it [00:48,  4.55s/it]11it [00:52,  4.50s/it]12it [00:57,  4.57s/it]13it [01:01,  4.40s/it]14it [01:05,  4.45s/it]15it [01:10,  4.42s/it]16it [01:15,  4.58s/it]17it [01:19,  4.58s/it]18it [01:24,  4.65s/it]19it [01:28,  4.52s/it]20it [01:32,  4.46s/it]21it [01:37,  4.36s/it]22it [01:41,  4.33s/it]23it [01:46,  4.42s/it]24it [01:50,  4.43s/it]25it [01:54,  4.44s/it]26it [01:58,  4.31s/it]27it [02:03,  4.33s/it]28it [02:08,  4.47s/it]29it [02:13,  4.64s/it]30it [02:17,  4.61s/it]31it [02:22,  4.67s/it]32it [02:27,  4.74s/it]33it [02:31,  4.67s/it]34it [02:36,  4.71s/it]35it [02:41,  4.71s/it]36it [02:46,  4.75s/it]37it [02:51,  4.76s/it]38it [02:56,  4.95s/it]39it [03:01,  4.90s/it]40it [03:05,  4.77s/it]41it [03:10,  4.92s/it]42it [03:15,  4.93s/it]43it [03:20,  4.84s/it]44it [03:25,  4.86s/it]45it [03:30,  4.91s/it]46it [03:35,  4.83s/it]47it [03:39,  4.80s/it]48it [03:44,  4.72s/it]49it [03:49,  4.77s/it]50it [03:54,  4.79s/it]51it [03:58,  4.54s/it]52it [04:02,  4.47s/it]53it [04:06,  4.46s/it]54it [04:11,  4.51s/it]55it [04:15,  4.46s/it]56it [04:19,  4.37s/it]57it [04:24,  4.45s/it]58it [04:28,  4.28s/it]59it [04:33,  4.45s/it]60it [04:37,  4.45s/it]61it [04:42,  4.42s/it]62it [04:45,  4.19s/it]63it [04:50,  4.27s/it]64it [04:55,  4.42s/it]65it [04:59,  4.58s/it]66it [05:04,  4.64s/it]67it [05:09,  4.82s/it]68it [05:15,  4.88s/it]69it [05:19,  4.80s/it]70it [05:24,  4.91s/it]71it [05:30,  5.03s/it]72it [05:35,  5.08s/it]73it [05:40,  4.99s/it]74it [05:44,  4.89s/it]75it [05:49,  4.81s/it]76it [05:54,  4.96s/it]77it [05:59,  4.87s/it]78it [06:04,  4.88s/it]79it [06:09,  4.88s/it]80it [06:13,  4.78s/it]81it [06:18,  4.92s/it]82it [06:23,  4.86s/it]83it [06:27,  4.69s/it]84it [06:31,  4.46s/it]85it [06:35,  4.18s/it]86it [06:39,  4.16s/it]87it [06:44,  4.35s/it]88it [06:48,  4.21s/it]89it [06:52,  4.14s/it]90it [06:55,  4.02s/it]91it [07:00,  4.17s/it]92it [07:05,  4.35s/it]93it [07:08,  4.18s/it]94it [07:13,  4.21s/it]95it [07:17,  4.25s/it]96it [07:21,  4.25s/it]97it [07:26,  4.24s/it]98it [07:30,  4.25s/it]99it [07:34,  4.26s/it]100it [07:38,  4.25s/it]101it [07:43,  4.33s/it]102it [07:47,  4.38s/it]103it [07:52,  4.59s/it]104it [07:57,  4.70s/it]105it [08:02,  4.81s/it]106it [08:07,  4.80s/it]107it [08:12,  4.76s/it]108it [08:16,  4.56s/it]109it [08:20,  4.42s/it]110it [08:25,  4.50s/it]111it [08:30,  4.61s/it]112it [08:34,  4.68s/it]113it [08:39,  4.63s/it]114it [08:43,  4.48s/it]115it [08:47,  4.33s/it]116it [08:51,  4.25s/it]117it [08:56,  4.33s/it]118it [09:00,  4.22s/it]119it [09:04,  4.14s/it]120it [09:08,  4.20s/it]121it [09:12,  4.24s/it]122it [09:17,  4.27s/it]123it [09:20,  4.10s/it]124it [09:24,  4.00s/it]125it [09:28,  4.06s/it]126it [09:32,  4.03s/it]127it [09:36,  4.05s/it]128it [09:41,  4.13s/it]129it [09:45,  4.13s/it]130it [09:49,  4.05s/it]131it [09:53,  4.14s/it]132it [09:57,  4.13s/it]133it [10:01,  4.14s/it]134it [10:06,  4.18s/it]135it [10:09,  3.99s/it]136it [10:13,  3.97s/it]137it [10:17,  3.90s/it]138it [10:21,  4.14s/it]139it [10:26,  4.22s/it]140it [10:30,  4.19s/it]141it [10:35,  4.35s/it]142it [10:39,  4.47s/it]143it [10:44,  4.58s/it]144it [10:49,  4.55s/it]145it [10:54,  4.63s/it]146it [10:58,  4.63s/it]147it [11:03,  4.69s/it]148it [11:07,  4.57s/it]149it [11:12,  4.70s/it]150it [11:17,  4.64s/it]151it [11:21,  4.60s/it]152it [11:26,  4.57s/it]153it [11:30,  4.45s/it]154it [11:34,  4.39s/it]155it [11:39,  4.35s/it]156it [11:42,  4.22s/it]157it [11:47,  4.42s/it]158it [11:52,  4.52s/it]159it [11:55,  4.17s/it]160it [12:00,  4.24s/it]161it [12:04,  4.16s/it]162it [12:08,  4.16s/it]163it [12:12,  4.12s/it]164it [12:16,  3.97s/it]165it [12:19,  3.89s/it]166it [12:23,  3.95s/it]167it [12:28,  4.07s/it]168it [12:31,  3.87s/it]169it [12:35,  3.76s/it]170it [12:38,  3.54s/it]171it [12:41,  3.52s/it]172it [12:45,  3.71s/it]173it [12:50,  4.06s/it]174it [12:54,  4.04s/it]175it [12:58,  4.11s/it]176it [13:03,  4.28s/it]177it [13:08,  4.39s/it]178it [13:13,  4.52s/it]179it [13:17,  4.47s/it]180it [13:21,  4.48s/it]181it [13:26,  4.46s/it]182it [13:31,  4.61s/it]183it [13:35,  4.57s/it]184it [13:40,  4.57s/it]185it [13:44,  4.51s/it]186it [13:49,  4.62s/it]187it [13:54,  4.78s/it]188it [13:59,  4.78s/it]189it [14:04,  4.83s/it]190it [14:09,  4.82s/it]191it [14:14,  4.80s/it]192it [14:18,  4.75s/it]193it [14:24,  4.95s/it]194it [14:28,  4.77s/it]195it [14:33,  4.72s/it]196it [14:37,  4.61s/it]197it [14:41,  4.47s/it]198it [14:46,  4.47s/it]199it [14:50,  4.37s/it]200it [14:54,  4.45s/it]201it [14:59,  4.63s/it]202it [15:04,  4.69s/it]203it [15:08,  4.55s/it]204it [15:13,  4.65s/it]205it [15:19,  4.83s/it]206it [15:23,  4.78s/it]207it [15:28,  4.73s/it]208it [15:33,  4.89s/it]209it [15:38,  4.80s/it]210it [15:43,  4.93s/it]211it [15:48,  4.82s/it]212it [15:53,  4.88s/it]213it [15:57,  4.76s/it]214it [16:02,  4.75s/it]215it [16:07,  4.82s/it]216it [16:12,  4.83s/it]217it [16:16,  4.78s/it]218it [16:21,  4.70s/it]219it [16:25,  4.61s/it]220it [16:30,  4.65s/it]221it [16:34,  4.50s/it]222it [16:38,  4.41s/it]223it [16:42,  4.34s/it]224it [16:47,  4.36s/it]225it [16:51,  4.43s/it]226it [16:56,  4.51s/it]227it [17:01,  4.49s/it]228it [17:05,  4.56s/it]229it [17:10,  4.56s/it]229it [17:10,  4.50s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:08<03:14,  8.12s/it]  8%|▊         | 2/25 [00:09<01:32,  4.01s/it] 12%|█▏        | 3/25 [00:09<00:50,  2.27s/it] 16%|█▌        | 4/25 [00:09<00:30,  1.46s/it] 20%|██        | 5/25 [00:09<00:20,  1.01s/it] 24%|██▍       | 6/25 [00:10<00:14,  1.35it/s] 28%|██▊       | 7/25 [00:10<00:10,  1.75it/s] 32%|███▏      | 8/25 [00:10<00:07,  2.19it/s] 36%|███▌      | 9/25 [00:10<00:06,  2.64it/s] 40%|████      | 10/25 [00:10<00:04,  3.08it/s] 44%|████▍     | 11/25 [00:11<00:04,  3.46it/s] 48%|████▊     | 12/25 [00:11<00:03,  3.79it/s] 52%|█████▏    | 13/25 [00:11<00:02,  4.01it/s] 56%|█████▌    | 14/25 [00:11<00:02,  4.22it/s] 60%|██████    | 15/25 [00:11<00:02,  4.40it/s] 64%|██████▍   | 16/25 [00:12<00:02,  4.47it/s] 68%|██████▊   | 17/25 [00:12<00:01,  4.54it/s] 72%|███████▏  | 18/25 [00:12<00:02,  3.23it/s] 76%|███████▌  | 19/25 [00:13<00:01,  3.49it/s] 80%|████████  | 20/25 [00:13<00:01,  3.77it/s] 84%|████████▍ | 21/25 [00:13<00:00,  4.03it/s] 88%|████████▊ | 22/25 [00:13<00:00,  4.23it/s] 92%|█████████▏| 23/25 [00:14<00:00,  4.39it/s] 96%|█████████▌| 24/25 [00:14<00:00,  2.74it/s]100%|██████████| 25/25 [00:15<00:00,  1.63it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288027763367
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:06<02:32,  6.36s/it]  8%|▊         | 2/25 [00:06<01:05,  2.84s/it] 12%|█▏        | 3/25 [00:06<00:36,  1.64s/it] 16%|█▌        | 4/25 [00:07<00:22,  1.08s/it] 20%|██        | 5/25 [00:07<00:15,  1.30it/s] 24%|██▍       | 6/25 [00:07<00:10,  1.73it/s] 28%|██▊       | 7/25 [00:07<00:08,  2.18it/s] 32%|███▏      | 8/25 [00:08<00:06,  2.63it/s] 36%|███▌      | 9/25 [00:08<00:05,  3.07it/s] 40%|████      | 10/25 [00:08<00:04,  3.46it/s] 44%|████▍     | 11/25 [00:08<00:03,  3.69it/s] 48%|████▊     | 12/25 [00:08<00:03,  3.86it/s] 52%|█████▏    | 13/25 [00:09<00:02,  4.05it/s] 56%|█████▌    | 14/25 [00:09<00:02,  4.22it/s] 60%|██████    | 15/25 [00:09<00:02,  4.35it/s] 64%|██████▍   | 16/25 [00:09<00:02,  4.43it/s] 68%|██████▊   | 17/25 [00:09<00:01,  4.39it/s] 72%|███████▏  | 18/25 [00:10<00:01,  4.42it/s] 76%|███████▌  | 19/25 [00:10<00:01,  4.46it/s] 80%|████████  | 20/25 [00:10<00:01,  4.56it/s] 84%|████████▍ | 21/25 [00:10<00:00,  4.52it/s] 88%|████████▊ | 22/25 [00:11<00:00,  4.52it/s] 92%|█████████▏| 23/25 [00:11<00:00,  4.62it/s] 96%|█████████▌| 24/25 [00:11<00:00,  4.62it/s]100%|██████████| 25/25 [00:11<00:00,  2.09it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.47423043847084045


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.81288146972656
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 57.440234184420234
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.8044502046923
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 52.21980679663702


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.42304229736328
=========================          END          =========================
0it [00:00, ?it/s]1it [00:04,  4.25s/it]2it [00:08,  4.11s/it]3it [00:11,  3.87s/it]4it [00:15,  3.73s/it]5it [00:19,  3.79s/it]6it [00:23,  3.92s/it]7it [00:27,  3.98s/it]8it [00:32,  4.22s/it]9it [00:37,  4.66s/it]10it [00:42,  4.68s/it]11it [00:47,  4.66s/it]12it [00:51,  4.58s/it]13it [00:56,  4.52s/it]14it [01:00,  4.47s/it]15it [01:04,  4.48s/it]16it [01:09,  4.67s/it]17it [01:14,  4.55s/it]18it [01:18,  4.51s/it]19it [01:22,  4.44s/it]20it [01:27,  4.51s/it]21it [01:32,  4.51s/it]22it [01:36,  4.47s/it]23it [01:41,  4.67s/it]24it [01:46,  4.69s/it]25it [01:50,  4.63s/it]26it [01:54,  4.47s/it]27it [01:58,  4.29s/it]28it [02:03,  4.44s/it]29it [02:08,  4.61s/it]30it [02:13,  4.74s/it]31it [02:18,  4.87s/it]32it [02:24,  5.05s/it]33it [02:29,  5.04s/it]34it [02:34,  4.96s/it]35it [02:39,  4.96s/it]36it [02:43,  4.89s/it]37it [02:48,  4.86s/it]38it [02:53,  4.80s/it]39it [02:58,  4.80s/it]40it [03:02,  4.73s/it]41it [03:07,  4.68s/it]42it [03:11,  4.65s/it]43it [03:16,  4.60s/it]44it [03:21,  4.77s/it]45it [03:26,  4.74s/it]46it [03:31,  4.90s/it]47it [03:35,  4.78s/it]48it [03:40,  4.68s/it]49it [03:45,  4.80s/it]50it [03:49,  4.65s/it]51it [03:54,  4.63s/it]52it [03:58,  4.53s/it]53it [04:03,  4.71s/it]54it [04:08,  4.86s/it]55it [04:14,  5.00s/it]56it [04:19,  5.00s/it]57it [04:24,  4.97s/it]58it [04:29,  5.05s/it]59it [04:34,  5.03s/it]60it [04:39,  5.10s/it]61it [04:44,  5.03s/it]62it [04:49,  5.05s/it]63it [04:54,  5.07s/it]64it [04:59,  4.95s/it]65it [05:04,  5.01s/it]66it [05:09,  5.08s/it]67it [05:14,  5.08s/it]68it [05:19,  5.08s/it]69it [05:25,  5.15s/it]70it [05:30,  5.15s/it]71it [05:35,  5.18s/it]72it [05:40,  5.04s/it]73it [05:44,  4.87s/it]74it [05:49,  4.81s/it]75it [05:53,  4.66s/it]76it [05:58,  4.69s/it]77it [06:02,  4.56s/it]78it [06:07,  4.56s/it]79it [06:12,  4.64s/it]80it [06:16,  4.57s/it]81it [06:21,  4.52s/it]82it [06:25,  4.53s/it]83it [06:30,  4.56s/it]84it [06:33,  4.31s/it]85it [06:37,  4.15s/it]86it [06:42,  4.28s/it]87it [06:46,  4.18s/it]88it [06:50,  4.23s/it]89it [06:54,  4.15s/it]90it [06:58,  4.16s/it]91it [07:02,  4.17s/it]92it [07:06,  4.07s/it]93it [07:10,  4.05s/it]94it [07:14,  3.85s/it]95it [07:17,  3.78s/it]96it [07:21,  3.77s/it]97it [07:25,  3.97s/it]98it [07:29,  3.96s/it]99it [07:32,  3.53s/it]100it [07:34,  3.09s/it]101it [07:37,  2.98s/it]102it [07:40,  3.16s/it]103it [07:44,  3.35s/it]104it [07:49,  3.68s/it]105it [07:52,  3.76s/it]106it [07:56,  3.63s/it]107it [07:59,  3.56s/it]108it [08:03,  3.52s/it]109it [08:06,  3.53s/it]110it [08:10,  3.65s/it]111it [08:15,  3.96s/it]112it [08:19,  4.01s/it]113it [08:24,  4.23s/it]114it [08:28,  4.19s/it]115it [08:32,  4.27s/it]116it [08:37,  4.52s/it]117it [08:42,  4.66s/it]118it [08:48,  4.89s/it]119it [08:53,  5.01s/it]120it [08:58,  4.98s/it]121it [09:03,  4.96s/it]122it [09:08,  5.08s/it]123it [09:13,  5.15s/it]124it [09:19,  5.22s/it]125it [09:24,  5.15s/it]126it [09:29,  5.13s/it]127it [09:34,  4.98s/it]128it [09:39,  5.07s/it]129it [09:44,  5.03s/it]130it [09:49,  5.02s/it]131it [09:54,  4.98s/it]132it [09:59,  5.05s/it]133it [10:04,  5.12s/it]134it [10:09,  5.19s/it]135it [10:14,  5.08s/it]136it [10:20,  5.12s/it]137it [10:25,  5.08s/it]138it [10:30,  5.13s/it]139it [10:35,  5.13s/it]140it [10:40,  5.17s/it]141it [10:46,  5.26s/it]142it [10:51,  5.26s/it]143it [10:56,  5.17s/it]144it [11:00,  4.86s/it]145it [11:05,  4.99s/it]146it [11:10,  4.80s/it]147it [11:15,  4.95s/it]148it [11:20,  4.89s/it]149it [11:24,  4.86s/it]150it [11:29,  4.71s/it]151it [11:33,  4.60s/it]152it [11:37,  4.45s/it]153it [11:42,  4.60s/it]154it [11:47,  4.52s/it]155it [11:51,  4.57s/it]156it [11:55,  4.46s/it]157it [12:00,  4.36s/it]158it [12:04,  4.42s/it]159it [12:09,  4.54s/it]160it [12:14,  4.77s/it]161it [12:20,  5.10s/it]162it [12:26,  5.19s/it]163it [12:31,  5.17s/it]164it [12:36,  5.13s/it]165it [12:41,  5.24s/it]166it [12:47,  5.31s/it]167it [12:53,  5.52s/it]168it [12:58,  5.54s/it]169it [13:03,  5.29s/it]170it [13:08,  5.31s/it]171it [13:14,  5.35s/it]172it [13:19,  5.39s/it]173it [13:25,  5.50s/it]174it [13:30,  5.42s/it]175it [13:35,  5.35s/it]176it [13:41,  5.28s/it]177it [13:46,  5.30s/it]178it [13:51,  5.29s/it]179it [13:57,  5.36s/it]180it [14:02,  5.34s/it]181it [14:08,  5.43s/it]182it [14:13,  5.51s/it]183it [14:18,  5.36s/it]184it [14:23,  5.28s/it]185it [14:29,  5.38s/it]186it [14:34,  5.32s/it]187it [14:39,  5.23s/it]188it [14:43,  4.83s/it]189it [14:48,  4.76s/it]190it [14:52,  4.59s/it]191it [14:57,  4.72s/it]192it [15:01,  4.60s/it]193it [15:05,  4.43s/it]194it [15:09,  4.23s/it]195it [15:14,  4.42s/it]196it [15:18,  4.34s/it]197it [15:23,  4.50s/it]198it [15:26,  4.19s/it]199it [15:30,  4.06s/it]200it [15:36,  4.55s/it]201it [15:41,  4.66s/it]202it [15:46,  4.83s/it]203it [15:51,  4.84s/it]204it [15:56,  5.01s/it]205it [16:01,  5.03s/it]206it [16:06,  5.05s/it]207it [16:12,  5.11s/it]208it [16:17,  5.25s/it]209it [16:23,  5.28s/it]210it [16:28,  5.39s/it]211it [16:34,  5.35s/it]212it [16:39,  5.34s/it]213it [16:44,  5.27s/it]214it [16:49,  5.19s/it]215it [16:55,  5.33s/it]216it [17:00,  5.32s/it]217it [17:05,  5.22s/it]218it [17:10,  5.17s/it]219it [17:15,  5.20s/it]220it [17:20,  5.21s/it]221it [17:26,  5.23s/it]222it [17:31,  5.26s/it]223it [17:36,  5.25s/it]224it [17:41,  5.08s/it]225it [17:46,  5.11s/it]226it [17:51,  5.17s/it]227it [17:57,  5.27s/it]228it [18:02,  5.07s/it]229it [18:07,  5.13s/it]229it [18:07,  4.75s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:06<02:47,  6.97s/it]  8%|▊         | 2/25 [00:07<01:15,  3.27s/it] 12%|█▏        | 3/25 [00:07<00:41,  1.88s/it] 16%|█▌        | 4/25 [00:08<00:28,  1.35s/it] 20%|██        | 5/25 [00:08<00:18,  1.06it/s] 24%|██▍       | 6/25 [00:09<00:16,  1.14it/s] 28%|██▊       | 7/25 [00:09<00:11,  1.52it/s] 32%|███▏      | 8/25 [00:09<00:08,  1.94it/s] 36%|███▌      | 9/25 [00:10<00:06,  2.38it/s] 40%|████      | 10/25 [00:10<00:05,  2.77it/s] 44%|████▍     | 11/25 [00:10<00:04,  3.04it/s] 48%|████▊     | 12/25 [00:10<00:03,  3.36it/s] 52%|█████▏    | 13/25 [00:10<00:03,  3.53it/s] 56%|█████▌    | 14/25 [00:11<00:02,  3.74it/s] 60%|██████    | 15/25 [00:11<00:02,  3.84it/s] 64%|██████▍   | 16/25 [00:11<00:02,  3.93it/s] 68%|██████▊   | 17/25 [00:11<00:01,  4.15it/s] 72%|███████▏  | 18/25 [00:12<00:01,  4.32it/s] 76%|███████▌  | 19/25 [00:12<00:01,  4.33it/s] 80%|████████  | 20/25 [00:12<00:01,  4.31it/s] 84%|████████▍ | 21/25 [00:12<00:00,  4.27it/s] 88%|████████▊ | 22/25 [00:13<00:00,  4.41it/s] 92%|█████████▏| 23/25 [00:13<00:00,  4.46it/s] 96%|█████████▌| 24/25 [00:13<00:00,  4.50it/s]100%|██████████| 25/25 [00:14<00:00,  1.76it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288027763367
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:07<03:01,  7.54s/it]  8%|▊         | 2/25 [00:07<01:14,  3.23s/it] 12%|█▏        | 3/25 [00:08<00:44,  2.03s/it] 16%|█▌        | 4/25 [00:08<00:27,  1.31s/it] 20%|██        | 5/25 [00:08<00:18,  1.08it/s] 24%|██▍       | 6/25 [00:09<00:13,  1.46it/s] 28%|██▊       | 7/25 [00:09<00:09,  1.89it/s] 32%|███▏      | 8/25 [00:09<00:07,  2.35it/s] 36%|███▌      | 9/25 [00:09<00:05,  2.80it/s] 40%|████      | 10/25 [00:09<00:04,  3.22it/s] 44%|████▍     | 11/25 [00:10<00:03,  3.58it/s] 48%|████▊     | 12/25 [00:10<00:03,  3.73it/s] 52%|█████▏    | 13/25 [00:10<00:03,  3.91it/s] 56%|█████▌    | 14/25 [00:10<00:02,  4.12it/s] 60%|██████    | 15/25 [00:10<00:02,  4.25it/s] 64%|██████▍   | 16/25 [00:11<00:02,  4.38it/s] 68%|██████▊   | 17/25 [00:11<00:02,  2.68it/s] 72%|███████▏  | 18/25 [00:12<00:02,  3.07it/s] 76%|███████▌  | 19/25 [00:14<00:04,  1.22it/s] 80%|████████  | 20/25 [00:14<00:03,  1.58it/s] 84%|████████▍ | 21/25 [00:14<00:02,  1.98it/s] 88%|████████▊ | 22/25 [00:14<00:01,  2.41it/s] 92%|█████████▏| 23/25 [00:14<00:00,  2.81it/s] 96%|█████████▌| 24/25 [00:15<00:00,  3.19it/s]100%|██████████| 25/25 [00:15<00:00,  1.61it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.47649112343788147


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.81288146972656
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 57.554073833143605
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 75.0141154353219
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 52.49025800974748


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.649112701416016
=========================          END          =========================
0it [00:00, ?it/s]1it [00:05,  5.28s/it]2it [00:10,  4.95s/it]3it [00:14,  4.61s/it]4it [00:18,  4.43s/it]5it [00:22,  4.44s/it]6it [00:27,  4.52s/it]7it [00:32,  4.57s/it]8it [00:36,  4.55s/it]9it [00:41,  4.51s/it]10it [00:45,  4.43s/it]11it [00:50,  4.56s/it]12it [00:54,  4.51s/it]13it [00:59,  4.54s/it]14it [01:03,  4.45s/it]15it [01:07,  4.27s/it]16it [01:11,  4.31s/it]17it [01:16,  4.41s/it]18it [01:21,  4.49s/it]19it [01:26,  4.75s/it]20it [01:31,  4.79s/it]21it [01:36,  4.92s/it]22it [01:41,  4.97s/it]23it [01:46,  4.99s/it]24it [01:51,  5.07s/it]25it [01:56,  4.97s/it]26it [02:01,  4.98s/it]27it [02:06,  5.03s/it]28it [02:11,  5.06s/it]29it [02:16,  5.03s/it]30it [02:21,  5.03s/it]31it [02:26,  5.04s/it]32it [02:31,  4.93s/it]33it [02:36,  4.99s/it]34it [02:41,  4.95s/it]35it [02:46,  4.86s/it]36it [02:50,  4.81s/it]37it [02:56,  4.94s/it]38it [03:00,  4.90s/it]39it [03:06,  4.98s/it]40it [03:11,  4.98s/it]41it [03:16,  5.00s/it]42it [03:21,  5.22s/it]43it [03:27,  5.21s/it]44it [03:31,  5.02s/it]45it [03:36,  4.94s/it]46it [03:41,  4.97s/it]47it [03:46,  4.95s/it]48it [03:51,  5.08s/it]49it [03:56,  5.12s/it]50it [04:01,  5.03s/it]51it [04:07,  5.08s/it]52it [04:12,  5.06s/it]53it [04:16,  4.99s/it]54it [04:21,  4.81s/it]55it [04:25,  4.66s/it]56it [04:29,  4.41s/it]57it [04:33,  4.30s/it]58it [04:37,  4.27s/it]59it [04:41,  4.24s/it]60it [04:46,  4.27s/it]61it [04:50,  4.36s/it]62it [04:55,  4.39s/it]63it [04:59,  4.48s/it]64it [05:03,  4.16s/it]65it [05:05,  3.69s/it]66it [05:09,  3.54s/it]67it [05:12,  3.41s/it]68it [05:15,  3.34s/it]69it [05:18,  3.39s/it]70it [05:22,  3.33s/it]71it [05:24,  2.99s/it]72it [05:26,  2.87s/it]73it [05:29,  2.87s/it]74it [05:32,  2.80s/it]75it [05:34,  2.65s/it]76it [05:36,  2.54s/it]77it [05:39,  2.59s/it]78it [05:42,  2.68s/it]79it [05:44,  2.56s/it]80it [05:47,  2.61s/it]81it [05:50,  2.70s/it]82it [05:53,  2.79s/it]83it [05:56,  2.93s/it]84it [05:59,  2.77s/it]85it [06:01,  2.63s/it]86it [06:03,  2.53s/it]87it [06:06,  2.56s/it]88it [06:09,  2.78s/it]89it [06:12,  2.80s/it]90it [06:15,  2.99s/it]91it [06:20,  3.35s/it]92it [06:23,  3.35s/it]93it [06:26,  3.18s/it]94it [06:28,  3.03s/it]95it [06:31,  2.94s/it]96it [06:33,  2.76s/it]97it [06:36,  2.79s/it]98it [06:39,  2.77s/it]99it [06:42,  2.79s/it]100it [06:44,  2.66s/it]101it [06:47,  2.66s/it]102it [06:50,  2.89s/it]103it [06:54,  3.12s/it]104it [06:57,  3.20s/it]105it [07:00,  3.07s/it]106it [07:03,  3.05s/it]107it [07:06,  3.00s/it]108it [07:10,  3.15s/it]109it [07:13,  3.24s/it]110it [07:16,  3.19s/it]111it [07:19,  3.06s/it]112it [07:21,  2.80s/it]113it [07:23,  2.59s/it]114it [07:26,  2.67s/it]115it [07:29,  2.78s/it]116it [07:32,  2.82s/it]117it [07:35,  2.91s/it]118it [07:38,  2.95s/it]119it [07:40,  2.75s/it]120it [07:44,  2.96s/it]121it [07:47,  2.92s/it]122it [07:50,  2.97s/it]123it [07:52,  2.85s/it]124it [07:55,  2.75s/it]125it [07:58,  2.85s/it]126it [08:02,  3.17s/it]127it [08:05,  3.29s/it]128it [08:09,  3.47s/it]129it [08:13,  3.46s/it]130it [08:16,  3.48s/it]131it [08:19,  3.31s/it]132it [08:22,  3.29s/it]133it [08:25,  3.25s/it]134it [08:29,  3.25s/it]135it [08:33,  3.44s/it]136it [08:37,  3.66s/it]137it [08:41,  3.79s/it]138it [08:44,  3.63s/it]139it [08:48,  3.83s/it]140it [08:52,  3.82s/it]141it [08:56,  3.82s/it]142it [09:00,  3.92s/it]143it [09:04,  3.96s/it]144it [09:08,  4.00s/it]145it [09:12,  4.02s/it]146it [09:16,  3.78s/it]147it [09:19,  3.52s/it]148it [09:22,  3.37s/it]149it [09:24,  3.21s/it]150it [09:27,  3.08s/it]151it [09:31,  3.16s/it]152it [09:34,  3.23s/it]153it [09:37,  3.19s/it]154it [09:41,  3.35s/it]155it [09:44,  3.25s/it]156it [09:47,  3.14s/it]157it [09:50,  3.15s/it]158it [09:53,  3.10s/it]159it [09:56,  3.23s/it]160it [10:00,  3.39s/it]161it [10:04,  3.44s/it]162it [10:07,  3.35s/it]163it [10:09,  3.14s/it]164it [10:12,  3.07s/it]165it [10:15,  3.07s/it]166it [10:18,  2.99s/it]167it [10:21,  2.91s/it]168it [10:24,  2.96s/it]169it [10:26,  2.79s/it]170it [10:29,  2.78s/it]171it [10:32,  2.91s/it]172it [10:35,  2.95s/it]173it [10:38,  2.81s/it]174it [10:41,  2.76s/it]175it [10:43,  2.65s/it]176it [10:46,  2.69s/it]177it [10:49,  2.72s/it]178it [10:51,  2.75s/it]179it [10:54,  2.71s/it]180it [10:56,  2.58s/it]181it [10:59,  2.60s/it]182it [11:01,  2.52s/it]183it [11:04,  2.59s/it]184it [11:07,  2.57s/it]185it [11:09,  2.68s/it]186it [11:12,  2.67s/it]187it [11:15,  2.69s/it]188it [11:17,  2.63s/it]189it [11:20,  2.58s/it]190it [11:23,  2.63s/it]191it [11:26,  2.78s/it]192it [11:29,  2.82s/it]193it [11:31,  2.82s/it]194it [11:34,  2.84s/it]195it [11:37,  2.82s/it]196it [11:40,  2.92s/it]197it [11:43,  2.84s/it]198it [11:46,  2.78s/it]199it [11:48,  2.63s/it]200it [11:50,  2.50s/it]201it [11:52,  2.35s/it]202it [11:54,  2.34s/it]203it [11:57,  2.37s/it]204it [11:59,  2.35s/it]205it [12:01,  2.28s/it]206it [12:03,  2.25s/it]207it [12:06,  2.27s/it]208it [12:09,  2.45s/it]209it [12:11,  2.46s/it]210it [12:13,  2.43s/it]211it [12:16,  2.45s/it]212it [12:18,  2.42s/it]213it [12:20,  2.34s/it]214it [12:23,  2.47s/it]215it [12:25,  2.39s/it]216it [12:28,  2.41s/it]217it [12:30,  2.49s/it]218it [12:33,  2.41s/it]219it [12:35,  2.25s/it]220it [12:37,  2.21s/it]221it [12:39,  2.35s/it]222it [12:41,  2.26s/it]223it [12:43,  2.09s/it]224it [12:45,  1.96s/it]225it [12:47,  1.94s/it]226it [12:49,  1.92s/it]227it [12:50,  1.88s/it]228it [12:52,  1.88s/it]229it [12:54,  1.79s/it]229it [12:54,  3.38s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:13<05:28, 13.69s/it]  8%|▊         | 2/25 [00:14<02:13,  5.83s/it] 12%|█▏        | 3/25 [00:14<01:11,  3.26s/it] 16%|█▌        | 4/25 [00:14<00:43,  2.06s/it] 20%|██        | 5/25 [00:14<00:27,  1.39s/it] 24%|██▍       | 6/25 [00:14<00:18,  1.01it/s] 28%|██▊       | 7/25 [00:15<00:16,  1.11it/s] 32%|███▏      | 8/25 [00:15<00:11,  1.47it/s] 36%|███▌      | 9/25 [00:15<00:08,  1.87it/s] 40%|████      | 10/25 [00:16<00:06,  2.31it/s] 44%|████▍     | 11/25 [00:16<00:05,  2.73it/s] 48%|████▊     | 12/25 [00:16<00:04,  3.12it/s] 52%|█████▏    | 13/25 [00:16<00:03,  3.49it/s] 56%|█████▌    | 14/25 [00:17<00:02,  3.81it/s] 60%|██████    | 15/25 [00:17<00:02,  4.08it/s] 64%|██████▍   | 16/25 [00:17<00:02,  4.29it/s] 68%|██████▊   | 17/25 [00:24<00:18,  2.34s/it] 72%|███████▏  | 18/25 [00:24<00:11,  1.70s/it] 76%|███████▌  | 19/25 [00:25<00:07,  1.25s/it] 80%|████████  | 20/25 [00:25<00:04,  1.07it/s] 84%|████████▍ | 21/25 [00:25<00:02,  1.39it/s] 88%|████████▊ | 22/25 [00:25<00:01,  1.77it/s] 92%|█████████▏| 23/25 [00:28<00:02,  1.12s/it] 96%|█████████▌| 24/25 [00:28<00:00,  1.18it/s]100%|██████████| 25/25 [00:28<00:00,  1.15s/it]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288623809814
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:06<02:45,  6.88s/it]  8%|▊         | 2/25 [00:07<01:12,  3.15s/it] 12%|█▏        | 3/25 [00:07<00:41,  1.88s/it] 16%|█▌        | 4/25 [00:07<00:25,  1.22s/it] 20%|██        | 5/25 [00:08<00:17,  1.17it/s] 24%|██▍       | 6/25 [00:08<00:12,  1.58it/s] 28%|██▊       | 7/25 [00:08<00:08,  2.02it/s] 32%|███▏      | 8/25 [00:08<00:06,  2.48it/s] 36%|███▌      | 9/25 [00:09<00:05,  2.91it/s] 40%|████      | 10/25 [00:09<00:04,  3.32it/s] 44%|████▍     | 11/25 [00:09<00:03,  3.64it/s] 48%|████▊     | 12/25 [00:09<00:03,  3.93it/s] 52%|█████▏    | 13/25 [00:09<00:02,  4.14it/s] 56%|█████▌    | 14/25 [00:10<00:02,  4.33it/s] 60%|██████    | 15/25 [00:10<00:02,  4.40it/s] 64%|██████▍   | 16/25 [00:10<00:01,  4.53it/s] 68%|██████▊   | 17/25 [00:10<00:01,  4.63it/s] 72%|███████▏  | 18/25 [00:10<00:01,  4.70it/s] 76%|███████▌  | 19/25 [00:11<00:01,  4.74it/s] 80%|████████  | 20/25 [00:11<00:01,  4.73it/s] 84%|████████▍ | 21/25 [00:11<00:00,  4.72it/s] 88%|████████▊ | 22/25 [00:11<00:00,  4.64it/s] 92%|█████████▏| 23/25 [00:11<00:00,  4.68it/s] 96%|█████████▌| 24/25 [00:12<00:00,  4.72it/s]100%|██████████| 25/25 [00:12<00:00,  1.99it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.4763534367084503


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.8128890991211
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 58.07448365587901
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.90369709865593
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 52.425823096859304


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.635345458984375
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.35s/it]2it [00:04,  2.30s/it]3it [00:06,  2.29s/it]4it [00:09,  2.22s/it]5it [00:10,  2.13s/it]6it [00:13,  2.22s/it]7it [00:15,  2.17s/it]8it [00:17,  2.15s/it]9it [00:19,  2.17s/it]10it [00:22,  2.22s/it]11it [00:24,  2.18s/it]12it [00:26,  2.14s/it]13it [00:28,  2.12s/it]14it [00:30,  2.00s/it]15it [00:31,  1.94s/it]16it [00:33,  1.91s/it]17it [00:35,  1.78s/it]18it [00:37,  1.82s/it]19it [00:39,  1.87s/it]20it [00:41,  2.01s/it]21it [00:43,  1.95s/it]22it [00:45,  2.05s/it]23it [00:47,  2.07s/it]24it [00:49,  2.13s/it]25it [00:52,  2.21s/it]26it [00:54,  2.14s/it]27it [00:57,  2.33s/it]28it [00:58,  2.22s/it]29it [01:00,  2.10s/it]30it [01:02,  1.96s/it]31it [01:04,  1.92s/it]32it [01:06,  1.94s/it]33it [01:08,  2.01s/it]34it [01:10,  2.03s/it]35it [01:11,  1.73s/it]36it [01:13,  1.76s/it]37it [01:15,  1.77s/it]38it [01:17,  1.81s/it]39it [01:19,  1.89s/it]40it [01:21,  1.89s/it]41it [01:22,  1.88s/it]42it [01:24,  1.90s/it]43it [01:26,  1.97s/it]44it [01:28,  1.98s/it]45it [01:30,  1.93s/it]46it [01:32,  1.97s/it]47it [01:34,  2.01s/it]48it [01:36,  1.89s/it]49it [01:38,  1.94s/it]50it [01:41,  2.09s/it]51it [01:43,  2.09s/it]52it [01:45,  2.05s/it]53it [01:47,  2.12s/it]54it [01:48,  1.84s/it]55it [01:50,  1.94s/it]56it [01:52,  1.96s/it]57it [01:55,  2.06s/it]58it [01:56,  2.01s/it]59it [01:58,  2.00s/it]60it [02:01,  2.04s/it]61it [02:03,  2.11s/it]62it [02:05,  2.13s/it]63it [02:06,  1.91s/it]64it [02:08,  1.95s/it]65it [02:10,  1.95s/it]66it [02:13,  2.09s/it]67it [02:15,  2.03s/it]68it [02:16,  1.93s/it]69it [02:19,  2.00s/it]70it [02:21,  2.09s/it]71it [02:23,  2.00s/it]72it [02:25,  2.04s/it]73it [02:26,  1.71s/it]74it [02:27,  1.50s/it]75it [02:29,  1.74s/it]76it [02:31,  1.96s/it]77it [02:33,  1.89s/it]78it [02:35,  1.94s/it]79it [02:37,  1.97s/it]80it [02:39,  1.99s/it]81it [02:41,  2.01s/it]82it [02:44,  2.05s/it]83it [02:46,  2.06s/it]84it [02:48,  2.04s/it]85it [02:50,  2.01s/it]86it [02:52,  2.14s/it]87it [02:54,  2.09s/it]88it [02:55,  1.80s/it]89it [02:57,  1.85s/it]90it [02:59,  1.77s/it]91it [03:03,  2.44s/it]92it [03:04,  2.16s/it]93it [03:06,  2.07s/it]94it [03:08,  2.01s/it]95it [03:10,  1.92s/it]96it [03:12,  1.98s/it]97it [03:14,  1.99s/it]98it [03:17,  2.35s/it]99it [03:19,  2.25s/it]100it [03:21,  2.19s/it]101it [03:23,  2.12s/it]102it [03:25,  2.07s/it]103it [03:26,  1.91s/it]104it [03:28,  1.96s/it]105it [03:30,  1.85s/it]106it [03:32,  1.77s/it]107it [03:33,  1.73s/it]108it [03:35,  1.73s/it]109it [03:37,  1.79s/it]110it [03:39,  1.76s/it]111it [03:40,  1.71s/it]112it [03:41,  1.54s/it]113it [03:42,  1.40s/it]114it [03:44,  1.37s/it]115it [03:45,  1.42s/it]116it [03:47,  1.61s/it]117it [03:49,  1.66s/it]118it [03:51,  1.76s/it]119it [03:53,  1.78s/it]120it [03:55,  1.72s/it]121it [03:57,  1.82s/it]122it [03:58,  1.83s/it]123it [04:00,  1.85s/it]124it [04:02,  1.86s/it]125it [04:04,  1.79s/it]126it [04:06,  1.88s/it]127it [04:08,  1.88s/it]128it [04:09,  1.73s/it]129it [04:11,  1.75s/it]130it [04:13,  1.75s/it]131it [04:14,  1.72s/it]132it [04:17,  1.86s/it]133it [04:18,  1.81s/it]134it [04:20,  1.85s/it]135it [04:22,  1.81s/it]136it [04:24,  1.77s/it]137it [04:25,  1.70s/it]138it [04:27,  1.79s/it]139it [04:29,  1.85s/it]140it [04:31,  1.87s/it]141it [04:33,  1.75s/it]142it [04:35,  1.85s/it]143it [04:37,  1.86s/it]144it [04:38,  1.74s/it]145it [04:39,  1.67s/it]146it [04:41,  1.67s/it]147it [04:44,  1.94s/it]148it [04:47,  2.24s/it]149it [04:49,  2.34s/it]150it [04:51,  2.26s/it]151it [04:54,  2.31s/it]152it [04:56,  2.34s/it]153it [04:59,  2.44s/it]154it [05:02,  2.67s/it]155it [05:05,  2.81s/it]156it [05:08,  2.87s/it]157it [05:11,  2.92s/it]158it [05:14,  2.88s/it]159it [05:17,  2.87s/it]160it [05:20,  2.94s/it]161it [05:23,  3.01s/it]162it [05:26,  2.97s/it]163it [05:28,  2.80s/it]164it [05:31,  2.76s/it]165it [05:33,  2.63s/it]166it [05:36,  2.55s/it]167it [05:38,  2.54s/it]168it [05:40,  2.41s/it]169it [05:43,  2.34s/it]170it [05:45,  2.28s/it]171it [05:47,  2.22s/it]172it [05:49,  2.16s/it]173it [05:51,  2.16s/it]174it [05:54,  2.39s/it]175it [05:56,  2.31s/it]176it [05:58,  2.29s/it]177it [06:01,  2.30s/it]178it [06:03,  2.28s/it]179it [06:05,  2.24s/it]180it [06:07,  2.15s/it]181it [06:09,  2.12s/it]182it [06:11,  2.16s/it]183it [06:13,  2.14s/it]184it [06:15,  2.13s/it]185it [06:18,  2.18s/it]186it [06:20,  2.36s/it]187it [06:23,  2.28s/it]188it [06:25,  2.26s/it]189it [06:27,  2.24s/it]190it [06:29,  2.24s/it]191it [06:31,  2.20s/it]192it [06:34,  2.28s/it]193it [06:36,  2.36s/it]194it [06:39,  2.43s/it]195it [06:42,  2.52s/it]196it [06:44,  2.53s/it]197it [06:46,  2.43s/it]198it [06:49,  2.33s/it]199it [06:51,  2.28s/it]200it [06:53,  2.36s/it]201it [06:56,  2.36s/it]202it [06:58,  2.43s/it]203it [07:01,  2.59s/it]204it [07:04,  2.57s/it]205it [07:06,  2.42s/it]206it [07:08,  2.40s/it]207it [07:10,  2.30s/it]208it [07:12,  2.29s/it]209it [07:15,  2.32s/it]210it [07:17,  2.23s/it]211it [07:19,  2.17s/it]212it [07:21,  2.12s/it]213it [07:23,  2.03s/it]214it [07:25,  2.13s/it]215it [07:27,  2.13s/it]216it [07:29,  2.18s/it]217it [07:32,  2.28s/it]218it [07:34,  2.19s/it]219it [07:36,  2.11s/it]220it [07:38,  2.12s/it]221it [07:40,  2.22s/it]222it [07:43,  2.21s/it]223it [07:45,  2.11s/it]224it [07:47,  2.07s/it]225it [07:49,  2.06s/it]226it [07:50,  2.01s/it]227it [07:53,  2.05s/it]228it [07:55,  2.05s/it]229it [07:57,  2.13s/it]229it [07:57,  2.08s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:05<02:17,  5.75s/it]  8%|▊         | 2/25 [00:06<01:03,  2.76s/it] 12%|█▏        | 3/25 [00:06<00:35,  1.60s/it] 16%|█▌        | 4/25 [00:06<00:21,  1.05s/it] 20%|██        | 5/25 [00:07<00:14,  1.34it/s] 24%|██▍       | 6/25 [00:07<00:10,  1.78it/s] 28%|██▊       | 7/25 [00:07<00:08,  2.25it/s] 32%|███▏      | 8/25 [00:07<00:06,  2.71it/s] 36%|███▌      | 9/25 [00:07<00:05,  3.15it/s] 40%|████      | 10/25 [00:08<00:04,  3.53it/s] 44%|████▍     | 11/25 [00:08<00:03,  3.85it/s] 48%|████▊     | 12/25 [00:08<00:03,  4.11it/s] 52%|█████▏    | 13/25 [00:08<00:02,  4.31it/s] 56%|█████▌    | 14/25 [00:08<00:02,  4.47it/s] 60%|██████    | 15/25 [00:09<00:02,  4.58it/s] 64%|██████▍   | 16/25 [00:09<00:01,  4.66it/s] 68%|██████▊   | 17/25 [00:09<00:01,  4.72it/s] 72%|███████▏  | 18/25 [00:09<00:01,  4.75it/s] 76%|███████▌  | 19/25 [00:09<00:01,  4.79it/s] 80%|████████  | 20/25 [00:10<00:01,  4.82it/s] 84%|████████▍ | 21/25 [00:10<00:00,  4.84it/s] 88%|████████▊ | 22/25 [00:10<00:00,  4.86it/s] 92%|█████████▏| 23/25 [00:10<00:00,  4.87it/s] 96%|█████████▌| 24/25 [00:10<00:00,  4.86it/s]100%|██████████| 25/25 [00:11<00:00,  2.24it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288027763367
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:06<02:47,  6.99s/it]  8%|▊         | 2/25 [00:07<01:09,  3.00s/it] 12%|█▏        | 3/25 [00:07<00:38,  1.73s/it] 16%|█▌        | 4/25 [00:07<00:23,  1.13s/it] 20%|██        | 5/25 [00:07<00:15,  1.26it/s] 24%|██▍       | 6/25 [00:08<00:11,  1.68it/s] 28%|██▊       | 7/25 [00:08<00:08,  2.13it/s] 32%|███▏      | 8/25 [00:08<00:06,  2.60it/s] 36%|███▌      | 9/25 [00:08<00:05,  3.00it/s] 40%|████      | 10/25 [00:08<00:04,  3.40it/s] 44%|████▍     | 11/25 [00:09<00:03,  3.75it/s] 48%|████▊     | 12/25 [00:09<00:03,  4.03it/s] 52%|█████▏    | 13/25 [00:09<00:02,  4.24it/s] 56%|█████▌    | 14/25 [00:09<00:02,  4.36it/s] 60%|██████    | 15/25 [00:09<00:02,  4.44it/s] 64%|██████▍   | 16/25 [00:10<00:01,  4.57it/s] 68%|██████▊   | 17/25 [00:10<00:01,  4.65it/s] 72%|███████▏  | 18/25 [00:10<00:01,  4.71it/s] 76%|███████▌  | 19/25 [00:10<00:01,  4.76it/s] 80%|████████  | 20/25 [00:10<00:01,  4.77it/s] 84%|████████▍ | 21/25 [00:11<00:00,  4.78it/s] 88%|████████▊ | 22/25 [00:11<00:00,  4.81it/s] 92%|█████████▏| 23/25 [00:11<00:00,  4.82it/s] 96%|█████████▌| 24/25 [00:11<00:00,  4.82it/s]100%|██████████| 25/25 [00:12<00:00,  2.07it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.4715315103530884


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.81288146972656
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 57.001138396487235
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.38486011067901
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 50.96357745894372


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.15315246582031
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.41s/it]2it [00:04,  2.32s/it]3it [00:06,  2.26s/it]4it [00:08,  2.18s/it]5it [00:11,  2.26s/it]6it [00:13,  2.20s/it]7it [00:15,  2.15s/it]8it [00:17,  2.18s/it]9it [00:20,  2.25s/it]10it [00:22,  2.25s/it]11it [00:24,  2.19s/it]12it [00:26,  2.24s/it]13it [00:28,  2.24s/it]14it [00:31,  2.19s/it]15it [00:33,  2.25s/it]16it [00:35,  2.09s/it]17it [00:37,  2.11s/it]18it [00:39,  2.01s/it]19it [00:41,  2.02s/it]20it [00:43,  1.98s/it]21it [00:44,  1.92s/it]22it [00:47,  2.03s/it]23it [00:49,  2.02s/it]24it [00:51,  2.09s/it]25it [00:53,  2.04s/it]26it [00:55,  2.02s/it]27it [00:56,  1.93s/it]28it [00:58,  1.76s/it]29it [00:59,  1.72s/it]30it [01:02,  1.83s/it]31it [01:04,  1.97s/it]32it [01:06,  2.10s/it]33it [01:08,  2.05s/it]34it [01:10,  1.88s/it]35it [01:11,  1.82s/it]36it [01:14,  1.92s/it]37it [01:15,  1.85s/it]38it [01:17,  1.81s/it]39it [01:18,  1.72s/it]40it [01:20,  1.78s/it]41it [01:22,  1.78s/it]42it [01:24,  1.71s/it]43it [01:25,  1.70s/it]44it [01:27,  1.70s/it]45it [01:29,  1.72s/it]46it [01:31,  1.71s/it]47it [01:32,  1.78s/it]48it [01:34,  1.86s/it]49it [01:36,  1.82s/it]50it [01:38,  1.77s/it]51it [01:39,  1.72s/it]52it [01:41,  1.73s/it]53it [01:43,  1.84s/it]54it [01:45,  1.90s/it]55it [01:47,  1.89s/it]56it [01:49,  1.87s/it]57it [01:51,  1.75s/it]58it [01:52,  1.78s/it]59it [01:54,  1.79s/it]60it [01:56,  1.76s/it]61it [01:58,  1.78s/it]62it [01:59,  1.78s/it]63it [02:01,  1.73s/it]64it [02:03,  1.69s/it]65it [02:04,  1.66s/it]66it [02:06,  1.59s/it]67it [02:07,  1.61s/it]68it [02:09,  1.60s/it]69it [02:11,  1.60s/it]70it [02:13,  1.75s/it]71it [02:15,  1.84s/it]72it [02:17,  1.91s/it]73it [02:19,  1.93s/it]74it [02:20,  1.87s/it]75it [02:22,  1.84s/it]76it [02:24,  1.77s/it]77it [02:25,  1.61s/it]78it [02:27,  1.61s/it]79it [02:28,  1.63s/it]80it [02:30,  1.65s/it]81it [02:32,  1.68s/it]82it [02:34,  1.71s/it]83it [02:36,  1.82s/it]84it [02:38,  1.88s/it]85it [02:40,  2.00s/it]86it [02:42,  2.02s/it]87it [02:45,  2.15s/it]88it [02:47,  2.29s/it]89it [02:50,  2.42s/it]90it [02:52,  2.31s/it]91it [02:54,  2.14s/it]92it [02:56,  2.12s/it]93it [02:58,  2.03s/it]94it [03:00,  2.07s/it]95it [03:02,  2.03s/it]96it [03:04,  1.98s/it]97it [03:05,  1.84s/it]98it [03:07,  1.92s/it]99it [03:09,  2.01s/it]100it [03:12,  2.08s/it]101it [03:14,  2.09s/it]102it [03:16,  2.04s/it]103it [03:17,  1.93s/it]104it [03:19,  1.87s/it]105it [03:21,  1.91s/it]106it [03:23,  1.93s/it]107it [03:25,  1.91s/it]108it [03:27,  1.96s/it]109it [03:29,  1.91s/it]110it [03:31,  1.89s/it]111it [03:32,  1.87s/it]112it [03:35,  1.95s/it]113it [03:36,  1.91s/it]114it [03:38,  1.84s/it]115it [03:39,  1.70s/it]116it [03:41,  1.67s/it]117it [03:43,  1.72s/it]118it [03:45,  1.78s/it]119it [03:47,  1.86s/it]120it [03:49,  1.89s/it]121it [03:51,  1.92s/it]122it [03:52,  1.83s/it]123it [03:54,  1.71s/it]124it [03:55,  1.63s/it]125it [03:57,  1.63s/it]126it [03:58,  1.61s/it]127it [04:00,  1.59s/it]128it [04:01,  1.49s/it]129it [04:03,  1.47s/it]130it [04:04,  1.46s/it]131it [04:06,  1.48s/it]132it [04:07,  1.45s/it]133it [04:09,  1.50s/it]134it [04:10,  1.46s/it]135it [04:11,  1.39s/it]136it [04:13,  1.40s/it]137it [04:14,  1.40s/it]138it [04:15,  1.38s/it]139it [04:17,  1.41s/it]140it [04:18,  1.42s/it]141it [04:20,  1.39s/it]142it [04:21,  1.41s/it]143it [04:23,  1.43s/it]144it [04:24,  1.41s/it]145it [04:25,  1.43s/it]146it [04:27,  1.41s/it]147it [04:28,  1.37s/it]148it [04:29,  1.34s/it]149it [04:31,  1.40s/it]150it [04:32,  1.40s/it]151it [04:34,  1.48s/it]152it [04:35,  1.46s/it]153it [04:37,  1.46s/it]154it [04:38,  1.43s/it]155it [04:40,  1.47s/it]156it [04:41,  1.50s/it]157it [04:43,  1.52s/it]158it [04:44,  1.42s/it]159it [04:45,  1.26s/it]160it [04:46,  1.07s/it]161it [04:46,  1.02s/it]162it [04:47,  1.03it/s]163it [04:48,  1.03it/s]164it [04:49,  1.01s/it]165it [04:50,  1.02s/it]166it [04:52,  1.06s/it]167it [04:53,  1.09s/it]168it [04:54,  1.06s/it]169it [04:55,  1.03s/it]170it [04:55,  1.05it/s]171it [04:56,  1.11it/s]172it [04:57,  1.14it/s]173it [04:58,  1.14it/s]174it [04:59,  1.19it/s]175it [05:00,  1.15it/s]176it [05:01,  1.01s/it]177it [05:02,  1.03s/it]178it [05:03,  1.00it/s]179it [05:04,  1.02s/it]180it [05:05,  1.04s/it]181it [05:06,  1.03s/it]182it [05:07,  1.02s/it]183it [05:08,  1.00it/s]184it [05:09,  1.01it/s]185it [05:10,  1.01it/s]186it [05:11,  1.01it/s]187it [05:12,  1.02it/s]188it [05:13,  1.04it/s]189it [05:14,  1.08it/s]190it [05:15,  1.10it/s]191it [05:16,  1.08it/s]192it [05:17,  1.06it/s]193it [05:18,  1.05it/s]194it [05:19,  1.03it/s]195it [05:20,  1.00it/s]196it [05:21,  1.04s/it]197it [05:22,  1.07s/it]198it [05:23,  1.02s/it]199it [05:24,  1.05it/s]200it [05:25,  1.06it/s]201it [05:25,  1.14it/s]202it [05:26,  1.17it/s]203it [05:27,  1.22it/s]204it [05:28,  1.26it/s]205it [05:28,  1.30it/s]206it [05:29,  1.27it/s]207it [05:31,  1.00it/s]208it [05:31,  1.04it/s]209it [05:32,  1.08it/s]210it [05:33,  1.02it/s]211it [05:35,  1.02s/it]212it [05:36,  1.03s/it]213it [05:37,  1.03s/it]214it [05:37,  1.03it/s]215it [05:38,  1.00it/s]216it [05:40,  1.03s/it]217it [05:40,  1.01it/s]218it [05:42,  1.06s/it]219it [05:43,  1.08s/it]220it [05:44,  1.06s/it]221it [05:45,  1.03s/it]222it [05:46,  1.03it/s]223it [05:47,  1.03it/s]224it [05:48,  1.02it/s]225it [05:49,  1.02it/s]226it [05:50,  1.01it/s]227it [05:50,  1.05it/s]228it [05:51,  1.05it/s]229it [05:52,  1.08it/s]229it [05:52,  1.54s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:05<02:23,  5.98s/it]  8%|▊         | 2/25 [00:06<00:59,  2.59s/it] 12%|█▏        | 3/25 [00:06<00:37,  1.72s/it] 16%|█▌        | 4/25 [00:07<00:23,  1.12s/it] 20%|██        | 5/25 [00:07<00:15,  1.26it/s] 24%|██▍       | 6/25 [00:07<00:11,  1.68it/s] 28%|██▊       | 7/25 [00:07<00:08,  2.14it/s] 32%|███▏      | 8/25 [00:07<00:06,  2.60it/s] 36%|███▌      | 9/25 [00:08<00:05,  3.04it/s] 40%|████      | 10/25 [00:08<00:04,  3.44it/s] 44%|████▍     | 11/25 [00:08<00:03,  3.76it/s] 48%|████▊     | 12/25 [00:08<00:03,  4.03it/s] 52%|█████▏    | 13/25 [00:08<00:02,  4.25it/s] 56%|█████▌    | 14/25 [00:09<00:02,  4.41it/s] 60%|██████    | 15/25 [00:09<00:02,  4.55it/s] 64%|██████▍   | 16/25 [00:09<00:01,  4.64it/s] 68%|██████▊   | 17/25 [00:09<00:01,  4.70it/s] 72%|███████▏  | 18/25 [00:09<00:01,  4.71it/s] 76%|███████▌  | 19/25 [00:10<00:01,  4.75it/s] 80%|████████  | 20/25 [00:10<00:01,  4.74it/s] 84%|████████▍ | 21/25 [00:10<00:00,  4.77it/s] 88%|████████▊ | 22/25 [00:10<00:00,  4.78it/s] 92%|█████████▏| 23/25 [00:11<00:00,  4.76it/s] 96%|█████████▌| 24/25 [00:11<00:00,  4.80it/s]100%|██████████| 25/25 [00:11<00:00,  2.17it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288027763367
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:06<02:27,  6.17s/it]  8%|▊         | 2/25 [00:06<01:01,  2.66s/it] 12%|█▏        | 3/25 [00:06<00:33,  1.54s/it] 16%|█▌        | 4/25 [00:06<00:21,  1.02s/it] 20%|██        | 5/25 [00:07<00:14,  1.37it/s] 24%|██▍       | 6/25 [00:07<00:10,  1.81it/s] 28%|██▊       | 7/25 [00:07<00:07,  2.26it/s] 32%|███▏      | 8/25 [00:07<00:06,  2.70it/s] 36%|███▌      | 9/25 [00:07<00:05,  3.12it/s] 40%|████      | 10/25 [00:08<00:04,  3.50it/s] 44%|████▍     | 11/25 [00:08<00:03,  3.83it/s] 48%|████▊     | 12/25 [00:08<00:03,  4.07it/s] 52%|█████▏    | 13/25 [00:08<00:02,  4.27it/s] 56%|█████▌    | 14/25 [00:08<00:02,  4.42it/s] 60%|██████    | 15/25 [00:09<00:02,  4.55it/s] 64%|██████▍   | 16/25 [00:09<00:01,  4.65it/s] 68%|██████▊   | 17/25 [00:09<00:01,  4.67it/s] 72%|███████▏  | 18/25 [00:09<00:01,  4.68it/s] 76%|███████▌  | 19/25 [00:09<00:01,  4.73it/s] 80%|████████  | 20/25 [00:10<00:01,  4.70it/s] 84%|████████▍ | 21/25 [00:10<00:00,  4.73it/s] 88%|████████▊ | 22/25 [00:10<00:00,  4.77it/s] 92%|█████████▏| 23/25 [00:10<00:00,  4.76it/s] 96%|█████████▌| 24/25 [00:10<00:00,  4.75it/s]100%|██████████| 25/25 [00:11<00:00,  2.21it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.474498450756073


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.81288146972656
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 57.505285412262154
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.6450835207092
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 51.53702063332318


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.44984436035156
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.47s/it]2it [00:02,  1.47s/it]3it [00:04,  1.38s/it]4it [00:05,  1.39s/it]5it [00:07,  1.41s/it]6it [00:08,  1.45s/it]7it [00:10,  1.46s/it]8it [00:11,  1.42s/it]9it [00:12,  1.41s/it]10it [00:14,  1.43s/it]11it [00:15,  1.39s/it]12it [00:16,  1.36s/it]13it [00:18,  1.36s/it]14it [00:19,  1.37s/it]15it [00:20,  1.36s/it]16it [00:22,  1.37s/it]17it [00:23,  1.36s/it]18it [00:25,  1.37s/it]19it [00:26,  1.35s/it]20it [00:27,  1.33s/it]21it [00:29,  1.41s/it]22it [00:30,  1.38s/it]23it [00:31,  1.35s/it]24it [00:33,  1.35s/it]25it [00:34,  1.38s/it]26it [00:36,  1.42s/it]27it [00:38,  1.60s/it]28it [00:39,  1.63s/it]29it [00:41,  1.69s/it]30it [00:43,  1.61s/it]31it [00:44,  1.65s/it]32it [00:46,  1.63s/it]33it [00:48,  1.70s/it]34it [00:49,  1.67s/it]35it [00:51,  1.62s/it]36it [00:52,  1.53s/it]37it [00:54,  1.48s/it]38it [00:55,  1.42s/it]39it [00:56,  1.38s/it]40it [00:58,  1.43s/it]41it [00:59,  1.47s/it]42it [01:01,  1.46s/it]43it [01:02,  1.33s/it]44it [01:03,  1.24s/it]45it [01:04,  1.23s/it]46it [01:05,  1.26s/it]47it [01:06,  1.22s/it]48it [01:08,  1.24s/it]49it [01:09,  1.25s/it]50it [01:10,  1.27s/it]51it [01:12,  1.28s/it]52it [01:13,  1.29s/it]53it [01:14,  1.30s/it]54it [01:15,  1.25s/it]55it [01:17,  1.28s/it]56it [01:18,  1.23s/it]57it [01:19,  1.25s/it]58it [01:21,  1.28s/it]59it [01:22,  1.27s/it]60it [01:23,  1.25s/it]61it [01:24,  1.25s/it]62it [01:26,  1.28s/it]63it [01:27,  1.26s/it]64it [01:28,  1.30s/it]65it [01:30,  1.32s/it]66it [01:31,  1.27s/it]67it [01:32,  1.28s/it]68it [01:33,  1.17s/it]69it [01:34,  1.08s/it]70it [01:34,  1.03it/s]71it [01:35,  1.17it/s]72it [01:36,  1.24it/s]73it [01:36,  1.29it/s]74it [01:37,  1.29it/s]75it [01:38,  1.33it/s]76it [01:39,  1.24it/s]77it [01:40,  1.31it/s]78it [01:40,  1.24it/s]79it [01:41,  1.14it/s]80it [01:43,  1.02it/s]81it [01:44,  1.01it/s]82it [01:45,  1.00it/s]83it [01:46,  1.03it/s]84it [01:47,  1.05it/s]85it [01:48,  1.05it/s]86it [01:49,  1.18s/it]87it [01:51,  1.37s/it]88it [01:53,  1.63s/it]89it [01:55,  1.70s/it]90it [01:57,  1.68s/it]91it [01:58,  1.62s/it]92it [02:00,  1.57s/it]93it [02:01,  1.48s/it]94it [02:02,  1.33s/it]95it [02:03,  1.12s/it]96it [02:04,  1.23s/it]97it [02:06,  1.37s/it]98it [02:07,  1.41s/it]99it [02:09,  1.37s/it]100it [02:10,  1.40s/it]101it [02:12,  1.48s/it]102it [02:13,  1.45s/it]103it [02:15,  1.51s/it]104it [02:16,  1.43s/it]105it [02:17,  1.28s/it]106it [02:18,  1.19s/it]107it [02:19,  1.20s/it]108it [02:20,  1.18s/it]109it [02:21,  1.13s/it]110it [02:22,  1.07s/it]111it [02:23,  1.06s/it]112it [02:25,  1.21s/it]113it [02:26,  1.25s/it]114it [02:27,  1.24s/it]115it [02:29,  1.23s/it]116it [02:30,  1.20s/it]117it [02:31,  1.16s/it]118it [02:32,  1.26s/it]119it [02:33,  1.24s/it]120it [02:35,  1.29s/it]121it [02:36,  1.34s/it]122it [02:38,  1.40s/it]123it [02:39,  1.40s/it]124it [02:41,  1.40s/it]125it [02:42,  1.39s/it]126it [02:43,  1.36s/it]127it [02:45,  1.53s/it]128it [02:47,  1.75s/it]129it [02:49,  1.69s/it]130it [02:51,  1.75s/it]131it [02:53,  1.80s/it]132it [02:55,  1.83s/it]133it [02:57,  1.93s/it]134it [02:59,  1.93s/it]135it [03:01,  1.97s/it]136it [03:03,  1.93s/it]137it [03:04,  1.86s/it]138it [03:06,  1.82s/it]139it [03:08,  1.80s/it]140it [03:09,  1.72s/it]141it [03:11,  1.65s/it]142it [03:12,  1.62s/it]143it [03:14,  1.61s/it]144it [03:16,  1.67s/it]145it [03:18,  1.68s/it]146it [03:19,  1.68s/it]147it [03:21,  1.71s/it]148it [03:22,  1.65s/it]149it [03:24,  1.47s/it]150it [03:25,  1.39s/it]151it [03:26,  1.35s/it]152it [03:28,  1.40s/it]153it [03:29,  1.49s/it]154it [03:31,  1.59s/it]155it [03:33,  1.58s/it]156it [03:34,  1.46s/it]157it [03:35,  1.43s/it]158it [03:36,  1.34s/it]159it [03:37,  1.30s/it]160it [03:39,  1.28s/it]161it [03:40,  1.25s/it]162it [03:42,  1.46s/it]163it [03:43,  1.48s/it]164it [03:45,  1.57s/it]165it [03:47,  1.65s/it]166it [03:49,  1.72s/it]167it [03:51,  1.77s/it]168it [03:52,  1.69s/it]169it [03:54,  1.63s/it]170it [03:55,  1.59s/it]171it [03:57,  1.49s/it]172it [03:58,  1.39s/it]173it [03:59,  1.31s/it]174it [04:00,  1.26s/it]175it [04:01,  1.17s/it]176it [04:02,  1.15s/it]177it [04:03,  1.12s/it]178it [04:04,  1.20s/it]179it [04:06,  1.23s/it]180it [04:07,  1.21s/it]181it [04:08,  1.17s/it]182it [04:09,  1.16s/it]183it [04:10,  1.14s/it]184it [04:11,  1.16s/it]185it [04:13,  1.15s/it]186it [04:14,  1.12s/it]187it [04:15,  1.13s/it]188it [04:16,  1.08s/it]189it [04:17,  1.01s/it]190it [04:17,  1.04it/s]191it [04:19,  1.02s/it]192it [04:20,  1.06s/it]193it [04:21,  1.05s/it]194it [04:22,  1.05s/it]195it [04:23,  1.05s/it]196it [04:24,  1.05s/it]197it [04:25,  1.04s/it]198it [04:26,  1.15s/it]199it [04:27,  1.09s/it]200it [04:28,  1.10s/it]201it [04:29,  1.06s/it]202it [04:30,  1.04s/it]203it [04:31,  1.02it/s]204it [04:32,  1.02it/s]205it [04:33,  1.02it/s]206it [04:34,  1.02it/s]207it [04:35,  1.02it/s]208it [04:36,  1.03s/it]209it [04:37,  1.03s/it]210it [04:38,  1.01it/s]211it [04:39,  1.02it/s]212it [04:40,  1.02it/s]213it [04:41,  1.04it/s]214it [04:42,  1.06it/s]215it [04:42,  1.24it/s]216it [04:43,  1.28it/s]217it [04:44,  1.31it/s]218it [04:45,  1.25it/s]219it [04:46,  1.17it/s]220it [04:47,  1.14it/s]221it [04:48,  1.16it/s]222it [04:48,  1.13it/s]223it [04:49,  1.16it/s]224it [04:50,  1.14it/s]225it [04:51,  1.12it/s]226it [04:52,  1.15it/s]227it [04:53,  1.17it/s]228it [04:54,  1.15it/s]229it [04:55,  1.13it/s]229it [04:55,  1.29s/it]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:05<02:00,  5.04s/it]  8%|▊         | 2/25 [00:05<00:50,  2.20s/it] 12%|█▏        | 3/25 [00:05<00:28,  1.29s/it] 16%|█▌        | 4/25 [00:05<00:18,  1.16it/s] 20%|██        | 5/25 [00:05<00:12,  1.60it/s] 24%|██▍       | 6/25 [00:06<00:09,  2.07it/s] 28%|██▊       | 7/25 [00:06<00:07,  2.55it/s] 32%|███▏      | 8/25 [00:06<00:05,  3.01it/s] 36%|███▌      | 9/25 [00:06<00:04,  3.42it/s] 40%|████      | 10/25 [00:06<00:03,  3.77it/s] 44%|████▍     | 11/25 [00:07<00:03,  4.06it/s] 48%|████▊     | 12/25 [00:07<00:03,  4.28it/s] 52%|█████▏    | 13/25 [00:07<00:02,  4.45it/s] 56%|█████▌    | 14/25 [00:07<00:02,  4.57it/s] 60%|██████    | 15/25 [00:07<00:02,  4.67it/s] 64%|██████▍   | 16/25 [00:08<00:01,  4.73it/s] 68%|██████▊   | 17/25 [00:08<00:01,  4.79it/s] 72%|███████▏  | 18/25 [00:08<00:01,  4.83it/s] 76%|███████▌  | 19/25 [00:08<00:01,  4.85it/s] 80%|████████  | 20/25 [00:08<00:01,  4.87it/s] 84%|████████▍ | 21/25 [00:09<00:00,  4.89it/s] 88%|████████▊ | 22/25 [00:09<00:00,  4.90it/s] 92%|█████████▏| 23/25 [00:09<00:00,  4.90it/s] 96%|█████████▌| 24/25 [00:09<00:00,  4.91it/s]100%|██████████| 25/25 [00:09<00:00,  2.51it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288623809814
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:04<01:56,  4.86s/it]  8%|▊         | 2/25 [00:05<00:50,  2.20s/it] 12%|█▏        | 3/25 [00:05<00:28,  1.29s/it] 16%|█▌        | 4/25 [00:05<00:18,  1.16it/s] 20%|██        | 5/25 [00:05<00:12,  1.60it/s] 24%|██▍       | 6/25 [00:06<00:09,  2.08it/s] 28%|██▊       | 7/25 [00:06<00:07,  2.56it/s] 32%|███▏      | 8/25 [00:06<00:06,  2.74it/s] 36%|███▌      | 9/25 [00:06<00:05,  3.17it/s] 40%|████      | 10/25 [00:06<00:04,  3.55it/s] 44%|████▍     | 11/25 [00:07<00:03,  3.87it/s] 48%|████▊     | 12/25 [00:07<00:03,  4.13it/s] 52%|█████▏    | 13/25 [00:07<00:02,  4.33it/s] 56%|█████▌    | 14/25 [00:07<00:02,  4.48it/s] 60%|██████    | 15/25 [00:07<00:02,  4.60it/s] 64%|██████▍   | 16/25 [00:08<00:01,  4.68it/s] 68%|██████▊   | 17/25 [00:08<00:01,  4.74it/s] 72%|███████▏  | 18/25 [00:08<00:01,  4.79it/s] 76%|███████▌  | 19/25 [00:08<00:01,  4.81it/s] 80%|████████  | 20/25 [00:08<00:01,  4.83it/s] 84%|████████▍ | 21/25 [00:09<00:00,  4.85it/s] 88%|████████▊ | 22/25 [00:09<00:00,  4.86it/s] 92%|█████████▏| 23/25 [00:09<00:00,  4.87it/s] 96%|█████████▌| 24/25 [00:09<00:00,  4.87it/s]100%|██████████| 25/25 [00:10<00:00,  2.50it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.4743463099002838


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.8128890991211
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 57.505285412262154
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.6386389108282
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 51.76297179329775


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.43463134765625
=========================          END          =========================
0it [00:00, ?it/s]2it [00:00, 10.54it/s]4it [00:00, 10.14it/s]6it [00:00, 10.28it/s]8it [00:00, 10.89it/s]10it [00:00, 10.58it/s]12it [00:01, 10.25it/s]14it [00:01,  8.82it/s]16it [00:01,  9.46it/s]18it [00:01,  9.50it/s]19it [00:01,  9.45it/s]21it [00:02,  9.69it/s]23it [00:02,  9.85it/s]25it [00:02,  9.93it/s]27it [00:02, 10.17it/s]29it [00:02, 10.45it/s]31it [00:03, 11.18it/s]33it [00:03, 11.07it/s]35it [00:03, 11.26it/s]37it [00:03, 10.68it/s]39it [00:03, 10.60it/s]41it [00:03, 10.96it/s]43it [00:04, 10.61it/s]45it [00:04, 10.38it/s]47it [00:04, 10.69it/s]49it [00:04, 10.56it/s]51it [00:04, 10.79it/s]53it [00:05, 10.70it/s]55it [00:05, 10.79it/s]57it [00:05,  8.22it/s]58it [00:06,  4.39it/s]59it [00:06,  4.40it/s]61it [00:06,  5.28it/s]62it [00:07,  4.14it/s]63it [00:07,  3.33it/s]64it [00:08,  3.03it/s]65it [00:08,  3.21it/s]66it [00:08,  3.71it/s]67it [00:08,  4.00it/s]68it [00:09,  3.36it/s]69it [00:09,  3.29it/s]70it [00:09,  3.54it/s]71it [00:10,  3.44it/s]72it [00:10,  3.18it/s]73it [00:10,  3.09it/s]74it [00:11,  3.58it/s]75it [00:11,  4.22it/s]76it [00:11,  4.58it/s]77it [00:11,  4.66it/s]78it [00:11,  3.63it/s]79it [00:12,  4.34it/s]80it [00:12,  4.25it/s]81it [00:12,  4.48it/s]82it [00:12,  4.54it/s]83it [00:13,  3.80it/s]84it [00:13,  3.71it/s]85it [00:13,  3.28it/s]86it [00:14,  2.01it/s]87it [00:15,  1.98it/s]88it [00:15,  1.95it/s]89it [00:16,  2.16it/s]90it [00:16,  2.41it/s]91it [00:16,  2.79it/s]92it [00:16,  2.92it/s]93it [00:17,  2.80it/s]94it [00:17,  2.76it/s]95it [00:18,  2.73it/s]96it [00:18,  2.96it/s]97it [00:18,  3.20it/s]98it [00:18,  3.45it/s]99it [00:19,  3.23it/s]100it [00:19,  2.47it/s]101it [00:20,  2.93it/s]102it [00:20,  3.17it/s]103it [00:20,  3.65it/s]104it [00:20,  3.33it/s]105it [00:21,  3.11it/s]106it [00:21,  3.19it/s]107it [00:21,  3.63it/s]108it [00:21,  4.20it/s]109it [00:22,  3.26it/s]110it [00:22,  3.81it/s]111it [00:22,  3.58it/s]112it [00:23,  3.47it/s]113it [00:23,  3.64it/s]114it [00:23,  3.73it/s]115it [00:23,  3.59it/s]116it [00:24,  3.62it/s]117it [00:24,  3.38it/s]118it [00:24,  3.41it/s]119it [00:25,  3.23it/s]120it [00:25,  3.75it/s]121it [00:25,  3.48it/s]122it [00:26,  2.62it/s]123it [00:26,  2.85it/s]124it [00:26,  2.92it/s]125it [00:27,  2.54it/s]126it [00:27,  2.72it/s]127it [00:27,  2.81it/s]128it [00:28,  3.36it/s]129it [00:28,  3.79it/s]130it [00:28,  4.64it/s]131it [00:28,  5.15it/s]132it [00:28,  4.81it/s]133it [00:29,  3.95it/s]134it [00:29,  4.07it/s]135it [00:29,  4.05it/s]136it [00:29,  4.08it/s]137it [00:30,  3.89it/s]138it [00:30,  4.21it/s]139it [00:30,  3.58it/s]140it [00:30,  3.78it/s]141it [00:31,  3.74it/s]142it [00:31,  4.28it/s]143it [00:31,  4.34it/s]144it [00:31,  4.27it/s]145it [00:32,  4.62it/s]146it [00:32,  4.82it/s]147it [00:32,  4.62it/s]148it [00:32,  4.07it/s]149it [00:33,  4.23it/s]150it [00:33,  4.23it/s]151it [00:33,  3.94it/s]152it [00:33,  4.29it/s]153it [00:34,  3.18it/s]154it [00:34,  2.36it/s]155it [00:35,  2.18it/s]156it [00:36,  1.98it/s]157it [00:36,  1.96it/s]158it [00:36,  2.33it/s]159it [00:37,  2.52it/s]160it [00:37,  2.84it/s]161it [00:37,  2.92it/s]162it [00:37,  3.18it/s]163it [00:38,  3.39it/s]164it [00:38,  3.25it/s]165it [00:38,  3.15it/s]166it [00:39,  3.29it/s]167it [00:39,  3.19it/s]168it [00:40,  2.60it/s]169it [00:40,  2.58it/s]170it [00:40,  2.67it/s]171it [00:41,  2.89it/s]172it [00:41,  3.25it/s]173it [00:41,  3.32it/s]174it [00:41,  3.26it/s]175it [00:42,  3.98it/s]176it [00:42,  4.14it/s]177it [00:42,  3.56it/s]178it [00:42,  3.70it/s]179it [00:43,  3.58it/s]180it [00:43,  4.41it/s]181it [00:43,  4.86it/s]182it [00:43,  4.70it/s]183it [00:43,  4.00it/s]184it [00:44,  3.91it/s]185it [00:44,  3.97it/s]186it [00:44,  4.14it/s]187it [00:44,  3.96it/s]188it [00:45,  3.69it/s]189it [00:45,  4.51it/s]190it [00:45,  4.94it/s]191it [00:45,  5.70it/s]192it [00:45,  5.40it/s]193it [00:46,  5.39it/s]194it [00:46,  5.18it/s]195it [00:46,  5.36it/s]196it [00:46,  4.67it/s]197it [00:46,  4.56it/s]198it [00:47,  4.94it/s]199it [00:47,  4.57it/s]200it [00:47,  4.61it/s]201it [00:47,  5.14it/s]202it [00:48,  4.24it/s]203it [00:48,  4.56it/s]204it [00:48,  4.47it/s]205it [00:48,  4.26it/s]206it [00:49,  3.69it/s]207it [00:49,  3.64it/s]208it [00:49,  3.94it/s]209it [00:49,  4.65it/s]210it [00:49,  4.97it/s]211it [00:50,  4.54it/s]212it [00:50,  4.27it/s]213it [00:50,  4.59it/s]214it [00:51,  3.44it/s]215it [00:51,  2.62it/s]216it [00:52,  2.27it/s]217it [00:53,  1.50it/s]218it [00:54,  1.06it/s]219it [00:56,  1.14s/it]220it [00:57,  1.19s/it]221it [00:58,  1.08s/it]222it [00:59,  1.14it/s]223it [01:00,  1.02it/s]224it [01:01,  1.09s/it]225it [01:02,  1.11s/it]226it [01:04,  1.14s/it]227it [01:05,  1.19s/it]228it [01:06,  1.22s/it]229it [01:07,  1.15s/it]229it [01:07,  3.39it/s]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:06<02:25,  6.05s/it]  8%|▊         | 2/25 [00:06<01:00,  2.61s/it] 12%|█▏        | 3/25 [00:06<00:33,  1.51s/it] 16%|█▌        | 4/25 [00:06<00:20,  1.00it/s] 20%|██        | 5/25 [00:06<00:14,  1.41it/s] 24%|██▍       | 6/25 [00:07<00:10,  1.86it/s] 28%|██▊       | 7/25 [00:07<00:07,  2.33it/s] 32%|███▏      | 8/25 [00:07<00:06,  2.79it/s] 36%|███▌      | 9/25 [00:07<00:04,  3.23it/s] 40%|████      | 10/25 [00:07<00:04,  3.59it/s] 44%|████▍     | 11/25 [00:08<00:03,  3.91it/s] 48%|████▊     | 12/25 [00:08<00:03,  4.17it/s] 52%|█████▏    | 13/25 [00:08<00:02,  4.37it/s] 56%|█████▌    | 14/25 [00:08<00:02,  4.52it/s] 60%|██████    | 15/25 [00:08<00:02,  4.63it/s] 64%|██████▍   | 16/25 [00:09<00:01,  4.71it/s] 68%|██████▊   | 17/25 [00:09<00:01,  4.77it/s] 72%|███████▏  | 18/25 [00:09<00:01,  4.81it/s] 76%|███████▌  | 19/25 [00:09<00:01,  4.84it/s] 80%|████████  | 20/25 [00:09<00:01,  4.86it/s] 84%|████████▍ | 21/25 [00:10<00:00,  4.87it/s] 88%|████████▊ | 22/25 [00:10<00:00,  4.88it/s] 92%|█████████▏| 23/25 [00:10<00:00,  4.89it/s] 96%|█████████▌| 24/25 [00:10<00:00,  4.89it/s]100%|██████████| 25/25 [00:10<00:00,  2.28it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288623809814
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:04<01:58,  4.95s/it]  8%|▊         | 2/25 [00:05<00:49,  2.16s/it] 12%|█▏        | 3/25 [00:05<00:27,  1.26s/it] 16%|█▌        | 4/25 [00:05<00:17,  1.18it/s] 20%|██        | 5/25 [00:05<00:12,  1.63it/s] 24%|██▍       | 6/25 [00:05<00:09,  2.11it/s] 28%|██▊       | 7/25 [00:06<00:06,  2.59it/s] 32%|███▏      | 8/25 [00:06<00:05,  3.04it/s] 36%|███▌      | 9/25 [00:06<00:04,  3.45it/s] 40%|████      | 10/25 [00:06<00:03,  3.80it/s] 44%|████▍     | 11/25 [00:06<00:03,  4.07it/s] 48%|████▊     | 12/25 [00:07<00:03,  4.29it/s] 52%|█████▏    | 13/25 [00:07<00:02,  4.45it/s] 56%|█████▌    | 14/25 [00:07<00:02,  4.57it/s] 60%|██████    | 15/25 [00:07<00:02,  4.65it/s] 64%|██████▍   | 16/25 [00:08<00:01,  4.72it/s] 68%|██████▊   | 17/25 [00:08<00:01,  4.76it/s] 72%|███████▏  | 18/25 [00:08<00:01,  4.80it/s] 76%|███████▌  | 19/25 [00:08<00:01,  4.83it/s] 80%|████████  | 20/25 [00:08<00:01,  4.84it/s] 84%|████████▍ | 21/25 [00:09<00:00,  4.84it/s] 88%|████████▊ | 22/25 [00:09<00:00,  4.85it/s] 92%|█████████▏| 23/25 [00:09<00:00,  4.86it/s] 96%|█████████▌| 24/25 [00:09<00:00,  4.87it/s]100%|██████████| 25/25 [00:09<00:00,  2.52it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.47535645961761475


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.8128890991211
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 57.911855586274186
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 75.0632821182557
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 52.6948737122982


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.53564453125
=========================          END          =========================
0it [00:00, ?it/s]1it [00:00,  4.12it/s]2it [00:00,  2.41it/s]3it [00:01,  2.00it/s]4it [00:01,  2.53it/s]5it [00:01,  2.84it/s]6it [00:02,  3.16it/s]7it [00:02,  3.22it/s]8it [00:02,  3.37it/s]9it [00:03,  3.32it/s]10it [00:03,  3.41it/s]11it [00:03,  2.93it/s]12it [00:03,  3.58it/s]13it [00:04,  3.68it/s]14it [00:04,  4.03it/s]15it [00:04,  4.00it/s]16it [00:04,  4.04it/s]17it [00:05,  3.61it/s]18it [00:05,  3.46it/s]19it [00:05,  2.91it/s]20it [00:06,  3.17it/s]21it [00:06,  3.26it/s]22it [00:06,  3.46it/s]23it [00:07,  3.24it/s]24it [00:07,  3.47it/s]25it [00:07,  3.26it/s]26it [00:08,  2.77it/s]27it [00:08,  2.73it/s]28it [00:08,  2.89it/s]29it [00:09,  2.82it/s]30it [00:09,  2.81it/s]31it [00:09,  3.56it/s]32it [00:10,  3.02it/s]33it [00:10,  3.06it/s]34it [00:11,  2.54it/s]35it [00:11,  2.80it/s]36it [00:11,  2.99it/s]37it [00:11,  3.63it/s]38it [00:11,  4.22it/s]39it [00:12,  4.12it/s]40it [00:12,  4.59it/s]41it [00:12,  4.41it/s]42it [00:12,  3.55it/s]43it [00:13,  3.60it/s]44it [00:13,  3.49it/s]45it [00:13,  3.12it/s]46it [00:14,  3.49it/s]47it [00:14,  4.02it/s]48it [00:14,  3.65it/s]49it [00:14,  3.72it/s]50it [00:15,  4.07it/s]51it [00:15,  3.99it/s]52it [00:16,  2.29it/s]53it [00:17,  1.59it/s]54it [00:17,  1.78it/s]55it [00:17,  2.23it/s]56it [00:18,  2.46it/s]57it [00:18,  2.68it/s]58it [00:18,  3.16it/s]59it [00:19,  2.99it/s]60it [00:19,  3.27it/s]61it [00:19,  3.26it/s]62it [00:19,  3.17it/s]63it [00:20,  3.26it/s]64it [00:20,  2.68it/s]65it [00:21,  1.71it/s]66it [00:22,  1.34it/s]67it [00:23,  1.25it/s]68it [00:24,  1.45it/s]69it [00:24,  1.78it/s]70it [00:25,  1.58it/s]71it [00:26,  1.37it/s]72it [00:27,  1.21it/s]73it [00:28,  1.10it/s]74it [00:30,  1.15s/it]75it [00:31,  1.14s/it]76it [00:32,  1.04s/it]77it [00:32,  1.06it/s]78it [00:33,  1.11it/s]79it [00:34,  1.13it/s]80it [00:35,  1.18it/s]81it [00:35,  1.20it/s]82it [00:36,  1.22it/s]83it [00:37,  1.28it/s]84it [00:38,  1.29it/s]85it [00:39,  1.23it/s]86it [00:39,  1.22it/s]87it [00:40,  1.21it/s]88it [00:41,  1.22it/s]89it [00:42,  1.03it/s]90it [00:44,  1.13s/it]91it [00:45,  1.01s/it]92it [00:45,  1.05it/s]93it [00:46,  1.08it/s]94it [00:47,  1.10it/s]95it [00:48,  1.14it/s]96it [00:49,  1.16it/s]97it [00:50,  1.14it/s]98it [00:51,  1.19it/s]99it [00:51,  1.19it/s]100it [00:52,  1.19it/s]101it [00:53,  1.19it/s]102it [00:54,  1.09it/s]103it [00:55,  1.10it/s]104it [00:56,  1.06it/s]105it [00:57,  1.05it/s]106it [00:58,  1.03it/s]107it [00:59,  1.01it/s]108it [01:00,  1.02it/s]109it [01:01,  1.03it/s]110it [01:02,  1.05it/s]111it [01:03,  1.06it/s]112it [01:04,  1.07it/s]113it [01:05,  1.08it/s]114it [01:05,  1.12it/s]115it [01:06,  1.15it/s]116it [01:07,  1.12it/s]117it [01:08,  1.10it/s]118it [01:09,  1.11it/s]119it [01:10,  1.09it/s]120it [01:11,  1.12it/s]121it [01:12,  1.11it/s]122it [01:13,  1.15it/s]123it [01:14,  1.10it/s]124it [01:14,  1.10it/s]125it [01:15,  1.09it/s]126it [01:16,  1.09it/s]127it [01:17,  1.13it/s]128it [01:18,  1.14it/s]129it [01:19,  1.09it/s]130it [01:20,  1.08it/s]131it [01:21,  1.06it/s]132it [01:22,  1.07it/s]133it [01:23,  1.10it/s]134it [01:24,  1.07it/s]135it [01:25,  1.09it/s]136it [01:25,  1.10it/s]137it [01:26,  1.12it/s]138it [01:27,  1.16it/s]139it [01:28,  1.39it/s]140it [01:28,  1.81it/s]141it [01:28,  2.30it/s]142it [01:28,  2.71it/s]143it [01:28,  3.00it/s]144it [01:29,  3.32it/s]145it [01:29,  3.81it/s]146it [01:29,  4.17it/s]147it [01:29,  4.65it/s]148it [01:29,  4.47it/s]149it [01:30,  4.33it/s]150it [01:30,  3.93it/s]151it [01:30,  3.80it/s]152it [01:30,  3.41it/s]153it [01:31,  3.66it/s]154it [01:31,  3.88it/s]155it [01:31,  3.56it/s]156it [01:31,  4.14it/s]157it [01:32,  4.54it/s]158it [01:32,  4.68it/s]159it [01:32,  4.73it/s]160it [01:32,  5.38it/s]161it [01:32,  6.16it/s]162it [01:32,  6.71it/s]164it [01:33,  8.29it/s]166it [01:33,  8.11it/s]167it [01:33,  7.37it/s]168it [01:33,  5.93it/s]169it [01:34,  4.92it/s]170it [01:34,  5.44it/s]171it [01:34,  4.99it/s]172it [01:34,  5.47it/s]173it [01:34,  4.88it/s]174it [01:35,  4.88it/s]175it [01:35,  4.77it/s]176it [01:35,  4.89it/s]177it [01:35,  3.23it/s]178it [01:36,  2.24it/s]179it [01:37,  1.98it/s]180it [01:37,  2.19it/s]181it [01:38,  2.27it/s]182it [01:38,  2.41it/s]183it [01:38,  2.69it/s]184it [01:39,  2.78it/s]185it [01:39,  2.96it/s]186it [01:39,  3.36it/s]187it [01:39,  3.46it/s]188it [01:40,  3.48it/s]189it [01:40,  3.98it/s]190it [01:40,  4.52it/s]191it [01:40,  4.22it/s]192it [01:40,  4.23it/s]193it [01:41,  4.11it/s]194it [01:41,  4.96it/s]195it [01:41,  4.26it/s]196it [01:41,  4.37it/s]197it [01:42,  4.63it/s]198it [01:42,  4.63it/s]199it [01:42,  5.00it/s]200it [01:42,  5.44it/s]201it [01:42,  6.29it/s]202it [01:42,  4.78it/s]203it [01:43,  5.47it/s]204it [01:43,  5.39it/s]205it [01:43,  5.79it/s]206it [01:43,  5.34it/s]207it [01:43,  4.55it/s]208it [01:44,  2.50it/s]209it [01:45,  2.73it/s]210it [01:45,  2.47it/s]211it [01:45,  2.52it/s]212it [01:46,  3.13it/s]213it [01:46,  3.35it/s]214it [01:46,  3.58it/s]215it [01:46,  3.87it/s]216it [01:47,  4.05it/s]217it [01:47,  4.12it/s]218it [01:47,  4.07it/s]219it [01:47,  4.60it/s]220it [01:47,  4.65it/s]221it [01:48,  4.28it/s]222it [01:48,  3.96it/s]223it [01:48,  4.08it/s]224it [01:48,  3.93it/s]225it [01:49,  2.88it/s]226it [01:49,  3.32it/s]227it [01:49,  4.09it/s]228it [01:49,  4.97it/s]229it [01:50,  5.72it/s]229it [01:50,  2.08it/s]
Number of selected candidates = 112
---> Each Classifier' shapes
	 GT_classifier = 102
	 ViLang_guessed = 112
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:04<01:48,  4.54s/it]  8%|▊         | 2/25 [00:05<00:49,  2.16s/it] 12%|█▏        | 3/25 [00:05<00:27,  1.27s/it] 16%|█▌        | 4/25 [00:05<00:17,  1.18it/s] 20%|██        | 5/25 [00:05<00:12,  1.63it/s] 24%|██▍       | 6/25 [00:05<00:09,  1.96it/s] 28%|██▊       | 7/25 [00:06<00:07,  2.43it/s] 32%|███▏      | 8/25 [00:06<00:05,  2.90it/s] 36%|███▌      | 9/25 [00:06<00:04,  3.32it/s] 40%|████      | 10/25 [00:06<00:04,  3.69it/s] 44%|████▍     | 11/25 [00:06<00:03,  3.99it/s] 48%|████▊     | 12/25 [00:07<00:03,  4.23it/s] 52%|█████▏    | 13/25 [00:07<00:02,  4.41it/s] 56%|█████▌    | 14/25 [00:07<00:02,  4.54it/s] 60%|██████    | 15/25 [00:07<00:02,  4.65it/s] 64%|██████▍   | 16/25 [00:07<00:01,  4.72it/s] 68%|██████▊   | 17/25 [00:08<00:01,  4.77it/s] 72%|███████▏  | 18/25 [00:08<00:01,  4.81it/s] 76%|███████▌  | 19/25 [00:08<00:01,  4.83it/s] 80%|████████  | 20/25 [00:08<00:01,  4.84it/s] 84%|████████▍ | 21/25 [00:09<00:00,  4.85it/s] 88%|████████▊ | 22/25 [00:09<00:00,  4.84it/s] 92%|█████████▏| 23/25 [00:09<00:00,  4.85it/s] 96%|█████████▌| 24/25 [00:09<00:00,  4.85it/s]100%|██████████| 25/25 [00:09<00:00,  2.53it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.7781288623809814
---> Evaluating
  0%|          | 0/25 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/25 [00:03<01:32,  3.86s/it]  8%|▊         | 2/25 [00:04<00:45,  1.97s/it] 12%|█▏        | 3/25 [00:04<00:25,  1.16s/it] 16%|█▌        | 4/25 [00:04<00:16,  1.27it/s] 20%|██        | 5/25 [00:05<00:11,  1.74it/s] 24%|██▍       | 6/25 [00:05<00:08,  2.22it/s] 28%|██▊       | 7/25 [00:05<00:06,  2.71it/s] 32%|███▏      | 8/25 [00:05<00:05,  3.16it/s] 36%|███▌      | 9/25 [00:05<00:04,  3.55it/s] 40%|████      | 10/25 [00:06<00:03,  3.88it/s] 44%|████▍     | 11/25 [00:06<00:03,  4.13it/s] 48%|████▊     | 12/25 [00:06<00:03,  4.33it/s] 52%|█████▏    | 13/25 [00:06<00:02,  4.48it/s] 56%|█████▌    | 14/25 [00:06<00:02,  4.59it/s] 60%|██████    | 15/25 [00:07<00:02,  4.68it/s] 64%|██████▍   | 16/25 [00:07<00:01,  4.73it/s] 68%|██████▊   | 17/25 [00:07<00:01,  4.78it/s] 72%|███████▏  | 18/25 [00:07<00:01,  4.81it/s] 76%|███████▌  | 19/25 [00:07<00:01,  4.83it/s] 80%|████████  | 20/25 [00:08<00:01,  4.84it/s] 84%|████████▍ | 21/25 [00:08<00:00,  4.85it/s] 88%|████████▊ | 22/25 [00:08<00:00,  4.86it/s] 92%|█████████▏| 23/25 [00:08<00:00,  4.87it/s] 96%|█████████▌| 24/25 [00:09<00:00,  4.87it/s]100%|██████████| 25/25 [00:09<00:00,  2.71it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([6149, 768])
gt_feats.shape torch.Size([6149, 768])
Semantic similarity score = 0.4786745309829712


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 69.93006993006993
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 81.59529380891625
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 62.20632601351731


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 77.8128890991211
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 58.36721418116767
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 74.97812441661128
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 52.72088910895035


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 47.867454528808594
=========================          END          =========================


========================= ViLang Final Results of 10 runs, w/ random imgs per class=========================


[Clustering]
Clustering ACC: 57.68092372743536
Semantic ACC:   47.501678466796875
=========================          END          =========================
