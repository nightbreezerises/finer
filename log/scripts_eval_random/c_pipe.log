Namespace(config_file_env='./configs/env_machine.yml', config_file_expt='./configs/expts/car196_all.yml', alpha=0.7, N_tta=10, num_per_category='random', num_runs=10)
Number of GPUs: 1
Device ID: 0 Device Name: NVIDIA RTX A6000
['Ford Mustang Shelby GT500 Convertible 2010s', 'Audi A5 S Line Coupe 2017', 'Chevrolet Corvette Z06 Coupe 2010', 'Chevrolet Trailblazer SUV 2000s', 'Buick Regal Sedan 2012', 'Buick Enclave SUV 2015', 'BMW M6 GTS Convertible', 'Toyota Corolla S Sedan 2018', 'Cadillac CTS Black Diamond Edition Coupe 2010', 'Aston Martin DB9 V12 Vantage Roadster', 'Jeep Grand Cherokee Limited 2013', 'Audi A4 S Line Sedan 2016', 'Range Rover Evoque SUV 2010s', 'Land Rover Discovery SUV 2012', 'Audi TT RS MK7 Coupe 2012', 'Buick Verano 4-Door Sedan 2016', 'Hyundai Tucson Limited SUV 2010s', 'BMW X5 2010s', 'Hyundai Veloster S Coupe 2010s', 'Ferrari California T Roadster Yellow', 'Audi A4 B5 S4 Sedan', 'Chevrolet Colorado Z71', 'Aston Martin Vantage V8 Roadster 2010', 'Audi RS5 Cabriolet 2019', 'Chevrolet Traverse LT SUV 2010s', 'Ferrari F430 Spider Roadster 2000s', 'Chevrolet Silverado 1500 Crew Cab Truck 2010s', 'Chevrolet Suburban Premier 2018', 'Dodge Ram SUV 2005', 'Chevrolet Silverado 1500 LT Extended Cab 2012', 'Audi A4 B5 Sedan with Curved Roofline 1990s', 'Audi S5 Sedan 2013', 'Chevrolet Trailblazer SS LTZ SUV 2000s', 'BMW X3 xDrive28i', 'Nissan Versa Sedan 2016', 'Audi A5 Coupe 2010s', 'BMW M135i Coupe 2018', 'BMW M6 Convertible 2008', 'Hummer H3 Yellow SUV 2000s', 'Audi R8 Roadster 2016', 'Audi S5 Coupe 2010', 'Chevrolet Camaro Convertible 2010', 'Toyota Sequoia SR5 SUV 2010', 'BMW 1 Series Convertible M Sport 2012', 'Audi S3 Cabriolet', 'Mercedes-Benz C-Class Sedan 2013', 'BMW 3 Series M3 Sedan', 'Bentley Mulsanne Sedan 2005', 'Audi A4 Sedan 1995', 'Ford F-150 Lariat Crew Cab 2010s', 'Acura TLX Sedan 2010', 'Audi A4 S Line Sedan 2010', 'Ford F-350 Super Duty DRW 4x4 Pickup Truck 2016', 'Audi A5 Coupe 2015', 'BMW 3 Series 330i Sedan', 'Chevrolet Silverado 1500 LT 4-Door Truck 2015', 'Chevrolet Suburban LT Beige', 'Acura TL Sedan 2005', 'Chevrolet Silverado 1500 LT Diesel 2010', 'Honda Accord Hybrid Sedan 2013', 'Acura TSX Sedan 2013', 'Aston Martin DB9 Coupe 2010s', 'Aston Martin V8 Vantage Roadster 2010s', 'Mercedes-Benz S-Class Sedan 2010', 'Dodge Dakota Crew Cab Truck 2007', 'Chevrolet Express Cargo Van LT WD LT W', 'Acura RDX Sedan 2018', 'BMW Z4 sDrive30i Coupe 2010', 'Ferrari 458 Italia GT 2000s', 'Bentley Continental GT Silver Coupe', 'Aston Martin DB9 V12 Vantage Roadster 2010', 'BMW X5 XDrive35i', 'Aston Martin DB9 V12 Vantage S V Coupe 2010s', 'Bentley Continental GT V8', 'Ford F-350 XL DRW 4x4 Pickup Truck 2016', 'Volkswagen Golf Mk3 Hatchback 1990s', 'Audi A3 S Line Convertible 2000s', 'Bentley Mulsanne Speed Sedan 2015', 'Volkswagen Golf Mk4 Hatchback 1990s', 'Aston Martin DB9 V12 S Vantage Roadster', 'BMW Z4 Convertible 2013', 'Bugatti Veyron Grand Sport Vitesse 2013', 'Acura TLX Sedan 2015', 'Chevrolet Colorado Rim-designed GMC Sierra Orange Pickup Truck 2010s', 'Acura MDX Sedan 2012', 'Audi A3 S Line Convertible', 'Chevrolet Corvette Coupe 2005', 'Dodge Avenger SXT Sedan 2007', 'BMW 1 Series M Sport Convertible 2010s', 'Acura TL Sedan 2010s', 'Audi A5 Cabriolet 2010s', 'Aston Martin DB11 GTS 2-Door Coupe', 'Acura TSX Sedan 2012', 'Volvo 240 S Sedan 1990s', 'Chevrolet Cobalt SS Sedan 2010', 'BMW M6 Convertible 2009', 'Chevrolet Malibu LTZ Sedan 2012', 'Chevrolet Tahoe LT SUV 2010s', 'Hummer H1 Wagon 2000s', 'Acura TL V6 Sedan 2012', 'Audi A4 Avant 1990s', 'Chevrolet Corvette Z06 Coupe White 2010', 'Lamborghini Huracan LP610-4 Coupe', 'Chevrolet Blazer SUV 2010s', 'Honda Civic EX Sedan 1990', 'Hummer H3 Truck 2000s', 'BMW 1 Series Coupe 2012', 'Acura RLX Sedan 2014', 'Nissan Leaf Sedan 2019', 'Bugatti Veyron GTS Roadster 2012', 'Aston Martin DB9 V12 Vantage SVR', 'Audi TT S Line Coupe 2000', 'Audi A5 Cabriolet', 'Honda Accord Sedan 2013', 'Audi A4 B5 Sedan 1990s', 'Audi A5 Coupe 2012', 'Chevrolet Corvette Coupe 2010s', 'Audi Q5 S line SUV 2000s', 'Honda Civic Si Coupe 2005', 'Audi TTS Coupe 2010s', 'Dodge Grand Caravan Minivan 2010s', 'Porsche Panamera 4S Sedan 2015', 'Ferrari FT Swiss Ice Challenge Coupe 2010s', 'Acura TL Sedan 2007', 'Infiniti QX56 Limited Edition SUV 2018', 'BMW M5 Sedan 2010s', 'Dodge Dakota SLT 4x4 Truck 2004', 'Ford Expedition XLT SUV 2010s', 'BMW Z4 Roadster Coupe 2010s', 'BMW 1 Series Convertible M Sport 2014', 'Audi A4 S Line Sedan 2008', 'Buick Enclave Premium SUV 2017', 'Honda Odyssey Touring Minivan 2019', 'Jeep Wrangler Rubicon Orange', 'Audi TT S Line Coupe 2005', 'Audi TT S Line Coupe', 'Jaguar XF SVR Coupe 2010', 'Audi A5 2.0T Quattro Sedan 2012', 'Volkswagen Beetle 2-Door Hatchback 2010', 'Acura RDX AWD SUV 2010', 'Audi A4 B5 TDI Sedan 1990s', 'Lamborghini Gallardo GTS V Coupe 2000', 'Buick Verano Dark Gray Sedan 2016', 'Aston Martin Vantage Coupe 2012', 'Chevrolet Express Van 2000s', 'Volvo 240 S60 Sedan 1990s', 'Dodge Journey SXT SUV 2010s', 'Cadillac CTS Premium Sedan 2019', 'Aston Martin Vantage V8 Roadster 2010s', 'Ford Taurus Sedan 1990s', 'Buick Regal GS Sedan 2012', 'Lincoln Town Car Sedan 2000', 'BMW X3 xDrive30i 2012', 'Aston Martin V8 Vantage S Roadster', 'Cadillac Escalade SUV 2010', 'Hummer H3 2-Door SUV 2000s', 'Audi R8 V10 Plus Coupe Black Hood Scoop 2000s', 'Audi S5 Cabriolet 2010s', 'Cadillac CTS Coupe 2010', 'Chevrolet Camaro ZL1 Convertible', 'Cadillac CTS Sedan 2015', 'Bentley Mulsanne Sedan 2009', 'Chrysler Town and Country 2011 Minivan', 'Dodge Charger RT Sedan 2000s', 'Hummer H3 Black Edition SUV 2000s', 'Audi A4 Sedan 2014', 'Mercedes-Benz S550 Sedan', 'Honda Odyssey LX V6 Minivan 2012', 'Aston Martin Vantage Convertible 2010', 'BMW 528i Sedan 2015', 'Chevrolet Impala LS Sedan 2011', 'Bugatti Veyron Super Sport Coupe 2010s', 'Chevrolet Tahoe Premier SUV 2010s', 'Chevrolet Impala LS V6 4-Door Sedan 2000s', 'GMC Sierra Orange Pickup Truck 2010s', 'Chevrolet Sonic LT Sedan 2013', 'Bentley Continental GT V8 Coupe 2010s', 'BMW X3 M40i 2018', 'Aston Martin Vantage V8 S Coupe 2015', 'Chevrolet Corvette Grand Sport Coupe 2013', 'Aston Martin DB9 V12 S Vantage Convertible 2015', 'BMW Z4 sDrive35i Coupe 2010', 'Cadillac SRX 2013', 'BMW 5 Series Sedan 2012', 'Audi A4 4-Door Sedan 1990s', 'Aston Martin DB9 Vantage Coupe 2000s', 'Audi S4 Cabriolet 2010', 'BMW Z4 2-Door Roadster 2013', 'Aston Martin DB9 V12 Vantage Roadster 2015', 'Audi A5 Convertible S Line Black', 'BMW M3 Convertible 2005', 'Chrysler Crossfire Convertible 2005', 'Bentley Continental GT V8 GTS Convertible 2014', 'Audi TT S Line Coupe 2-Door', 'Ford Expedition Limited SUV 2010s', 'Acura TLX Sedan 2008', 'Suzuki Xterra SUV 2010', 'Audi A4 S Line Sedan 2005', 'BMW 3 Series 328i Sedan', 'Bugatti Veyron Coupe 2000s', 'Toyota Corolla Sedan 2013', 'Jeep Patriot Latitude SUV 201X', 'BMW M5 Sedan 2017', 'Chevrolet Tahoe SUV 2010s', 'Honda Accord LX Sedan 2012', 'Audi R8 Coupe 2010s', 'Bentley Mulsanne Sedan 2008', 'Cadillac Escalade ESV 2012', 'Honda Civic RS MK3 Coupe', 'Aston Martin DB9 Coupe Blue with Hood Scoop 2010', 'Acura RDX 4-Door SUV 2010', 'Chevrolet Camaro SS Convertible 2012', 'BMW M3 Coupe 2001', 'Mitsubishi Lancer Evo X GTS R HD Blue Sedan', 'Dodge Ram 1500 SUV 2008', 'Bentley Continental GT 2010s Coupe', 'Aston Martin V12 Vantage V8 S', '2018 Chevrolet Express Cargo Van 4x2', 'Audi R8 V10 Plus Sports Coupe 2010s', 'Hyundai Santa Fe SUV 2010', 'Audi A4 Sedan S Line 2010s', 'Rolls Royce Ghost Sedan 2012', 'BMW M3 Coupe 2000', 'Audi S4 Sedan', 'Chrysler Aspen SUV 2007', 'Acura TL Blue Sedan 2010', 'Audi A4 B5 Sedan 1990', 'Honda Civic LX Sedan 1990', 'Buick Enclave Crossover SUV 2010s', 'Audi TT RS Plus Coupé 2015', 'Ford Edge SE SUV 2011', 'Chevrolet Impala SS Sedan 2000', 'GMC Yukon XL SUV 2010', 'Acura TLX V6 Sedan 2013', 'BMW M135i Coupe 2015', 'GMC Terrain SLT SUV 2012', 'Toyota Yaris Sedan 2016', 'Acura TLX SH-AWD Sedan 2018', 'Audi A3 Sedan 2000s', 'Aston Martin Vantage V8 S Coupe 2012', 'BMW M5 M Sport Sedan 2010s', 'Bentley Mulsanne Blue Edition Sedan 2017', 'Mercedes-Benz S600 Sedan 2010', 'Dodge Grand Caravan SE SUV 2005', 'Chevrolet Impala Sedan 2005', 'Volvo 240 Sedan 1990s', 'Saturn SLT Sedan 2002', 'Hummer H2 SUV 2000s', 'Acura TL 2-Door Sedan 2012', 'Jeep Liberty SUV 2010', '2018 Chevrolet Express Cargo Van 4x2 CVT', 'Chevrolet Silverado 1500 LT w/LTZ 2010', 'Bugatti Veyron Silver Coupe 2000s', 'Acura TLX Hybrid Sedan 2010', 'GMC Savana 2500 Cargo Van 2019', 'Audi S3 Cabriolet 2000s', 'Chevrolet Corvette SS Coupe 1990s', 'Chevrolet Suburban LT SUV 2000s', 'Audi S5 Coupe 2015', 'Toyota Sequoia Platinum SUV 2010', 'Ram 2500 Truck 2010s', 'Audi TT Convertible 2000s', 'Acura TL Sedan 2009', 'Audi S5 Cabriolet 2010', 'Nissan NV200 Cargo Van 2015', 'Honda Civic Type R 1990s 2-Door Sedan', 'Chevrolet Impala SS Coupe 2000', 'Bentley Continental GT Coupe 2012', '2018 GMC Express Cargo Van 4x2', 'Chrysler Grand Caravan 2011 Passenger Van', 'Ford F-150 Black Truck 2000s', 'Aston Martin DB9 V12 Vantage Coupe 2010s', 'Aston Martin DB9 Coupe Blue 2010', 'Aston Martin V8 Vantage', 'Chevrolet Express Cargo Van', 'Audi A5 S Line TDI Quattro Sedan 2014', 'Chevrolet Malibu LS 4-Door Sedan 2012', 'Audi A4 Sedan 1998', 'Hummer H3 SUV 2008', 'Chevrolet Colorado LTZ', 'Dodge Challenger Sedan 2000s', 'Audi TT S Coupe 200X', 'Aston Martin DB9 V8 S Convertible 2010s', 'Toyota Scion iA Sedan 2016', 'BMW 128i Coupe 2010', 'Mini Cooper Roadster 2010s', 'Nissan Versa Sedan 2014', 'Mitsubishi Lancer Evo X GTS R HD 4-Door Sedan', 'Fiat Punto Sedan 2007', 'Chevrolet Silverado 2500HD LT 4x4', 'Hummer H3 Two-Door', 'Ford Expedition SUV 2010s', 'Hyundai Sonata Sedan 2013 with 20-inch Wheels', 'Audi TT Convertible 2010s', 'Acura MDX Sedan 2016', 'Mercedes-Benz S-Class W213 Sedan', 'Chevrolet Traverse LT Mid-Sized SUV 2010s', 'Chevrolet Malibu Sedan 2008', 'Audi TT S Line Coupe 2010s', 'Audi R8 Coupe 2015', 'Audi A4 Wagon 1990s', 'Hyundai Sonata 2013 Dark Gray Sedan', 'Chrysler Town and Country SUV 2010', 'Honda Accord LX Sedan 2013', 'Aston Martin DB9 V8 S V8 Roadster 2010', 'Acura TL Sedan', 'Aston Martin Vantage GT Coupe 2010s', 'Chevrolet Corvette Convertible 2014', 'Ram 3500 Truck 2010s', '2012 Chrysler PT Cruiser 2-Door Convertible', 'Acura TL SH-AWD Sedan 2012', 'Audi TT Coupe TTS 2012', 'Acura Integra GSR Coupe 1998', 'Audi R8 V10 Plus Red Coupe 2010s', 'Toyota Yaris Sedan 2008', 'BMW M6 Convertible 2005', 'Bentley Flying Spur V8 Sedan 2015', 'Bentley Continental GT Coupe 2010s', 'BMW 3 Series Sedan 2012', 'Mercedes-Benz S550 Sedan 2010', 'Audi A4 S Line Sedan 2013', 'Aston Martin DB9 V12 S Roadster 2010', 'McLaren 570S GT3 RS 2000s', 'Acura RLX Sedan 2019', 'Audi A3 Convertible S Line 2000s', 'Audi TT Coupe 2000', 'Audi A5 TDI S Line Sedan', 'BMW X5 XDrive35d SUV 2000s', 'Dodge Dakota Truck 2005', 'Honda Civic Si Coupe 2007', 'Cadillac CTS-V Coupe 2010', 'Acura TLX V6 AWD Sedan 2010s', 'BMW M6 Convertible GTS', 'Ferrari 458 Spider Roadster 2000s', 'Aston Martin V12 Vantage S Roadster', 'Audi A4 S Line Sedan 2014', 'Audi A4 S Line Sedan 2010s', 'BMW 1 Series M Sport Sedan 2015', 'Jaguar XF GT Coupe 2010', 'BMW Z4 Roadster 2008', 'Audi S5 Convertible 2010s', 'Audi R8 V10 Plus Super Coupe 2010s', 'Acura TSX Sedan 2008', 'Buick Regal Sedan 2010', 'Honda Odyssey EX-L Minivan 2015', 'Toyota Sequoia SUV 2010s', 'Dodge Charger SRT Sedan 2013', 'Ford F-150 Pickup Truck 2010s', 'Honda Civic GS Sedan 2009', 'Lamborghini Gallardo Coupe 2005', 'Acura TL V6 Sedan 2014', 'Nissan Xterra SUV 2000s', 'Nissan Versa Sedan 2012', 'Aston Martin DB9 Volante Convertible 2017', 'Mercedes-Benz SLK 320 Convertible 1990s', 'Aston Martin Vantage V8 Coupe Black Edition', 'Audi A5 S Line TDI Quattro Sedan 2012', 'Honda Civic Sedan 2005', 'Chevrolet Camaro LT SS Convertible', 'Chevrolet Malibu Premier Sedan 2018', 'Aston Martin DB9 GTS Coupe 2000s', 'Chevrolet Avalanche LTZ 2012', 'Chevrolet Impala LS V6 Sedan 2000s', 'Aston Martin V8 Supercar Black Edition Coupe 2014', 'GMC Acadia Denali 2012', 'Audi S5 Sedan 2010s', 'Chevrolet Trailblazer SS LTZ 4-Door SUV 2000s', 'Chevrolet Trailblazer SS 4-Door SUV 2010', 'Hyundai Accent Sedan 2015', 'Chevrolet Avalanche LT 2008', 'BMW Z4 Roadster Convertible 2010s', 'Ram 2500 Crew Cab Truck 2015', 'Hyundai Genesis Sedan 2014', 'Honda Civic Sedan 2013', 'Chevrolet Silverado 1500 LT 4x4', 'Cadillac Escalade ESV SUV Black 2010s', 'Audi A4 Convertible 2010', 'Chevrolet Silverado 1500 Extended Cab Truck 2010s', 'Volkswagen Golf GTI Hatchback 2015', 'Audi A4 Quattro Sedan', 'Toyota Camry Sedan 2013', 'Audi A4 B5 2-Door Sedan 1990', 'Audi A3 S Line Sedan 2000s', 'Mazda CX-7 SUV 2010s', 'Bugatti Veyron Super Sport Roadster 2011', 'Chevrolet Suburban LT 2010', 'Aston Martin DB9 V12 Vantage Convertible', 'Chevrolet Impala Premier Sedan 2019', 'Audi TT S Line Coupe 2000s', 'Fiat 500 SR Sedan 2010s', 'Chevrolet Blazer SUV 2000s', 'Audi A4 Sedan Black Edition 2017', 'Hyundai Accent Sedan 2008', 'Aston Martin DB11 V12 Vantage Coupe 2010s', 'BMW M3 GTS Sedan 2011', 'Chevrolet Impala SS Convertible 2000', 'Chrysler Sebring Convertible Limited 2008', 'Audi A4 S4 Sedan 1998', 'Chevrolet Avalanche SUV 2000s', 'Aston Martin DB9 Coupe 2000s', 'Volkswagen Golf Hatchback 2012', 'Honda Civic EK9 Sedan 2001', 'Honda Civic EK Sedan 2003', 'BMW X6 xDrive50i SUV 2015', 'Aston Martin V8 Vantage Convertible 2010', 'Ram 3500 Mega Cab Truck 2019', 'Audi A4 B5 Avant 1995', 'Mercedes Benz SLS C63 AM Coupe 2000', 'Mini Cooper Coupe 2010s', 'BMW X5 XDrive35', 'Audi A3 S Line 4-Door Sedan 2000s', 'BMW 3 Series Touring 2000', 'Chevrolet Silverado 1500 LT Crew Cab Truck 2010s', 'Hyundai Sonata Sedan 2015', 'Porsche Panamera S AWD Sedan 2015', 'Aston Martin DB9 V12 Vantage Roadster Black Roof', 'Bentley Flying Spur W12 Sedan 2019', 'Acura RDX SUV 2010', 'Hyundai Santa Fe SX L', 'Audi A4 Estate 1990s', 'Audi A5 Convertible S Line', 'Acura MDX Sedan 2005', 'Audi A5 S Line Convertible', 'BMW 328i xDrive Wagon 2012', 'Acura TLX Sedan 2013', 'Hummer H4 SUV 2010', 'Buick Verano Sedan 2015', 'Aston Martin V8 Vantage Convertible Black', 'Hummer H2 SUV', 'Bentley Continental GT V8 Coupe 2015', 'BMW 1 Series Convertible M Sport 2016', 'Audi A5 S line Sedan 2010s', 'Aston Martin DB9 V8 Coupe', 'Dodge Caliber SXT Sedan 2012', 'Lamborghini Gallardo LP570-4 Superleggera Coupe 2011', 'Audi A4 Sedan 2015', 'Aston Martin DB11 V12 Vantage SVR', 'Mercedes-Benz SLK R 55 AMG Coupe 2009', 'BMW X3 xDrive30i', 'Bugatti Veyron S Coupe 2000s', 'Toyota MR2 Coupe', 'Acura TLX Silver Sedan 2015', 'Audi A5 S line Sedan 2012', 'Audi R8 Coupe 2010', 'BMW 640i Convertible 2000s', 'Ford Mustang Convertible 2008', 'BMW X3 xDrive35i 2016', 'BMW 3 Series Sedan 2013', 'Aston Martin Vantage GT Coupe 2018', 'Aston Martin Vantage V8 Coupe 2000s', 'Jaguar XF V8 Supercharged Coupe 2010', 'Ford E-350 Van 2010s', 'Smart Fortwo Cabriolet 2008', 'Bugatti Veyron Coupe 2010', 'BMW M5 Sedan 2019', 'Chevrolet Malibu LS Sedan 2014', 'Hyundai Genesis Sedan 2016', 'Chevrolet Cavalier Convertible 1990s', 'Land Rover LR4 SUV 2014', 'Hummer H3T SUV 2000s', 'Chevrolet Camaro ZL1 Convertible 2012', 'Rolls Royce Phantom Sedan 2008', 'Audi A5 TDI S Line 2-Door Sedan', 'BMW Z4 Roadster 2013', 'Toyota MR2 Sedan 2000', 'Aston Martin DB9 Convertible 2010', 'Toyota Scion xB Sedan 2014', 'Audi A4 S Line 4-Door Sedan 200X', 'BMW 650i Convertible 2000s', 'Lamborghini Huracan LP610-4 S Silver', 'BMW 3 Series E90 Wagon 2000', 'Acura TL Type-S Sedan', 'Aston Martin Vantage V8 Coupe 2010s', 'Aston Martin DB9 V12 Vantage Roadster 2000s', 'Aston Martin Vantage Roadster', 'Hyundai Sonata Sedan 2013', 'BMW 1 Series Coupe 2011', 'Volvo C30 2010s Orange Sedan', 'Ford Focus Sedan 2008', 'Chevrolet Express Van', 'Dodge Charger SE Sedan 2000s', 'Lincoln Town Car Executive Sedan 2005', 'Toyota Corolla LE Sedan 2014', 'Audi TT RS MK7 Coupe 2016', 'Toyota Avensis Wagon 2005', 'Audi A4 S4 Sedan 1995', 'Ford E-250 Van 2010s', 'Audi A4 1990s', 'BMW M3 Coupe 2002', 'Mercedes-Benz E-Class Sedan 2015', 'Audi R8 Spyder Coupe 2010s', 'Audi A4 S line Sedan 2011', 'Chevrolet Corvette Coupe 1990s', 'Audi TT RS V8 Coupe 2010s', 'Honda Odyssey LX V6 Minivan 2015', 'Lincoln MKZ Roadster 2015', 'Chrysler 300 C Sedan 2012', 'Audi A4 S line Sedan 2012', 'Audi A3 Convertible 2010s', 'Acura TLX 4-Door Sedan 2012', 'Audi TT Coupe 2005', 'Bentley Continental GT V8 GTC Convertible 2010s', 'Chevrolet Camaro Convertible 2012', 'Honda Civic Coupe 2003', 'Audi A4 2.0T Quattro Sedan 2010s', 'BMW 540i Sedan 2020', 'Audi A4 B Sedan 1990s', 'Hyundai Accent Hatchback 2008', 'Aston Martin DB9 V12 S Convertible 2013', 'Chevrolet Impala Sedan 2008', 'Chevrolet Cobalt SS Sedan 2008', 'BMW X6 xDrive35i SUV 2018', 'Bentley Mulsanne Sedan 2015', 'BMW 3 Series Sedan 2011', 'Nissan Xterra SR Wagon 2006', 'Audi A4 Sedan S Line 2012', 'Audi R8 V10 Spyder Coupe 2010s', 'Audi TT Coupe RS 2008', 'Chrysler 300 SRT8 Sedan 2000s', 'Audi A5 S Line Coupe 2015', 'Acura TL Sedan 2008', 'Aston Martin DB9 V12 Vantage S Convertible', 'Acura RLX Sedan 2017', 'Ford GT Gulf Livery Coupe 2006', 'Dodge Dakota Quad Cab 2012', 'Chevrolet Trailblazer LT SUV 2010', 'Chevrolet Camaro LTZ RS Convertible', 'Ford Fiesta SE Sedan 2013', 'Bentley Continental GT V8 Convertible 2010s', 'Audi A4 B Sedan', 'Acura ILX Sedan 2015', 'Chevrolet Malibu Sedan 2005', 'Audi TT Coupe 2010s', 'Audi S5 Sedan 2010', 'Dodge Grand Caravan 7-Seater Minivan 2010s', 'Acura TLX Silver Sedan', 'Dodge Caliber R/T Sedan 2013', 'Bentley Continental GT V8 Coupe 2020', 'BMW 3 Series Wagon 2000', 'Aston Martin V8 Vantage Convertible 2010s', 'Hummer H3 Convertible 2000s', 'Mercedes-Benz Sprinter Van Silver', 'Audi R8 V10 Plus Coupe 2000s', 'Dodge Charger Hellcat Sedan 2013', 'Mercedes Sprinter ML 190 TDI ML Van', 'Audi S4 Sedan 2000s', 'Jeep Grand Cherokee Overland 2013', 'Jeep Compass Limited 2012', 'Aston Martin Vantage V8 Roadster 2014', 'Aston Martin DB9 V12 Vantage Coupe 2000s', 'Acura TL Sedan 2019', 'BMW X6 M Sport Edition', 'Mazda CX-9 SUV 2010s', 'Acura TLX A-Spec Sedan 2019', 'BMW 3 Series 328i Sedan 2011', 'Suzuki SX4 SUV 2012', 'Audi A4 Sedan 2010s', 'Audi TT RS Plus Coupé 2017', 'Dodge Ram SXT Sedan 2007', 'Bugatti Veyron Blue Roadster 2000s', 'Aston Martin Vantage Coupe 2010s', 'Toyota Yaris Sedan 2005', 'Aston Martin Vantage Roadster 2010s', 'Chevrolet Suburban LT Houston TX', 'Chrysler 300 Sedan 2000s', 'Hyundai Santa Fe SX 2010', 'BMW 1 Series M Coupe 2010s', 'Audi A5 S Line Sedan 4-Door', 'Buick Enclave LS SUV 2010s', 'Acura RLX Sedan 2012', 'BMW 1 Series Convertible M Sport', 'Infiniti QX56 Luxury SUV 2015', 'Volvo XC90 Hybrid SUV 2018', 'Toyota Yaris Sedan 2012', 'GMC Sierra', 'Aston Martin Vantage V8 Coupe 2012', 'Aston Martin DB9 V8 Coupe 2000s', 'Audi A8 Sedan 2000s', 'Audi A5 Convertible S Line 2010s', 'Chevrolet Saturn SLT 5-Seater SUV 2010s', 'Acura ZDX SUV 2012', 'BMW M6 Convertible 2017', 'Bentley Continental GT Coupe 2015', 'Ferrari California T 2-Door Roadster', 'Bentley Mulsanne Extended Wheelbase Sedan 2017', 'Acura TLX Compact Sedan 2012', 'Hyundai Santa Fe SUV', 'Rolls Royce Ghost Sedan 2016', 'Audi A5 Convertible 2010s', 'Audi TT S Line TTS T Coupe Silver', 'Audi S5 Sedan 2012', 'BMW M3 Competition Package Sedan 2012', 'Dodge Charger SRT8 Sedan 2000s', 'Dodge Grand Caravan SXT SUV 2005', 'Toyota Camry SE Sedan 2013', 'Honda Civic Sedan 2003', 'Audi R8 V10 Plus Spyder 2018', 'BMW X5 XDrive35i 2010', 'Infiniti Q50 Coupe 2013', 'Nissan Leaf Sedan 2015', 'Koenigsegg SC1 Coupe 2010s', 'Audi A3 S Line Sedan 2007', 'Chevrolet Corvette Z06 Convertible 2000s', 'BMW X1 Sedan 2018', 'Chevrolet Silverado 1500 Truck 2010s', 'Ford F-150 Truck 2000s', 'Volvo C30 2010s Sedan', 'Volkswagen Golf R Hatchback 2018', 'Aston Martin DB9 Volante Coupe 2000s', 'BMW 5 Series Sedan 2019', 'Mercedes-Benz SLK 200 Convertible 1990s', 'Saturn SLR Coupe 2000', 'Nissan NV3500 Cargo Van 2013', 'BMW M3 Coupe 2008', 'Chevrolet Corvette Stingray Coupe 1990s', 'BMW X6 M Sport SUV 2010s', 'Chevrolet Corvette Convertible 2013 Blue', 'Ford Fiesta Sedan 2013', 'Chevrolet Silverado 1500 LT - ST 2000s Dark Blue Truck', 'Audi A4 B5 Sedan', 'BMW X3 xDrive30d', 'Volkswagen Beetle Hatchback 2010', 'Acura TL AWD Sedan 2010', 'Range Rover Sport SUV 2010s', 'Ferrari FT Swiss Ice Challenge Coupe 2010', 'Acura TL Sedan 2014', 'Toyota Corolla Sedan 2007', 'Acura TL Sedan 2012', 'Audi A4 4-door Sedan', 'Audi A5 Convertible 2015', 'Nissan NV1500 Cargo Van 2019', 'BMW X1 Sedan 2015', 'BMW 1 Series 2-Door Convertible 2010s', 'Chevrolet Cobalt SS Sedan 2007', 'BMW 3 Series M Sport Sedan 2010', 'Bentley Flying Spur V8 Sedan 5-seater', 'Audi R8 V10 Plus Black Coupe 2000s', 'Toyota ECU Sedan 2008', 'Audi TT S Line TTS Coupe', 'Koenigsegg SC1 2-Door Coupe 2010s', 'Ferrari 458 Italia Speciale 2000s', 'Range Rover Vogue SUV 2010s', 'Nissan Sentra Sedan 2005', 'Audi RS4 V10 Plus Coupe 2010s', 'Acura RDX Base 2018', 'Lamborghini Huracan LP610-4 V10 S', 'Audi TT RS Coupe Blue 2000s', 'Audi TT Coupe 200X Silver', 'Bentley Flying Spur V8 Sedan Blue', 'Buick Regal Turbo Sedan 2012', 'Infiniti QX56 SUV 2012', 'Chevrolet Saturn SLT SUV 2010s', 'Audi S5 Coupe 2010s', 'BMW 535i Sedan 2013', 'Honda Accord Sedan 2010', 'Chevrolet Corvette Z06 Coupe White with Red Roof 2010', 'Ferrari 458 Italia Coupe 2000s', 'Hummer H1 Convertible 2000s', 'BMW X6 xDrive35i SUV 2012', 'Aston Martin DB9 V8 Vantage Coupe 2010s', 'Toyota Camry LE Sedan 2013', 'Audi R8 V10 Plus Coupe 2010s', 'Audi A5 S Line Coupe 2019', 'Bentley Continental GT GT GT GT GT', 'Acura ILX Sedan 2016', 'BMW 3 Series Wagon 2010s', 'Honda Civic Coupe 2000', 'Audi A5 Sedan 2010', 'GMC Yukon XL SLT SUV 2010', 'Aston Martin Vantage V12 Coupe 2010s', 'BMW M6 Convertible 2010s', 'Ford Mustang Convertible 2010s', 'Acura ILX Sedan 2018', 'Acura TL Type-S Sedan 2010', 'Chevrolet Traverse LS SUV 2012', 'Buick Regal GT V6 Sedan 2016', 'Audi S5 Coupe 2012', 'Cadillac SRX SUV 2012', 'Volkswagen Polo Wagon 1990s', 'Mercedes-Benz C 300 CDI SD Sedan 2013', 'Honda Civic Hatchback', 'BMW M135i Convertible Coupe 2015', 'Bentley Continental GT Coupe 2010', 'Audi A4 S Line Convertible 2000s', 'Acura TLX AWD w/Sunroof Sedan 2010s', 'Acura TL SH-AWD Sedan 2010', 'Bentley Continental GT Coupe', 'Audi TT Sedan 2010', 'BMW X5 XDrive35d 2011', 'BMW 328i Wagon 2010s', 'Jeep Compass SUV 2010', 'Hummer H3T Truck SUV 2000s', 'Bugatti Veyron Roadster 2000s', 'Ford Mustang GT Convertible 2010s', 'BMW 6 Series Convertible 2000s', 'Audi A4 S Line Sedan 200X Black', 'Acura RL Sedan 2002', 'Chevrolet Silverado 1500 LT W/Trim 2010s', 'Honda Accord Sedan 1990s', 'Aston Martin DB9 V12 S V8 Convertible 2010s', 'Chrysler Pacifica SUV 2019', 'Ford Edge SEL SUV 2012', 'Buick Verano Sedan 2016', 'Aston Martin DB9 V12 Vantage Convertible 2010s', 'Honda Odyssey Sedan 1990s', 'Hyundai Elantra Sedan 2013', 'Audi A4 Sedan 1990s', 'Lamborghini Huracan LP610-4 S Coupe', 'McLaren 570S GT3 RS Orange', 'Lamborghini Gallardo LP560-4 Coupe 2008', 'Acura LTZ Sedan 1990s', 'BMW 5 Series Sedan 2018', 'Audi A5 Coupe Premium Plus 2020', 'Cadillac CTS-V Sedan 2017', 'Aston Martin DB11 V12 S V8 Convertible 2010s', 'Bugatti Veyron Super Sport Coupe 2014', 'Acura Integra Coupe 1995', 'Mercedes-Benz SLK R Coupe 2008', 'Acura TL Sedan 2010', 'Jeep Grand Cherokee SUV 2000s', 'Hyundai Genesis Sedan 2012', 'Hummer H3 1990s', 'Acura TLX Sedan 2019', 'Nissan Pathfinder SUV 2000s', 'BMW X3 sDrive30i', 'Ford GT Coupe 2006', 'Audi A5 Sportback 2017', 'Aston Martin Vantage V8 Convertible 2010s', 'Hyundai Tucson SUV 2010s', 'Audi TT Coupe 200X', 'Audi A5 Coupe S Line 2019', 'Acura ILX Sedan 2012', 'Hummer H3 SUV 2010s', 'Bentley Mulsanne Sedan 2012', 'Aston Martin DB9 Roadster', 'Chevrolet Silverado 1500 LT W/Silver Accents 2010s', 'Audi S5 Cabriolet Concept 2017', 'Audi TT S Line TTS T Coupe 2000s', 'Audi A4 B5 Wagon 1990', 'Aston Martin Vantage V8 S Coupe 2017', 'Rolls Royce Phantom Convertible 2000', 'Bentley Continental GT V8 Coupe Silver 2010s', 'Audi A5 Convertible 2018', 'Chrysler 300 SRT8 Sedan Blue 2000s', 'BMW 325i Wagon 2005', 'Toyota Corolla Sedan 2012', 'Fiat 500 S Sedan 2010s', 'Acura TL Sedan 2015', 'Chrysler PT Cruiser Convertible 2010s', 'Aston Martin V12 Vantage DBS Coupe 2010s', 'Audi TT S T Coupe 2010', 'BMW X6 xDrive35i', 'BMW X6 M SUV 2010s', 'Chevrolet Corvette C7 Convertible 2013 Blue', 'BMW 1 Series M Coupe 2012', 'Hummer H2 SUV 2005', 'Audi S5 TT S Line Sedan 2010s', 'Audi S5 Coupe 2018', 'Hummer H1 Convertible SUV 2000s', 'Toyota MR2 Coupe 2001', 'Hyundai Santa Fe 2010s', 'Infiniti Q50 S V6 Coupe 2013', 'Audi S5 Cabriolet 2013', 'Audi TT S Line Coupe 2010', 'GMC Yukon XL Denali SUV 2010', 'Acura RLX Sedan 2018', 'Chevrolet Savana Minivan 2003', 'Chevrolet Impala LT Sedan 2008', 'BMW X6 xDrive35i SUV 2010s', 'Buick Enclave Avenir SUV 2019', 'BMW X6 xDrive50i', 'Audi A4 S Line TT Convertible 2000s', 'Mercedes-Benz E-Class Sedan 2017', 'Dodge Grand Caravan SE 2003', 'GMC Terrain SLE-2 SUV 2012', 'Aston Martin V8 Vantage Coupe 2012', 'Ram 1500 Truck 2010s', 'Jeep Wrangler Sport Orange', 'Cadillac Escalade EXT 2014', 'Hyundai Sonata Limited Sedan 2015', 'Aston Martin DB9 Coupe 2015', 'Aston Martin DB9 V12 S V12 Convertible 2010', 'Aston Martin DB9 V12 S Roadster 2013', 'Aston Martin SLR Coupe 2000', 'Audi TT RS TTS TTS T Coupe 2010s', 'BMW X3 sDrive20i 2019', 'Chevrolet Saturn SLT 4-Door SUV 2010s', 'Chevrolet Malibu LS Sedan 2012', 'Toyota MR2 Convertible 2002', 'Chevrolet Silverado 1500 LT Crew Cab Diesel 2018', 'Audi A5 S Line Coupe 2012', 'Acura TL AWD Sedan 2012', 'Aston Martin DB9 Coupe 2014', 'Audi TT RS MK7 Coupe 2008', 'Chevrolet Corvette Z06 Convertible 2010s', 'Hyundai Tucson SRT-4 SUV 2010s', 'Kia K900 Silver Coupe 2010s', 'Aston Martin DB9 V12 Vantage Coupe', 'Volvo C30 Sedan 2013', 'Fiat 500 Sedan 2005', 'Audi TT RS Plus Coupé 2019', 'Chevrolet Traverse SUV 2010', 'McLaren 570S GT3 RS Coupe', 'Bentley Flying Spur Sedan 2012', 'Smart Fortwo Passion Cabriolet 2012', 'Acura MDX SUV 2012', 'Bentley Continental GT 2-Door Coupe', 'Chevrolet Silverado 1500 LT Crew Cab 2010s', 'Koenigsegg SC1 Sports Car 2010s', 'Saturn SL2 Convertible 1990s', 'Hyundai Sonata Sedan 2018', 'GMC Savana 3500 Passenger Van 2019', 'Bentley Continental GT Sports Coupe 2010s', 'BMW 135i Coupe 2010', 'BMW 5 Series Sedan 2010', 'Bugatti Veyron 2-Door Roadster', 'Lamborghini Huracan LP610-4 S 2010s', 'Chevrolet Avalanche SUV 2010s', 'Honda Civic LX Sedan 2003', 'Nissan Juke RS GT-R Sedan 2010s', 'Ford E-150 Van 2010s', 'Bentley Continental GT V8 Coupe Silver 2010s with 20-inch Wheels', 'Audi A5 Sedan 2013', 'Chevrolet Silverado 1500 LT 2000s', 'Aston Martin DB9 V12 S Roadster', 'Audi A4 S Line Sedan 200X', 'Acura RDX Black Edition 2015', 'Bentley Continental GT Coupe 2011', 'BMW 5 Series M5 Sedan 2010', 'Acura TLX Sedan 2011', 'BMW X5 XDrive35d', 'Chrysler Sebring Convertible Touring 2007', 'BMW X3 sDrive28i 2014', 'Audi A4 S line Sedan 2010s', 'Aston Martin DB9 V12 S V Convertible 2015', 'Dodge Grand Caravan 2000', 'Acura TLX Blue Sedan 2012', 'Mercedes-Benz SLK R 350 Coupe 2006', 'Audi A4 Sedan', 'Audi A5 Coupe 2013', 'Aston Martin Vantage V8 Coupe 2010', 'Acura TSX Sedan 2009', 'Bentley Continental GT V8 GTS Convertible 2016', 'Rolls Royce Phantom Drophead Coupe 2014', 'Audi A5 S Line Sedan 2010s', 'BMW M550i xDrive Sedan 2018', 'Honda Civic Type R 1990s Sedan', 'Audi A4 B5 TDI S Line Sedan 1990s', 'Chevrolet Traverse LT 7-Seater SUV 2010s', 'Chevrolet Blazer-inspired GMC Sierra Orange Pickup Truck 2010s', 'Tesla Model S Sedan 2018', 'Audi A5 Convertible 2013', 'Audi A4 B5 S4 Sedan 1990s', 'Honda Accord Sport Sedan 2012', 'Jeep Liberty Sport SUV 2010', 'Honda Civic Sedan 1990', 'Bugatti Veyron Grand Sport Vitesse Coupe 2010s', 'Dodge Challenger SRT Hellcat Sedan 2000s', 'Acura TLX Black Sedan 2012', 'BMW X5 XDrive35i M Sport SUV 2000s', 'Audi S4 Cabriolet 2010s', 'Aston Martin DB9 Coupe 2010', 'Toyota Scion XD Sedan 2015', 'Acura Integra SL Coupe', 'BMW 5 Series Sedan 2015', 'Audi TT S Line Coupe 2009', 'Chevrolet Volt SUV 2010s', 'Audi A4 S4 Sedan 1999', 'Honda Accord LX Sedan 2015', 'Aston Martin DB9 V12 S Vantage Roadster 2010', 'BMW 1 Series Convertible 2010s', 'Chevrolet Malibu Sedan 2012', 'Chevrolet Sonic LT Sedan 2015', 'Buick Regal Premium Sedan 2010', 'Hyundai Veloster S 2-Door Coupe 2010s', 'Audi A4 Sedan 2012', 'Hummer H1 SUV 2000s', 'Chevrolet Express 1500 Cargo Van', 'Audi RS5 Coupe 2018', 'Ferrari 458 Italia Roadster 2000s', 'Chevrolet Silverado 1500 LT - SACRA 2000s Dark Blue Truck', 'Chrysler Grand Caravan 2011 Minivan', 'BMW 3 Series 335i Sedan 2012', 'Audi A4 TT Sedan 2010', 'Buick Verano Sedan 2012', 'Ford F-150 XL SuperCab 2010s', 'BMW Z4 sDrive28i Coupe 2010', 'BMW M5 Sedan Silver Black 2010s', 'Audi TT S Coupe 2005', 'Bentley Continental GT V8 GTS Convertible 2012', 'Acura RDX SUV 2012', 'BMW M3 Sedan 2010', 'Buick Regal GT Sedan 2010', 'BMW M5 Sedan 2014', 'Porsche Panamera 4S AWD Sedan 2015', 'Audi TT S Line Coupe Silver', 'Smart Fortwo Convertible 2005', 'Audi A4 Wagon 2000s', 'Ford F-150 XL Truck 2000s', 'Rolls Royce Ghost Sedan 2014', 'Chevrolet Impala LT Sedan 2014', 'Mini Cooper S Convertible 2010s', 'BMW M135i M Sport Coupe 2014', 'Dodge Charger SRT Hellcat 2000s', 'Jeep Patriot SUV 201X', 'Audi RS5 Coupe 2012', 'Hyundai Veloster S Small Coupe 2010s', 'BMW 6 Series Convertible 2010', 'Audi TT S Line RS Coupe 2000s', 'Audi A5 Coupe 2018', 'Honda Odyssey LX V6 Minivan 2010', 'Jeep Grand Cherokee SRT8 2013', 'Infiniti G35 Coupe 2013', 'Acura TSX Sedan 2007', 'Bentley Mulsanne Sedan 2018', 'Audi A6 S Line Sedan 2000s', 'Audi TT S Line Coupe 2007', 'Tesla Model S Sedan 2016', 'Volkswagen New Beetle 2-Door Hatchback 2010', 'Aston Martin DB9 V12 S V12 Convertible 2010s', 'Chevrolet Silverado 3500HD LTZ 4x4', 'Mercedes-Benz Sprinter 2.0 TDI 115 HP Van', 'BMW M6 Convertible 2000s', 'Chevrolet Express Passenger Van 2000s', 'Hyundai Sonata GLS Sedan 2015', 'BMW X3 M40i', 'Hummer H1 SUV', 'Chevrolet Silverado 1500 LTZ 2011', 'BMW X6 M Sport SUV 2010', 'Acura TLX Sedan', 'Audi A4 S Line 5-Seater Sedan 2010s', 'GMC Acadia SUV 2012', 'Aston Martin DB9 V8 Vantage Roadster', 'Dodge Charger SRT Hellcat Sedan 2013', 'Audi RS4 Sedan', 'Acura TL V6 Sedan 2010', 'Audi R8 GT Coupe 2013', 'Dodge Grand Caravan Minivan 2007', 'Aston Martin DB9 V12 Vantage Roadster White', 'Saturn SLT Sedan 2005', 'Acura ILX Sedan 2014', 'Aston Martin DB11 V8 S Roadster 2018', 'Acura RDX Sedan 2010', 'Audi TT RS Coupe 2010s', 'Chrysler 300 Limited Sedan 2015', 'Ford Bronco Sport SUV 2003', 'Chevrolet Sonic LT Sedan 2014', 'Dodge Caliber SXT Sedan 2007', 'Cadillac SRX Red 2013', 'BMW 1 Series Sedan 2012', 'Buick Regal GT V6 Sedan 2012', 'Honda Civic EK4 Sedan 2000', 'Audi R8 GT Spyder Coupe 2010s', 'Buick Regal GT V6 Sedan 2014', 'Lincoln Town Car Signature Limited Sedan 2010', 'Fiat Bravo Sedan 2009', 'Honda Accord EX-L Sedan 2017', 'Saturn SLT Sedan 2007', 'Audi Q7 SUV 2000s', 'Honda Accord EX-L Sedan 2014', 'BMW M6 Convertible 2012', 'Acura TLX Mid-sized Sedan', 'BMW 128i Convertible', 'Ford F-350 DRW 4x4 Pickup Truck 2016', 'Cadillac SRX SUV 2013', 'Audi R8 V10 Plus Performance 2020', 'Jeep Compass Trailhawk 2014', 'Chevrolet Silverado 1500 LT W/Trailer', 'Audi TT Coupe 2008', 'Chevrolet Corvette Z06 Coupe 2010s', 'Aston Martin V8 Vantage Roadster 2010', 'Ford Bronco Eddie Bauer Edition 2008', 'Audi A5 Convertible Concept 2000s', 'Cadillac Escalade Luxury SUV Black 2010s', 'Nissan Juke RS Sedan 2010s', 'Chevrolet Cobalt SS Sedan 2009', 'Chevrolet Corvette Z06 Convertible 2020s', 'Hummer H4 SUV 2000s', 'Mercedes Sprinter 2.0 TDI SLK 190 Van', 'Aston Martin Vantage Coupe 200X', 'Aston Martin V8 Vantage Coupe 200X', 'Audi TT TTS Coupe 2010s', 'Aston Martin DB11 V12 S Roadster', 'Audi A4 S Line Sedan 2012', 'BMW 1 Series Coupe 2010', 'Audi A4 Sedan 2013', 'Buick Enclave SUV 2010s', 'Aston Martin DB9 V8 S', 'Chevrolet Corvette Z06 Coupe 2007', 'Chevrolet Express Cargo Van 2000s', 'Fiat 500 SRT8 Sedan 2010s', 'Chrysler PT Cruiser Convertible 2012', 'Audi TT S Line TTS Coupe 2000s', 'BMW X5 XDrive35d M Sport SUV 2000s', 'Ferrari FT Coupe 2010s', 'Mercedes-Benz SLK 230 Convertible 1990s', 'Acura Integra Type R Coupe 1997', 'Audi A3 S Line Sedan 2009', 'Aston Martin DB9', 'Volvo 850 Wagon 1990s', 'Aston DB5', 'Ford E-150 Minivan 2005', 'Honda Odyssey Minivan 2018', 'Honda Civic Coupe', 'Chevrolet Express Van LT SWB 2000s', 'Audi TT Coupe 2010', 'Chevrolet Silverado 1500 LT w/Slee Truck 2010s', 'Audi R8 V10 Plus RS4 V10 Plus Coupe 2010s', 'Ford Focus Sedan 2005', 'BMW X5 SUV', 'Bentley Continental GT V8 Coupe 2018', 'Chrysler Sebring Convertible 2005', 'Acura TLX AWD Sedan 2012', 'Honda Civic EK 1990s Sedan', 'Mazda CX-5 SUV 2010s', 'Bugatti Veyron S V12 Coupe 2000s', 'Hyundai Accent SE Sedan 2013', 'Aston Martin DB9 V12 S Cabriolet 2013', 'Hyundai Accent Sedan 2010', 'Aston Martin DB9 Vantage Coupe 2010s', 'Dodge Grand Caravan SXT 2008', 'Bentley Mulsanne Sedan 2014', 'Lamborghini Huracan LP610-4 V10 S Coupe', 'Volvo XC90 SUV 2015', 'Chevrolet Impala LS Sedan 2000s', 'Dodge Grand Caravan 2010s', 'Nissan Juke SR spec GT-R Sedan 2010s', 'Chevrolet Avalanche LT3 2010', 'Acura TLX Sedan 2017', 'Mercedes-Benz S-Class Sedan', 'Ford Bronco XLT Truck 2005', 'Dodge Journey SUV 2010s', 'Dodge Dakota Pickup Truck 2005', 'Dodge Charger 2000s', 'Audi A3 S Line Sedan 2005', 'Buick Verano Sedan 2018', 'Mercedes-Benz Sprinter Van 2000s', 'Honda Civic SI Sedan 2007', 'Ford Edge SUV 2010', 'BMW X5 XDrive35 2012', 'Mitsubishi Lancer Evo X GTS R HD Sedan', 'Cadillac Escalade SUV Black 2010s', 'Bugatti Veyron Blue Roadster', 'Dodge Challenger R/T Sedan 2000s', 'BMW X1 Sedan 2012', 'BMW 330i xDrive Wagon 2010s', 'Toyota Sequoia Limited SUV 2010', 'Chrysler 300 Sedan 2017', 'Audi A5 S Line TDI Quattro Sedan 2013', 'Hummer H3 SUV 2000s', 'Hyundai Accent Sedan 2005', 'Ferrari California T Roadster 2000s', 'Acura RDX Sedan 2012', 'Audi A3 Convertible S Line', 'Ford GT Heritage Edition Coupe 2006', 'BMW Z4 Roadster 2010s', 'Audi A4 S Line 4-Door Sedan 2010s', 'Aston Martin DB9 V12 Vantage Coupe 2010', 'Dodge Grand Caravan SUV 2005', 'Chevrolet Corvette Roadster 2013 Convertible Blue', 'Volkswagen Golf Mk5 Hatchback 1990s', 'Cadillac XT5 SUV 2012', 'Audi R8 V10 Plus 2-Door Coupe 2010s', 'GMC Savana 1500 Explorer Limited SE Van 2019', 'Hyundai Santa Fe SX SX 2010', 'Aston Martin V8 Vantage S Coupe 2010s', 'Ram 1500 Quad Cab Truck 2018', 'Dodge Ram 2500 SUV 2006', 'Hyundai Accent GLS Sedan 2017', 'BMW 650i Convertible 2010', 'Audi A4 B5 TDI S Line Sedan 1990', 'Audi S5 Coupe 2019', 'BMW M6 Convertible 2015', 'Honda Prelude Convertible 1990s', 'Mercedes-Benz E-Class Sedan 2019', 'BMW 5 Series Sedan 2011', 'Chevrolet Sonic RS Sedan 2013', 'Acura ILX Sedan 2013', 'Dodge Dakota Sport Extended Cab 2008', 'Audi A5 TDI S Line Coupe', 'Rolls Royce Phantom Sedan 2005', 'Aston Martin Vantage DB9 Convertible 2015', 'Chevrolet Silverado 1500 LT Truck 2010s', 'Dodge Journey Crossroad SUV 2010s', 'Chevrolet Malibu LT Sedan 2016', 'Bentley Continental GT Coupe 2005', 'Audi R8 V10 Plus Coupe 2017', 'BMW 3 Series Sedan 2010', 'Ford Focus Sedan 2012', 'BMW M135i Coupe 2012', 'Acura TLX Sedan 2012', 'Aston Martin V8 Supercar Coupe 2010', 'Jeep Wrangler Sahara Unlimited Orange', 'Hummer H3 SUV', 'Audi S5 S Line Cabriolet 2012', 'Cadillac Escalade SUV 2012', 'Acura TL V6 Sedan 2015', 'Acura TSX Sedan 2010', 'Chevrolet Trailblazer SS LTZ 5-Seat SUV 2000s', 'Audi R8 V10 Coupe 2010s', 'Chevrolet Cobalt SS Sedan 2005', 'Volvo XC90 T6 Inscription 2019', 'Chevrolet Traverse LT SUV 2015', 'Rolls Royce Phantom Drophead Convertible 2007', 'Aston Martin DB9 V12 Coupe 2000s', 'Toyota Camry Sedan 1990s', 'Nissan Leaf Sedan 2017', 'Audi R8 GT Coupe 2011', 'Audi A4 S line Sedan 2000s', 'Audi R8 V10 Coupe 2012', 'Audi A4 S Line 2013', 'Kia K900 Coupe 2010s', 'Toyota Yaris Sedan 2014', 'GMC Acadia SLT 2012', 'Audi TT S Line TTS T Coupe 20-inch Wheels', 'Bentley Mulsanne Speed Sedan 2019', 'Bentley Continental GT V8 2-Door Convertible 2010s', 'Bugatti Veyron Coupe 2010s', 'Aston Martin Vantage S Coupe 2016', 'Land Rover Range Rover Sport SUV 2015', 'Aston Martin DB9 GT3 Coupe 2000s', 'Rolls Royce Phantom Sedan 2010', 'Acura RLX Sedan 2013', 'Acura TLX Luxury Sedan 2012', 'Chevrolet Silverado 1500 LT 2000s Dark Blue Truck', 'Chevrolet Silverado 1500 LT Truck', 'Aston Martin Vantage S Coupe 2010s', 'Jeep Patriot Limited SUV 201X', 'Aston Martin DB9 V8 S Roadster', 'Chevrolet Suburban LS 2012', 'Suzuki SCX SUV 2012', 'Acura RDX Sedan 2017', 'BMW M3 Coupe 2005', 'BMW M6 Convertible 2010', 'Bugatti Veyron Grand Sport Coupe 2011', 'Audi R8 V10 Plus Coupe 2019', 'Toyota Celica GTS Coupe', 'Audi A4 Sedan 2016', 'Chevrolet Malibu LS Compact Sedan 2012', 'Acura ILX Sedan 2017', 'Mercedes-Benz C 300 Sedan 2013', 'Mercedes-Benz Sprinter Van 2010s', 'Bugatti Veyron SC V12 Coupe 2000s', 'Lamborghini Gallardo LP560-4 Coupe 2007', 'Aston Martin V8 Supercharged V12 Coupe 2012', 'Chevrolet Trailblazer SS SUV 2010', 'Bentley Flying Spur V8 Sedan 2000s', 'Audi TT S Coupe Blue 2000s', 'Ford Fiesta Titanium Sedan 2013', 'Bentley Continental GT GTV8', 'Tesla Model S Sedan 2014', 'Chevrolet Corvette Grand Sport Coupe 2010s', 'Kia K900 Concept Coupe 2010s', 'Honda Civic Type R Coupe 2017', 'BMW M6 Convertible 2019', 'Audi A4 B5 Quattro 1998', 'Dodge Caliber SE Sedan 2011', 'Chevrolet Sonic Sedan 2013', 'Audi A5 Cabriolet 2013', 'Audi Q5 Convertible 2010s', 'Acura TLX 2012', 'Audi A5 S Line Sedan Black', 'BMW 1 Series 118i Sedan 2018', 'BMW 135i Convertible', 'GMC Terrain Denali SUV 2012', 'Dodge Charger SRT 2000s', 'Jeep Liberty Limited SUV 2010', 'Volkswagen Bora SUV 2000s', 'Aston Martin DB9 GT Coupe 2016']
0it [00:00, ?it/s]1it [00:01,  1.75s/it]2it [00:03,  1.58s/it]3it [00:04,  1.57s/it]4it [00:07,  2.06s/it]5it [00:09,  2.00s/it]6it [00:11,  2.13s/it]7it [00:13,  2.01s/it]8it [00:15,  1.85s/it]9it [00:16,  1.76s/it]10it [00:17,  1.61s/it]11it [00:20,  1.76s/it]12it [00:22,  1.91s/it]13it [00:24,  2.02s/it]14it [00:26,  2.08s/it]15it [00:28,  2.09s/it]16it [00:31,  2.29s/it]17it [00:33,  2.23s/it]18it [00:35,  2.15s/it]19it [00:38,  2.22s/it]20it [00:40,  2.27s/it]21it [00:42,  2.23s/it]22it [00:44,  2.20s/it]23it [00:46,  2.17s/it]24it [00:49,  2.15s/it]25it [00:51,  2.25s/it]26it [00:55,  2.70s/it]27it [00:57,  2.61s/it]28it [00:59,  2.49s/it]29it [01:02,  2.48s/it]30it [01:04,  2.33s/it]31it [01:06,  2.20s/it]32it [01:08,  2.12s/it]33it [01:10,  2.28s/it]34it [01:12,  2.24s/it]35it [01:14,  2.09s/it]36it [01:16,  2.08s/it]37it [01:18,  2.00s/it]38it [01:21,  2.18s/it]39it [01:22,  2.03s/it]40it [01:25,  2.11s/it]41it [01:27,  2.15s/it]42it [01:29,  2.05s/it]43it [01:31,  2.02s/it]44it [01:33,  2.05s/it]45it [01:35,  2.25s/it]46it [01:38,  2.33s/it]47it [01:40,  2.35s/it]48it [01:43,  2.30s/it]49it [01:45,  2.23s/it]50it [01:47,  2.21s/it]51it [01:49,  2.35s/it]52it [01:52,  2.29s/it]53it [01:54,  2.24s/it]54it [01:56,  2.39s/it]55it [01:58,  2.22s/it]56it [02:00,  2.12s/it]57it [02:02,  2.01s/it]58it [02:04,  2.06s/it]59it [02:06,  2.13s/it]60it [02:09,  2.13s/it]61it [02:10,  2.07s/it]62it [02:12,  2.04s/it]63it [02:15,  2.07s/it]64it [02:17,  2.08s/it]65it [02:18,  1.90s/it]66it [02:20,  1.81s/it]67it [02:22,  1.93s/it]68it [02:24,  2.00s/it]69it [02:27,  2.12s/it]70it [02:28,  2.07s/it]71it [02:30,  2.04s/it]72it [02:32,  2.04s/it]73it [02:35,  2.08s/it]74it [02:36,  2.00s/it]75it [02:38,  1.94s/it]76it [02:40,  1.97s/it]77it [02:42,  1.99s/it]78it [02:44,  2.03s/it]79it [02:46,  2.02s/it]80it [02:49,  2.19s/it]81it [02:53,  2.68s/it]82it [02:56,  2.74s/it]83it [02:59,  2.81s/it]84it [03:02,  2.80s/it]85it [03:04,  2.81s/it]86it [03:07,  2.79s/it]87it [03:10,  2.81s/it]88it [03:13,  2.78s/it]89it [03:16,  2.86s/it]90it [03:18,  2.74s/it]91it [03:21,  2.76s/it]92it [03:24,  2.79s/it]93it [03:27,  2.84s/it]94it [03:30,  2.91s/it]95it [03:32,  2.83s/it]96it [03:35,  2.77s/it]97it [03:38,  2.67s/it]98it [03:40,  2.62s/it]99it [03:43,  2.61s/it]100it [03:45,  2.62s/it]101it [03:49,  2.80s/it]102it [03:51,  2.84s/it]103it [03:54,  2.72s/it]104it [03:56,  2.66s/it]105it [03:59,  2.65s/it]106it [04:02,  2.81s/it]107it [04:05,  2.81s/it]108it [04:07,  2.68s/it]109it [04:10,  2.70s/it]110it [04:12,  2.57s/it]111it [04:15,  2.44s/it]112it [04:17,  2.36s/it]113it [04:19,  2.33s/it]114it [04:23,  2.73s/it]115it [04:25,  2.54s/it]116it [04:27,  2.43s/it]117it [04:30,  2.56s/it]118it [04:33,  2.73s/it]119it [04:35,  2.52s/it]120it [04:37,  2.38s/it]121it [04:39,  2.21s/it]122it [04:40,  1.99s/it]123it [04:42,  1.95s/it]124it [04:43,  1.76s/it]125it [04:45,  1.67s/it]126it [04:46,  1.65s/it]127it [04:48,  1.49s/it]128it [04:49,  1.36s/it]129it [04:50,  1.32s/it]130it [04:52,  1.58s/it]131it [04:54,  1.82s/it]132it [04:56,  1.63s/it]133it [04:57,  1.42s/it]134it [04:58,  1.38s/it]135it [04:59,  1.28s/it]136it [05:00,  1.23s/it]137it [05:01,  1.27s/it]138it [05:03,  1.51s/it]139it [05:05,  1.51s/it]140it [05:07,  1.57s/it]141it [05:09,  1.72s/it]142it [05:10,  1.73s/it]143it [05:12,  1.70s/it]144it [05:14,  1.64s/it]145it [05:15,  1.57s/it]146it [05:17,  1.58s/it]147it [05:18,  1.42s/it]148it [05:18,  1.16s/it]149it [05:20,  1.30s/it]150it [05:22,  1.51s/it]151it [05:24,  1.69s/it]152it [05:26,  1.83s/it]153it [05:28,  1.97s/it]154it [05:31,  2.21s/it]155it [05:32,  1.93s/it]156it [05:35,  2.18s/it]157it [05:38,  2.41s/it]158it [05:41,  2.48s/it]159it [05:44,  2.71s/it]160it [05:47,  2.87s/it]161it [05:50,  2.86s/it]162it [05:53,  2.79s/it]163it [05:56,  3.01s/it]164it [05:59,  2.90s/it]165it [06:02,  2.97s/it]166it [06:06,  3.19s/it]167it [06:10,  3.45s/it]168it [06:14,  3.55s/it]169it [06:17,  3.56s/it]170it [06:21,  3.51s/it]171it [06:25,  3.66s/it]172it [06:28,  3.56s/it]173it [06:31,  3.43s/it]174it [06:35,  3.62s/it]175it [06:39,  3.58s/it]176it [06:43,  3.78s/it]177it [06:46,  3.55s/it]178it [06:49,  3.42s/it]179it [06:52,  3.41s/it]180it [06:56,  3.57s/it]181it [07:00,  3.66s/it]182it [07:04,  3.70s/it]183it [07:07,  3.64s/it]184it [07:12,  3.84s/it]185it [07:16,  3.82s/it]186it [07:20,  3.86s/it]187it [07:24,  3.90s/it]188it [07:29,  4.23s/it]189it [07:33,  4.33s/it]190it [07:37,  4.21s/it]191it [07:41,  4.20s/it]192it [07:45,  4.11s/it]193it [07:49,  4.19s/it]194it [07:54,  4.15s/it]195it [07:57,  4.09s/it]196it [08:01,  4.03s/it]197it [08:05,  4.01s/it]198it [08:10,  4.18s/it]199it [08:14,  4.21s/it]200it [08:18,  4.14s/it]201it [08:22,  4.03s/it]202it [08:26,  4.11s/it]203it [08:31,  4.16s/it]203it [08:31,  2.52s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]  0%|          | 1/252 [00:03<12:50,  3.07s/it]  1%|          | 3/252 [00:03<03:28,  1.19it/s]  2%|▏         | 5/252 [00:03<01:48,  2.28it/s]  3%|▎         | 7/252 [00:03<01:08,  3.59it/s]  4%|▍         | 11/252 [00:03<00:34,  7.07it/s]  6%|▌         | 15/252 [00:03<00:21, 10.88it/s]  8%|▊         | 19/252 [00:03<00:15, 14.80it/s]  9%|▉         | 23/252 [00:03<00:12, 18.37it/s] 11%|█         | 27/252 [00:04<00:10, 21.93it/s] 12%|█▏        | 31/252 [00:04<00:08, 24.99it/s] 14%|█▍        | 35/252 [00:04<00:11, 18.53it/s] 15%|█▌        | 38/252 [00:04<00:12, 17.04it/s] 16%|█▋        | 41/252 [00:04<00:11, 18.77it/s] 18%|█▊        | 45/252 [00:04<00:09, 21.68it/s] 19%|█▉        | 48/252 [00:05<00:11, 17.53it/s] 20%|██        | 51/252 [00:05<00:10, 19.13it/s] 21%|██▏       | 54/252 [00:05<00:14, 13.99it/s] 23%|██▎       | 57/252 [00:05<00:12, 15.97it/s] 24%|██▍       | 60/252 [00:05<00:10, 18.12it/s] 25%|██▌       | 63/252 [00:06<00:12, 15.23it/s] 26%|██▌       | 66/252 [00:06<00:10, 17.42it/s] 28%|██▊       | 70/252 [00:06<00:08, 21.11it/s] 29%|██▉       | 74/252 [00:06<00:07, 23.92it/s] 31%|███       | 78/252 [00:06<00:06, 26.62it/s] 33%|███▎      | 82/252 [00:06<00:05, 28.43it/s] 34%|███▍      | 86/252 [00:06<00:05, 29.83it/s] 36%|███▌      | 90/252 [00:07<00:05, 30.05it/s] 37%|███▋      | 94/252 [00:07<00:05, 31.24it/s] 39%|███▉      | 98/252 [00:07<00:04, 31.31it/s] 40%|████      | 102/252 [00:07<00:05, 29.58it/s] 42%|████▏     | 106/252 [00:07<00:04, 30.41it/s] 44%|████▎     | 110/252 [00:07<00:04, 31.20it/s] 45%|████▌     | 114/252 [00:07<00:04, 31.27it/s] 47%|████▋     | 118/252 [00:07<00:04, 32.03it/s] 48%|████▊     | 122/252 [00:08<00:03, 32.59it/s] 50%|█████     | 126/252 [00:08<00:03, 32.96it/s] 52%|█████▏    | 130/252 [00:08<00:03, 33.60it/s] 53%|█████▎    | 134/252 [00:08<00:03, 31.94it/s] 55%|█████▍    | 138/252 [00:08<00:03, 31.71it/s] 56%|█████▋    | 142/252 [00:08<00:03, 32.21it/s] 58%|█████▊    | 146/252 [00:08<00:03, 32.20it/s] 60%|█████▉    | 150/252 [00:08<00:03, 33.05it/s] 61%|██████    | 154/252 [00:08<00:02, 33.21it/s] 63%|██████▎   | 158/252 [00:09<00:02, 33.81it/s] 64%|██████▍   | 162/252 [00:09<00:02, 32.66it/s] 66%|██████▌   | 166/252 [00:09<00:03, 25.98it/s] 67%|██████▋   | 170/252 [00:09<00:02, 27.74it/s] 69%|██████▉   | 174/252 [00:09<00:02, 28.59it/s] 71%|███████   | 178/252 [00:09<00:02, 30.18it/s] 72%|███████▏  | 182/252 [00:09<00:02, 27.52it/s] 74%|███████▍  | 186/252 [00:10<00:02, 28.96it/s] 75%|███████▌  | 190/252 [00:10<00:02, 29.83it/s] 77%|███████▋  | 194/252 [00:10<00:01, 30.78it/s] 79%|███████▊  | 198/252 [00:10<00:01, 31.91it/s] 80%|████████  | 202/252 [00:10<00:01, 31.99it/s] 82%|████████▏ | 206/252 [00:10<00:01, 32.81it/s] 83%|████████▎ | 210/252 [00:10<00:01, 32.41it/s] 85%|████████▍ | 214/252 [00:10<00:01, 32.83it/s] 87%|████████▋ | 218/252 [00:11<00:01, 32.41it/s] 88%|████████▊ | 222/252 [00:11<00:01, 29.24it/s] 90%|████████▉ | 226/252 [00:11<00:00, 29.90it/s] 91%|█████████▏| 230/252 [00:11<00:00, 30.85it/s] 93%|█████████▎| 234/252 [00:11<00:00, 31.41it/s] 94%|█████████▍| 238/252 [00:11<00:00, 32.09it/s] 96%|█████████▌| 242/252 [00:11<00:00, 29.89it/s] 98%|█████████▊| 246/252 [00:12<00:00, 31.41it/s] 99%|█████████▉| 250/252 [00:12<00:00, 32.48it/s]100%|██████████| 252/252 [00:12<00:00, 19.95it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:01<07:15,  1.73s/it]  1%|          | 2/252 [00:01<03:20,  1.25it/s]  2%|▏         | 6/252 [00:02<00:50,  4.84it/s]  4%|▍         | 10/252 [00:02<00:27,  8.79it/s]  6%|▌         | 14/252 [00:02<00:18, 12.87it/s]  7%|▋         | 18/252 [00:02<00:13, 16.89it/s]  9%|▊         | 22/252 [00:02<00:11, 20.47it/s] 10%|█         | 26/252 [00:02<00:09, 23.64it/s] 12%|█▏        | 30/252 [00:02<00:08, 26.10it/s] 13%|█▎        | 34/252 [00:02<00:07, 28.12it/s] 15%|█▌        | 38/252 [00:02<00:07, 29.76it/s] 17%|█▋        | 42/252 [00:03<00:06, 31.02it/s] 18%|█▊        | 46/252 [00:03<00:06, 32.11it/s] 20%|█▉        | 50/252 [00:03<00:06, 31.89it/s] 21%|██▏       | 54/252 [00:03<00:06, 32.60it/s] 23%|██▎       | 58/252 [00:03<00:07, 25.87it/s] 25%|██▍       | 62/252 [00:03<00:06, 27.88it/s] 26%|██▌       | 66/252 [00:03<00:06, 29.46it/s] 28%|██▊       | 70/252 [00:04<00:05, 30.52it/s] 29%|██▉       | 74/252 [00:04<00:09, 18.63it/s] 31%|███       | 78/252 [00:04<00:08, 21.41it/s] 33%|███▎      | 82/252 [00:04<00:07, 24.18it/s] 34%|███▍      | 86/252 [00:04<00:06, 26.58it/s] 36%|███▌      | 90/252 [00:04<00:05, 28.32it/s] 37%|███▋      | 94/252 [00:05<00:05, 29.84it/s] 39%|███▉      | 98/252 [00:05<00:05, 29.93it/s] 40%|████      | 102/252 [00:05<00:04, 31.10it/s] 42%|████▏     | 106/252 [00:05<00:04, 31.73it/s] 44%|████▎     | 110/252 [00:05<00:04, 31.88it/s] 45%|████▌     | 114/252 [00:05<00:04, 32.27it/s] 47%|████▋     | 118/252 [00:05<00:04, 32.91it/s] 48%|████▊     | 122/252 [00:05<00:03, 32.51it/s] 50%|█████     | 126/252 [00:05<00:03, 32.93it/s] 52%|█████▏    | 130/252 [00:06<00:03, 32.79it/s] 53%|█████▎    | 134/252 [00:06<00:03, 33.11it/s] 55%|█████▍    | 138/252 [00:06<00:03, 33.28it/s] 56%|█████▋    | 142/252 [00:06<00:06, 17.42it/s] 58%|█████▊    | 145/252 [00:06<00:05, 18.66it/s] 59%|█████▉    | 149/252 [00:07<00:04, 21.85it/s] 61%|██████    | 153/252 [00:07<00:04, 24.46it/s] 62%|██████▏   | 157/252 [00:07<00:03, 26.48it/s] 64%|██████▍   | 161/252 [00:07<00:03, 28.53it/s] 65%|██████▌   | 165/252 [00:07<00:02, 29.31it/s] 67%|██████▋   | 169/252 [00:07<00:02, 30.73it/s] 69%|██████▊   | 173/252 [00:07<00:02, 31.81it/s] 70%|███████   | 177/252 [00:07<00:02, 32.22it/s] 72%|███████▏  | 181/252 [00:08<00:02, 32.47it/s] 73%|███████▎  | 185/252 [00:08<00:02, 32.52it/s] 75%|███████▌  | 189/252 [00:08<00:02, 31.47it/s] 77%|███████▋  | 193/252 [00:08<00:01, 30.33it/s] 78%|███████▊  | 197/252 [00:08<00:01, 29.89it/s] 80%|███████▉  | 201/252 [00:08<00:01, 27.16it/s] 81%|████████▏ | 205/252 [00:08<00:01, 28.43it/s] 83%|████████▎ | 208/252 [00:09<00:01, 26.41it/s] 84%|████████▍ | 212/252 [00:09<00:01, 27.97it/s] 86%|████████▌ | 216/252 [00:09<00:01, 29.09it/s] 87%|████████▋ | 220/252 [00:09<00:01, 30.64it/s] 89%|████████▉ | 224/252 [00:09<00:01, 22.67it/s] 90%|█████████ | 228/252 [00:09<00:00, 25.41it/s] 92%|█████████▏| 232/252 [00:09<00:00, 27.35it/s] 94%|█████████▎| 236/252 [00:10<00:00, 28.78it/s] 95%|█████████▌| 240/252 [00:10<00:00, 30.08it/s] 97%|█████████▋| 244/252 [00:10<00:00, 31.25it/s] 98%|█████████▊| 248/252 [00:10<00:00, 32.17it/s]100%|██████████| 252/252 [00:10<00:00, 23.84it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6245402097702026


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.93160054719562
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.16115519090968
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 36.33708380788641


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.45402145385742
=========================          END          =========================
0it [00:00, ?it/s]1it [00:03,  3.12s/it]2it [00:06,  2.99s/it]3it [00:08,  2.90s/it]4it [00:11,  2.93s/it]5it [00:14,  2.86s/it]6it [00:17,  2.96s/it]7it [00:20,  2.86s/it]8it [00:22,  2.72s/it]9it [00:25,  2.66s/it]10it [00:27,  2.43s/it]11it [00:29,  2.44s/it]12it [00:32,  2.46s/it]13it [00:35,  2.72s/it]14it [00:38,  2.84s/it]15it [00:41,  2.95s/it]16it [00:45,  3.11s/it]17it [00:48,  3.08s/it]18it [00:51,  3.10s/it]19it [00:53,  2.89s/it]20it [00:56,  2.84s/it]21it [00:59,  2.87s/it]22it [01:02,  2.80s/it]23it [01:04,  2.79s/it]24it [01:07,  2.75s/it]25it [01:11,  3.01s/it]26it [01:13,  2.82s/it]27it [01:16,  2.84s/it]28it [01:18,  2.69s/it]29it [01:21,  2.75s/it]30it [01:24,  2.85s/it]31it [01:27,  2.86s/it]32it [01:30,  2.96s/it]33it [01:33,  2.92s/it]34it [01:36,  2.94s/it]35it [01:39,  2.89s/it]36it [01:42,  2.87s/it]37it [01:46,  3.20s/it]38it [01:49,  3.11s/it]39it [01:52,  3.07s/it]40it [01:55,  3.12s/it]41it [01:58,  3.03s/it]42it [02:00,  2.86s/it]43it [02:03,  2.95s/it]44it [02:06,  2.76s/it]45it [02:08,  2.71s/it]46it [02:11,  2.69s/it]47it [02:14,  2.86s/it]48it [02:17,  2.77s/it]49it [02:19,  2.79s/it]50it [02:22,  2.72s/it]51it [02:25,  2.83s/it]52it [02:28,  2.85s/it]53it [02:31,  2.76s/it]54it [02:33,  2.76s/it]55it [02:36,  2.66s/it]56it [02:38,  2.52s/it]57it [02:41,  2.60s/it]58it [02:44,  2.66s/it]59it [02:46,  2.74s/it]60it [02:49,  2.68s/it]61it [02:51,  2.60s/it]62it [02:54,  2.58s/it]63it [02:56,  2.49s/it]64it [02:59,  2.46s/it]65it [03:01,  2.40s/it]66it [03:03,  2.46s/it]67it [03:06,  2.45s/it]68it [03:09,  2.57s/it]69it [03:11,  2.60s/it]70it [03:15,  2.75s/it]71it [03:17,  2.76s/it]72it [03:21,  2.93s/it]73it [03:24,  2.93s/it]74it [03:27,  3.14s/it]75it [03:31,  3.26s/it]76it [03:33,  3.02s/it]77it [03:36,  2.97s/it]78it [03:39,  2.91s/it]79it [03:42,  2.89s/it]80it [03:45,  2.99s/it]81it [03:50,  3.73s/it]82it [03:53,  3.36s/it]83it [03:55,  3.15s/it]84it [03:58,  2.91s/it]85it [04:00,  2.80s/it]86it [04:03,  2.77s/it]87it [04:06,  2.96s/it]88it [04:09,  2.86s/it]89it [04:11,  2.67s/it]90it [04:14,  2.77s/it]91it [04:17,  2.71s/it]92it [04:20,  2.77s/it]93it [04:23,  2.90s/it]94it [04:26,  2.99s/it]95it [04:28,  2.77s/it]96it [04:31,  2.59s/it]97it [04:33,  2.50s/it]98it [04:36,  2.60s/it]99it [04:38,  2.50s/it]100it [04:40,  2.37s/it]101it [04:43,  2.55s/it]102it [04:46,  2.54s/it]103it [04:48,  2.49s/it]104it [04:51,  2.70s/it]105it [04:54,  2.77s/it]106it [04:57,  2.76s/it]107it [05:00,  2.76s/it]108it [05:02,  2.76s/it]109it [05:05,  2.77s/it]110it [05:09,  3.12s/it]111it [05:12,  3.16s/it]112it [05:16,  3.19s/it]113it [05:20,  3.48s/it]114it [05:24,  3.84s/it]115it [05:28,  3.72s/it]116it [05:31,  3.52s/it]117it [05:35,  3.57s/it]118it [05:37,  3.32s/it]119it [05:40,  3.14s/it]120it [05:43,  3.10s/it]121it [05:47,  3.27s/it]122it [05:50,  3.26s/it]123it [05:53,  3.25s/it]124it [05:56,  3.16s/it]125it [05:59,  3.08s/it]126it [06:02,  2.94s/it]127it [06:04,  2.72s/it]128it [06:07,  2.84s/it]129it [06:10,  2.85s/it]130it [06:13,  2.86s/it]131it [06:15,  2.78s/it]132it [06:18,  2.70s/it]133it [06:20,  2.53s/it]134it [06:23,  2.59s/it]135it [06:25,  2.47s/it]136it [06:27,  2.38s/it]137it [06:30,  2.43s/it]138it [06:33,  2.56s/it]139it [06:36,  2.73s/it]140it [06:39,  2.88s/it]141it [06:41,  2.78s/it]142it [06:45,  2.91s/it]143it [06:48,  2.93s/it]144it [06:50,  2.72s/it]145it [06:52,  2.59s/it]146it [06:54,  2.44s/it]147it [06:56,  2.37s/it]148it [06:59,  2.55s/it]149it [07:02,  2.53s/it]150it [07:06,  3.09s/it]151it [07:09,  3.04s/it]152it [07:12,  3.09s/it]153it [07:16,  3.21s/it]154it [07:19,  3.21s/it]155it [07:22,  3.18s/it]156it [07:27,  3.57s/it]157it [07:31,  3.68s/it]158it [07:34,  3.45s/it]159it [07:36,  3.25s/it]160it [07:39,  3.16s/it]161it [07:42,  2.89s/it]162it [07:45,  2.96s/it]163it [07:47,  2.89s/it]164it [07:50,  2.93s/it]165it [07:53,  2.79s/it]166it [07:56,  3.02s/it]167it [08:00,  3.30s/it]168it [08:04,  3.27s/it]169it [08:07,  3.36s/it]170it [08:10,  3.29s/it]171it [08:14,  3.39s/it]172it [08:17,  3.33s/it]173it [08:20,  3.24s/it]174it [08:23,  3.22s/it]175it [08:26,  3.17s/it]176it [08:30,  3.21s/it]177it [08:33,  3.13s/it]178it [08:36,  3.15s/it]179it [08:39,  3.19s/it]180it [08:42,  3.19s/it]181it [08:46,  3.29s/it]182it [08:49,  3.15s/it]183it [08:52,  3.26s/it]184it [08:55,  3.18s/it]185it [08:58,  3.17s/it]186it [09:01,  3.07s/it]187it [09:04,  3.05s/it]188it [09:07,  3.06s/it]189it [09:10,  3.00s/it]190it [09:13,  2.99s/it]191it [09:16,  3.00s/it]192it [09:19,  2.96s/it]193it [09:22,  3.02s/it]194it [09:25,  3.00s/it]195it [09:28,  2.98s/it]196it [09:31,  2.98s/it]197it [09:34,  3.06s/it]198it [09:37,  3.07s/it]199it [09:40,  3.07s/it]200it [09:43,  2.93s/it]201it [09:47,  3.41s/it]202it [09:52,  3.69s/it]203it [09:56,  3.87s/it]203it [09:56,  2.94s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:04<17:01,  4.07s/it]  1%|          | 3/252 [00:04<04:33,  1.10s/it]  2%|▏         | 5/252 [00:04<02:20,  1.76it/s]  3%|▎         | 7/252 [00:04<01:26,  2.82it/s]  4%|▎         | 9/252 [00:04<00:59,  4.10it/s]  4%|▍         | 11/252 [00:04<00:43,  5.51it/s]  5%|▌         | 13/252 [00:04<00:34,  6.95it/s]  6%|▌         | 15/252 [00:04<00:27,  8.62it/s]  7%|▋         | 17/252 [00:05<00:23, 10.09it/s]  8%|▊         | 19/252 [00:05<00:21, 10.61it/s]  8%|▊         | 21/252 [00:05<00:19, 11.66it/s]  9%|▉         | 23/252 [00:05<00:18, 12.40it/s] 10%|▉         | 25/252 [00:05<00:16, 13.37it/s] 11%|█         | 27/252 [00:05<00:16, 13.49it/s] 12%|█▏        | 29/252 [00:05<00:16, 13.45it/s] 12%|█▏        | 31/252 [00:06<00:16, 13.29it/s] 13%|█▎        | 33/252 [00:06<00:15, 13.84it/s] 14%|█▍        | 35/252 [00:06<00:14, 14.98it/s] 15%|█▍        | 37/252 [00:06<00:14, 14.64it/s] 15%|█▌        | 39/252 [00:06<00:14, 14.86it/s] 16%|█▋        | 41/252 [00:06<00:13, 15.77it/s] 17%|█▋        | 43/252 [00:06<00:13, 15.36it/s] 18%|█▊        | 45/252 [00:06<00:14, 14.73it/s] 19%|█▊        | 47/252 [00:07<00:13, 15.32it/s] 19%|█▉        | 49/252 [00:07<00:13, 15.02it/s] 20%|██        | 51/252 [00:07<00:13, 14.93it/s] 21%|██        | 53/252 [00:07<00:13, 15.21it/s] 22%|██▏       | 55/252 [00:07<00:12, 15.27it/s] 23%|██▎       | 57/252 [00:07<00:12, 15.30it/s] 23%|██▎       | 59/252 [00:07<00:12, 15.80it/s] 24%|██▍       | 61/252 [00:08<00:12, 15.04it/s] 25%|██▌       | 63/252 [00:08<00:12, 15.21it/s] 26%|██▌       | 65/252 [00:08<00:12, 15.58it/s] 27%|██▋       | 67/252 [00:08<00:12, 15.40it/s] 27%|██▋       | 69/252 [00:08<00:12, 15.20it/s] 28%|██▊       | 71/252 [00:08<00:11, 15.29it/s] 29%|██▉       | 73/252 [00:08<00:11, 15.15it/s] 30%|██▉       | 75/252 [00:08<00:11, 15.23it/s] 31%|███       | 77/252 [00:09<00:11, 15.89it/s] 31%|███▏      | 79/252 [00:09<00:11, 15.40it/s] 33%|███▎      | 82/252 [00:09<00:09, 17.58it/s] 34%|███▍      | 86/252 [00:09<00:07, 21.78it/s] 36%|███▌      | 90/252 [00:09<00:06, 25.05it/s] 37%|███▋      | 94/252 [00:09<00:05, 26.46it/s] 39%|███▉      | 98/252 [00:09<00:05, 28.33it/s] 40%|████      | 102/252 [00:09<00:04, 30.29it/s] 42%|████▏     | 106/252 [00:10<00:04, 30.21it/s] 44%|████▎     | 110/252 [00:10<00:04, 31.60it/s] 45%|████▌     | 114/252 [00:10<00:04, 32.58it/s] 47%|████▋     | 118/252 [00:10<00:04, 33.14it/s] 48%|████▊     | 122/252 [00:10<00:03, 33.32it/s] 50%|█████     | 126/252 [00:10<00:03, 33.19it/s] 52%|█████▏    | 130/252 [00:10<00:03, 33.58it/s] 53%|█████▎    | 134/252 [00:10<00:03, 32.99it/s] 55%|█████▍    | 138/252 [00:11<00:03, 32.73it/s] 56%|█████▋    | 142/252 [00:11<00:03, 32.30it/s] 58%|█████▊    | 146/252 [00:11<00:03, 31.12it/s] 60%|█████▉    | 150/252 [00:11<00:03, 32.03it/s] 61%|██████    | 154/252 [00:11<00:03, 32.27it/s] 63%|██████▎   | 158/252 [00:11<00:02, 32.71it/s] 64%|██████▍   | 162/252 [00:11<00:02, 32.48it/s] 66%|██████▌   | 166/252 [00:11<00:02, 32.44it/s] 67%|██████▋   | 170/252 [00:12<00:02, 32.41it/s] 69%|██████▉   | 174/252 [00:12<00:02, 31.41it/s] 71%|███████   | 178/252 [00:12<00:02, 31.92it/s] 72%|███████▏  | 182/252 [00:12<00:02, 32.62it/s] 74%|███████▍  | 186/252 [00:12<00:01, 33.27it/s] 75%|███████▌  | 190/252 [00:12<00:01, 33.35it/s] 77%|███████▋  | 194/252 [00:12<00:01, 33.49it/s] 79%|███████▊  | 198/252 [00:13<00:03, 17.16it/s] 80%|████████  | 202/252 [00:13<00:02, 20.09it/s] 82%|████████▏ | 206/252 [00:13<00:02, 22.72it/s] 83%|████████▎ | 210/252 [00:13<00:01, 24.82it/s] 85%|████████▍ | 214/252 [00:13<00:01, 24.89it/s] 86%|████████▌ | 217/252 [00:13<00:01, 25.74it/s] 87%|████████▋ | 220/252 [00:14<00:01, 24.79it/s] 88%|████████▊ | 223/252 [00:14<00:01, 25.44it/s] 90%|████████▉ | 226/252 [00:14<00:00, 26.21it/s] 91%|█████████▏| 230/252 [00:14<00:00, 28.27it/s] 93%|█████████▎| 234/252 [00:14<00:00, 29.88it/s] 94%|█████████▍| 238/252 [00:14<00:00, 30.96it/s] 96%|█████████▌| 242/252 [00:14<00:00, 32.21it/s] 98%|█████████▊| 246/252 [00:14<00:00, 32.43it/s] 99%|█████████▉| 250/252 [00:14<00:00, 33.30it/s]100%|██████████| 252/252 [00:15<00:00, 16.58it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<12:33,  3.00s/it]  1%|          | 3/252 [00:03<03:23,  1.22it/s]  2%|▏         | 5/252 [00:03<01:50,  2.24it/s]  3%|▎         | 7/252 [00:03<01:09,  3.52it/s]  4%|▎         | 9/252 [00:03<00:48,  5.06it/s]  4%|▍         | 11/252 [00:03<00:36,  6.65it/s]  5%|▌         | 13/252 [00:03<00:28,  8.26it/s]  6%|▋         | 16/252 [00:03<00:21, 10.95it/s]  7%|▋         | 18/252 [00:04<00:19, 12.02it/s]  8%|▊         | 20/252 [00:04<00:17, 12.95it/s]  9%|▊         | 22/252 [00:04<00:16, 14.19it/s] 10%|▉         | 24/252 [00:04<00:15, 14.61it/s] 10%|█         | 26/252 [00:04<00:15, 14.76it/s] 11%|█         | 28/252 [00:04<00:14, 15.30it/s] 12%|█▏        | 30/252 [00:04<00:13, 16.17it/s] 13%|█▎        | 32/252 [00:04<00:13, 16.20it/s] 13%|█▎        | 34/252 [00:05<00:13, 16.04it/s] 15%|█▍        | 37/252 [00:05<00:12, 17.37it/s] 15%|█▌        | 39/252 [00:05<00:12, 16.88it/s] 16%|█▋        | 41/252 [00:05<00:12, 16.71it/s] 17%|█▋        | 43/252 [00:05<00:11, 17.44it/s] 18%|█▊        | 46/252 [00:05<00:11, 18.01it/s] 19%|█▉        | 48/252 [00:05<00:11, 17.23it/s] 20%|█▉        | 50/252 [00:05<00:12, 16.77it/s] 21%|██        | 52/252 [00:06<00:11, 17.49it/s] 21%|██▏       | 54/252 [00:06<00:11, 17.12it/s] 22%|██▏       | 56/252 [00:06<00:11, 16.68it/s] 23%|██▎       | 59/252 [00:06<00:11, 17.53it/s] 24%|██▍       | 61/252 [00:06<00:11, 16.99it/s] 25%|██▌       | 63/252 [00:06<00:11, 16.69it/s] 26%|██▌       | 66/252 [00:06<00:09, 18.74it/s] 27%|██▋       | 68/252 [00:06<00:10, 17.78it/s] 28%|██▊       | 70/252 [00:07<00:10, 17.18it/s] 29%|██▊       | 72/252 [00:07<00:10, 16.83it/s] 29%|██▉       | 74/252 [00:07<00:10, 17.16it/s] 30%|███       | 76/252 [00:07<00:10, 16.70it/s] 31%|███       | 78/252 [00:07<00:10, 16.29it/s] 32%|███▏      | 80/252 [00:07<00:10, 16.83it/s] 33%|███▎      | 82/252 [00:07<00:10, 16.47it/s] 33%|███▎      | 84/252 [00:07<00:10, 16.31it/s] 34%|███▍      | 86/252 [00:08<00:09, 16.94it/s] 35%|███▍      | 88/252 [00:08<00:09, 16.42it/s] 36%|███▌      | 90/252 [00:08<00:10, 16.10it/s] 37%|███▋      | 92/252 [00:08<00:10, 15.98it/s] 37%|███▋      | 94/252 [00:08<00:09, 16.39it/s] 38%|███▊      | 96/252 [00:08<00:09, 16.15it/s] 39%|███▉      | 98/252 [00:08<00:09, 15.89it/s] 40%|████      | 101/252 [00:08<00:08, 17.22it/s] 41%|████      | 103/252 [00:09<00:08, 16.58it/s] 42%|████▏     | 105/252 [00:09<00:08, 16.39it/s] 43%|████▎     | 108/252 [00:09<00:08, 17.44it/s] 44%|████▎     | 110/252 [00:09<00:08, 16.94it/s] 44%|████▍     | 112/252 [00:09<00:09, 15.53it/s] 46%|████▌     | 115/252 [00:09<00:08, 16.46it/s] 46%|████▋     | 117/252 [00:09<00:08, 16.24it/s] 47%|████▋     | 119/252 [00:10<00:08, 16.07it/s] 48%|████▊     | 122/252 [00:10<00:07, 16.97it/s] 49%|████▉     | 124/252 [00:10<00:07, 16.51it/s] 50%|█████     | 126/252 [00:10<00:07, 16.12it/s] 51%|█████     | 129/252 [00:10<00:07, 17.04it/s] 52%|█████▏    | 131/252 [00:10<00:07, 16.57it/s] 53%|█████▎    | 133/252 [00:10<00:07, 16.24it/s] 54%|█████▎    | 135/252 [00:11<00:07, 16.68it/s] 54%|█████▍    | 137/252 [00:11<00:06, 16.78it/s] 56%|█████▌    | 141/252 [00:11<00:05, 21.57it/s] 58%|█████▊    | 145/252 [00:11<00:04, 25.30it/s] 59%|█████▉    | 149/252 [00:11<00:03, 27.86it/s] 61%|██████    | 153/252 [00:11<00:03, 29.36it/s] 62%|██████▏   | 157/252 [00:11<00:03, 30.81it/s] 64%|██████▍   | 161/252 [00:11<00:02, 31.94it/s] 65%|██████▌   | 165/252 [00:11<00:02, 32.65it/s] 67%|██████▋   | 169/252 [00:12<00:02, 32.94it/s] 69%|██████▊   | 173/252 [00:12<00:02, 33.48it/s] 70%|███████   | 177/252 [00:12<00:02, 33.32it/s] 72%|███████▏  | 181/252 [00:12<00:02, 33.18it/s] 73%|███████▎  | 185/252 [00:12<00:02, 33.24it/s] 75%|███████▌  | 189/252 [00:12<00:01, 32.64it/s] 77%|███████▋  | 193/252 [00:12<00:01, 32.81it/s] 78%|███████▊  | 197/252 [00:12<00:01, 32.64it/s] 80%|███████▉  | 201/252 [00:13<00:01, 33.13it/s] 81%|████████▏ | 205/252 [00:13<00:01, 32.96it/s] 83%|████████▎ | 209/252 [00:13<00:01, 32.61it/s] 85%|████████▍ | 213/252 [00:13<00:01, 32.00it/s] 86%|████████▌ | 217/252 [00:13<00:01, 32.74it/s] 88%|████████▊ | 221/252 [00:13<00:00, 31.88it/s] 89%|████████▉ | 225/252 [00:13<00:00, 32.60it/s] 91%|█████████ | 229/252 [00:13<00:00, 33.14it/s] 92%|█████████▏| 233/252 [00:14<00:00, 33.20it/s] 94%|█████████▍| 237/252 [00:14<00:00, 33.28it/s] 96%|█████████▌| 241/252 [00:14<00:00, 33.80it/s] 97%|█████████▋| 245/252 [00:14<00:00, 34.19it/s] 99%|█████████▉| 249/252 [00:14<00:00, 34.48it/s]100%|██████████| 252/252 [00:14<00:00, 17.10it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6255826354026794


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.29735107573685
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.07729593876006
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 35.67028257023489


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.55826187133789
=========================          END          =========================
0it [00:00, ?it/s]1it [00:04,  4.01s/it]2it [00:07,  3.72s/it]3it [00:11,  4.00s/it]4it [00:15,  3.85s/it]5it [00:19,  3.82s/it]6it [00:22,  3.74s/it]7it [00:27,  3.90s/it]8it [00:31,  4.04s/it]9it [00:35,  4.02s/it]10it [00:39,  4.08s/it]11it [00:43,  4.06s/it]12it [00:47,  4.16s/it]13it [00:52,  4.37s/it]14it [00:56,  4.14s/it]15it [01:00,  4.00s/it]16it [01:04,  4.03s/it]17it [01:08,  4.02s/it]18it [01:11,  3.80s/it]19it [01:15,  3.87s/it]20it [01:18,  3.75s/it]21it [01:23,  3.87s/it]22it [01:26,  3.75s/it]23it [01:29,  3.58s/it]24it [01:34,  3.78s/it]25it [01:37,  3.58s/it]26it [01:39,  3.31s/it]27it [01:43,  3.55s/it]28it [01:47,  3.49s/it]29it [01:50,  3.27s/it]30it [01:52,  3.07s/it]31it [01:56,  3.25s/it]32it [01:59,  3.11s/it]33it [02:02,  3.12s/it]34it [02:07,  3.61s/it]35it [02:10,  3.43s/it]36it [02:12,  3.19s/it]37it [02:16,  3.29s/it]38it [02:19,  3.28s/it]39it [02:22,  3.15s/it]40it [02:24,  2.98s/it]41it [02:27,  2.94s/it]42it [02:30,  2.93s/it]43it [02:33,  2.92s/it]44it [02:36,  3.00s/it]45it [02:39,  2.85s/it]46it [02:43,  3.16s/it]47it [02:46,  3.14s/it]48it [02:48,  2.91s/it]49it [02:51,  2.94s/it]50it [02:54,  3.00s/it]51it [02:57,  3.07s/it]52it [03:00,  2.97s/it]53it [03:03,  2.99s/it]54it [03:07,  3.14s/it]55it [03:09,  3.02s/it]56it [03:13,  3.33s/it]57it [03:17,  3.31s/it]58it [03:20,  3.41s/it]59it [03:23,  3.26s/it]60it [03:27,  3.32s/it]61it [03:30,  3.33s/it]62it [03:33,  3.27s/it]63it [03:35,  2.95s/it]64it [03:38,  2.93s/it]65it [03:41,  2.96s/it]66it [03:45,  3.12s/it]67it [03:48,  3.14s/it]68it [03:53,  3.56s/it]69it [03:57,  3.75s/it]70it [04:00,  3.62s/it]71it [04:04,  3.60s/it]72it [04:07,  3.39s/it]73it [04:10,  3.29s/it]74it [04:14,  3.52s/it]75it [04:17,  3.34s/it]76it [04:19,  3.20s/it]77it [04:22,  3.15s/it]78it [04:25,  3.01s/it]79it [04:28,  3.10s/it]80it [04:32,  3.25s/it]81it [04:37,  3.61s/it]82it [04:40,  3.48s/it]83it [04:42,  3.19s/it]84it [04:46,  3.39s/it]85it [04:48,  3.05s/it]86it [04:51,  2.86s/it]87it [04:54,  3.05s/it]88it [04:58,  3.11s/it]89it [05:00,  2.82s/it]90it [05:03,  3.06s/it]91it [05:08,  3.47s/it]92it [05:11,  3.35s/it]93it [05:14,  3.34s/it]94it [05:17,  3.21s/it]95it [05:19,  2.98s/it]96it [05:23,  3.18s/it]97it [05:26,  3.13s/it]98it [05:29,  3.02s/it]99it [05:32,  2.96s/it]100it [05:35,  3.16s/it]101it [05:40,  3.48s/it]102it [05:43,  3.60s/it]103it [05:47,  3.46s/it]104it [05:50,  3.50s/it]105it [05:53,  3.26s/it]106it [05:56,  3.24s/it]107it [05:58,  2.96s/it]108it [06:01,  2.90s/it]109it [06:04,  2.96s/it]110it [06:08,  3.15s/it]111it [06:11,  3.08s/it]112it [06:14,  3.07s/it]113it [06:17,  3.07s/it]114it [06:22,  3.58s/it]115it [06:25,  3.61s/it]116it [06:29,  3.55s/it]117it [06:31,  3.33s/it]118it [06:34,  3.22s/it]119it [06:37,  3.02s/it]120it [06:39,  2.85s/it]121it [06:42,  2.64s/it]122it [06:44,  2.56s/it]123it [06:47,  2.69s/it]124it [06:50,  2.88s/it]125it [06:53,  2.87s/it]126it [06:57,  3.11s/it]127it [07:01,  3.42s/it]128it [07:04,  3.16s/it]129it [07:06,  2.93s/it]130it [07:08,  2.67s/it]131it [07:10,  2.60s/it]132it [07:14,  2.78s/it]133it [07:16,  2.76s/it]134it [07:20,  3.10s/it]135it [07:23,  2.92s/it]136it [07:25,  2.85s/it]137it [07:28,  2.86s/it]138it [07:32,  3.15s/it]139it [07:36,  3.37s/it]140it [07:39,  3.31s/it]141it [07:42,  3.31s/it]142it [07:46,  3.23s/it]143it [07:49,  3.30s/it]144it [07:52,  3.18s/it]145it [07:56,  3.47s/it]146it [07:59,  3.30s/it]147it [08:01,  3.07s/it]148it [08:05,  3.15s/it]149it [08:08,  3.10s/it]150it [08:12,  3.40s/it]151it [08:15,  3.30s/it]152it [08:18,  3.31s/it]153it [08:21,  3.02s/it]154it [08:24,  3.07s/it]155it [08:27,  2.98s/it]156it [08:30,  3.13s/it]157it [08:33,  3.02s/it]158it [08:35,  2.89s/it]159it [08:40,  3.36s/it]160it [08:44,  3.65s/it]161it [08:47,  3.45s/it]162it [08:51,  3.53s/it]163it [08:54,  3.51s/it]164it [08:57,  3.37s/it]165it [09:01,  3.30s/it]166it [09:03,  3.06s/it]167it [09:07,  3.35s/it]168it [09:10,  3.23s/it]169it [09:12,  2.97s/it]170it [09:15,  2.99s/it]171it [09:19,  3.06s/it]172it [09:23,  3.30s/it]173it [09:25,  3.05s/it]174it [09:29,  3.19s/it]175it [09:32,  3.22s/it]176it [09:35,  3.31s/it]177it [09:38,  3.03s/it]178it [09:41,  3.19s/it]179it [09:45,  3.21s/it]180it [09:47,  2.94s/it]181it [09:50,  2.91s/it]182it [09:52,  2.88s/it]183it [09:55,  2.72s/it]184it [09:58,  2.70s/it]185it [10:00,  2.56s/it]186it [10:03,  2.80s/it]187it [10:06,  2.74s/it]188it [10:09,  2.80s/it]189it [10:11,  2.77s/it]190it [10:14,  2.65s/it]191it [10:17,  2.93s/it]192it [10:20,  2.83s/it]193it [10:23,  2.83s/it]194it [10:26,  3.00s/it]195it [10:29,  3.00s/it]196it [10:32,  2.91s/it]197it [10:35,  2.93s/it]198it [10:37,  2.79s/it]199it [10:41,  3.02s/it]200it [10:44,  2.98s/it]201it [10:47,  3.02s/it]202it [10:50,  3.05s/it]203it [10:53,  2.92s/it]203it [10:53,  3.22s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<15:37,  3.73s/it]  1%|          | 2/252 [00:04<07:20,  1.76s/it]  1%|          | 3/252 [00:04<04:53,  1.18s/it]  3%|▎         | 7/252 [00:04<01:25,  2.88it/s]  4%|▍         | 11/252 [00:04<00:44,  5.39it/s]  6%|▌         | 15/252 [00:04<00:28,  8.33it/s]  7%|▋         | 18/252 [00:05<00:22, 10.31it/s]  9%|▊         | 22/252 [00:05<00:16, 13.96it/s] 10%|█         | 26/252 [00:05<00:12, 17.62it/s] 12%|█▏        | 30/252 [00:05<00:10, 21.07it/s] 13%|█▎        | 34/252 [00:05<00:09, 23.35it/s] 15%|█▌        | 38/252 [00:05<00:08, 24.17it/s] 17%|█▋        | 42/252 [00:05<00:07, 26.45it/s] 18%|█▊        | 46/252 [00:06<00:16, 12.86it/s] 20%|█▉        | 50/252 [00:06<00:12, 15.93it/s] 21%|██▏       | 54/252 [00:06<00:10, 18.94it/s] 23%|██▎       | 58/252 [00:06<00:08, 21.75it/s] 25%|██▍       | 62/252 [00:07<00:08, 23.04it/s] 26%|██▌       | 66/252 [00:07<00:07, 25.26it/s] 28%|██▊       | 70/252 [00:07<00:06, 27.42it/s] 29%|██▉       | 74/252 [00:07<00:06, 28.92it/s] 31%|███       | 78/252 [00:07<00:11, 15.76it/s] 33%|███▎      | 82/252 [00:08<00:09, 18.78it/s] 34%|███▍      | 86/252 [00:08<00:07, 21.80it/s] 36%|███▌      | 90/252 [00:08<00:06, 24.47it/s] 37%|███▋      | 94/252 [00:08<00:08, 18.06it/s] 39%|███▉      | 98/252 [00:08<00:07, 21.15it/s] 40%|████      | 102/252 [00:08<00:06, 23.80it/s] 42%|████▏     | 105/252 [00:08<00:05, 24.94it/s] 43%|████▎     | 109/252 [00:09<00:05, 27.49it/s] 45%|████▍     | 113/252 [00:09<00:04, 29.47it/s] 46%|████▋     | 117/252 [00:09<00:04, 28.10it/s] 48%|████▊     | 121/252 [00:09<00:04, 29.94it/s] 50%|████▉     | 125/252 [00:10<00:12, 10.41it/s] 51%|█████     | 129/252 [00:10<00:09, 13.23it/s] 53%|█████▎    | 133/252 [00:10<00:07, 16.29it/s] 54%|█████▍    | 137/252 [00:10<00:05, 19.38it/s] 56%|█████▌    | 141/252 [00:10<00:04, 22.35it/s] 58%|█████▊    | 145/252 [00:10<00:04, 25.08it/s] 59%|█████▉    | 149/252 [00:11<00:03, 27.33it/s] 61%|██████    | 153/252 [00:11<00:03, 29.27it/s] 62%|██████▏   | 157/252 [00:11<00:04, 20.10it/s] 64%|██████▍   | 161/252 [00:11<00:03, 23.10it/s] 65%|██████▌   | 164/252 [00:11<00:05, 17.16it/s] 67%|██████▋   | 168/252 [00:12<00:04, 20.50it/s] 68%|██████▊   | 171/252 [00:12<00:04, 17.21it/s] 69%|██████▉   | 175/252 [00:12<00:03, 20.62it/s] 71%|███████   | 178/252 [00:12<00:04, 16.58it/s] 72%|███████▏  | 182/252 [00:12<00:03, 20.03it/s] 74%|███████▍  | 186/252 [00:12<00:02, 23.23it/s] 75%|███████▌  | 189/252 [00:13<00:03, 20.18it/s] 77%|███████▋  | 193/252 [00:13<00:02, 23.46it/s] 78%|███████▊  | 196/252 [00:13<00:02, 23.41it/s] 79%|███████▉  | 200/252 [00:13<00:02, 25.91it/s] 81%|████████  | 204/252 [00:13<00:01, 27.73it/s] 83%|████████▎ | 208/252 [00:13<00:01, 29.39it/s] 84%|████████▍ | 212/252 [00:14<00:03, 10.96it/s] 85%|████████▌ | 215/252 [00:14<00:02, 13.00it/s] 87%|████████▋ | 218/252 [00:14<00:02, 14.90it/s] 88%|████████▊ | 222/252 [00:14<00:01, 18.28it/s] 90%|████████▉ | 226/252 [00:15<00:01, 15.34it/s] 91%|█████████▏| 230/252 [00:15<00:01, 18.71it/s] 93%|█████████▎| 234/252 [00:15<00:00, 21.92it/s] 94%|█████████▍| 238/252 [00:15<00:00, 24.81it/s] 96%|█████████▌| 242/252 [00:15<00:00, 24.99it/s] 98%|█████████▊| 246/252 [00:15<00:00, 27.25it/s] 99%|█████████▉| 250/252 [00:16<00:00, 20.86it/s]100%|██████████| 252/252 [00:16<00:00, 14.91it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430387496948
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<12:38,  3.02s/it]  1%|          | 2/252 [00:03<05:47,  1.39s/it]  2%|▏         | 6/252 [00:03<01:24,  2.91it/s]  4%|▍         | 10/252 [00:03<00:43,  5.57it/s]  6%|▌         | 14/252 [00:03<00:27,  8.69it/s]  7%|▋         | 18/252 [00:03<00:19, 12.15it/s]  9%|▊         | 22/252 [00:03<00:14, 15.64it/s] 10%|█         | 26/252 [00:03<00:11, 19.08it/s] 12%|█▏        | 30/252 [00:04<00:09, 22.36it/s] 13%|█▎        | 34/252 [00:04<00:08, 25.13it/s] 15%|█▌        | 38/252 [00:04<00:07, 27.37it/s] 17%|█▋        | 42/252 [00:04<00:07, 29.20it/s] 18%|█▊        | 46/252 [00:04<00:06, 30.42it/s] 20%|█▉        | 50/252 [00:04<00:06, 29.39it/s] 21%|██▏       | 54/252 [00:04<00:06, 30.81it/s] 23%|██▎       | 58/252 [00:04<00:06, 31.73it/s] 25%|██▍       | 62/252 [00:05<00:05, 32.31it/s] 26%|██▌       | 66/252 [00:05<00:05, 32.85it/s] 28%|██▊       | 70/252 [00:05<00:07, 25.73it/s] 29%|██▉       | 74/252 [00:05<00:06, 27.59it/s] 31%|███       | 78/252 [00:05<00:05, 29.45it/s] 33%|███▎      | 82/252 [00:05<00:05, 30.57it/s] 34%|███▍      | 86/252 [00:05<00:05, 31.30it/s] 36%|███▌      | 90/252 [00:06<00:05, 31.99it/s] 37%|███▋      | 94/252 [00:06<00:04, 32.04it/s] 39%|███▉      | 98/252 [00:06<00:04, 32.45it/s] 40%|████      | 102/252 [00:06<00:04, 32.80it/s] 42%|████▏     | 106/252 [00:06<00:04, 32.71it/s] 44%|████▎     | 110/252 [00:06<00:04, 32.67it/s] 45%|████▌     | 114/252 [00:06<00:04, 33.01it/s] 47%|████▋     | 118/252 [00:06<00:04, 33.50it/s] 48%|████▊     | 122/252 [00:06<00:03, 33.70it/s] 50%|█████     | 126/252 [00:07<00:03, 33.98it/s] 52%|█████▏    | 130/252 [00:07<00:03, 34.09it/s] 53%|█████▎    | 134/252 [00:07<00:03, 33.22it/s] 55%|█████▍    | 138/252 [00:07<00:03, 32.99it/s] 56%|█████▋    | 142/252 [00:07<00:03, 33.05it/s] 58%|█████▊    | 146/252 [00:07<00:03, 33.28it/s] 60%|█████▉    | 150/252 [00:07<00:03, 33.32it/s] 61%|██████    | 154/252 [00:07<00:02, 33.41it/s] 63%|██████▎   | 158/252 [00:08<00:02, 33.58it/s] 64%|██████▍   | 162/252 [00:08<00:02, 33.84it/s] 66%|██████▌   | 166/252 [00:08<00:02, 33.55it/s] 67%|██████▋   | 170/252 [00:08<00:02, 32.80it/s] 69%|██████▉   | 174/252 [00:08<00:02, 32.97it/s] 71%|███████   | 178/252 [00:08<00:02, 33.19it/s] 72%|███████▏  | 182/252 [00:08<00:02, 32.27it/s] 74%|███████▍  | 186/252 [00:08<00:02, 32.13it/s] 75%|███████▌  | 190/252 [00:09<00:01, 32.74it/s] 77%|███████▋  | 194/252 [00:09<00:01, 33.00it/s] 79%|███████▊  | 198/252 [00:09<00:01, 33.51it/s] 80%|████████  | 202/252 [00:09<00:01, 33.92it/s] 82%|████████▏ | 206/252 [00:09<00:01, 33.90it/s] 83%|████████▎ | 210/252 [00:09<00:01, 33.96it/s] 85%|████████▍ | 214/252 [00:09<00:01, 34.05it/s] 87%|████████▋ | 218/252 [00:09<00:00, 34.28it/s] 88%|████████▊ | 222/252 [00:09<00:00, 34.51it/s] 90%|████████▉ | 226/252 [00:10<00:00, 34.34it/s] 91%|█████████▏| 230/252 [00:10<00:00, 34.55it/s] 93%|█████████▎| 234/252 [00:10<00:00, 34.76it/s] 94%|█████████▍| 238/252 [00:10<00:00, 34.73it/s] 96%|█████████▌| 242/252 [00:10<00:00, 34.92it/s] 98%|█████████▊| 246/252 [00:10<00:00, 35.06it/s] 99%|█████████▉| 250/252 [00:10<00:00, 35.22it/s]100%|██████████| 252/252 [00:10<00:00, 22.97it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6250102519989014


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.31430053710938
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.99378186792687
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 75.9712426636469
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 36.11253476265428


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.50102615356445
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.99s/it]2it [00:06,  3.03s/it]3it [00:08,  2.96s/it]4it [00:11,  2.80s/it]5it [00:14,  2.95s/it]6it [00:17,  3.04s/it]7it [00:21,  3.21s/it]8it [00:25,  3.34s/it]9it [00:28,  3.36s/it]10it [00:32,  3.43s/it]11it [00:35,  3.36s/it]12it [00:38,  3.30s/it]13it [00:42,  3.51s/it]14it [00:45,  3.43s/it]15it [00:48,  3.38s/it]16it [00:52,  3.41s/it]17it [00:55,  3.42s/it]18it [00:58,  3.32s/it]19it [01:02,  3.37s/it]20it [01:05,  3.42s/it]21it [01:09,  3.43s/it]22it [01:12,  3.43s/it]23it [01:16,  3.47s/it]24it [01:20,  3.60s/it]25it [01:23,  3.59s/it]26it [01:27,  3.50s/it]27it [01:30,  3.42s/it]28it [01:33,  3.23s/it]29it [01:35,  3.05s/it]30it [01:38,  3.04s/it]31it [01:41,  3.02s/it]32it [01:44,  2.90s/it]33it [01:47,  2.81s/it]34it [01:49,  2.65s/it]35it [01:51,  2.59s/it]36it [01:54,  2.52s/it]37it [01:56,  2.51s/it]38it [01:59,  2.64s/it]39it [02:02,  2.68s/it]40it [02:04,  2.62s/it]41it [02:07,  2.74s/it]42it [02:10,  2.78s/it]43it [02:12,  2.63s/it]44it [02:15,  2.70s/it]45it [02:18,  2.70s/it]46it [02:20,  2.58s/it]47it [02:23,  2.52s/it]48it [02:26,  2.61s/it]49it [02:28,  2.52s/it]50it [02:30,  2.42s/it]51it [02:33,  2.67s/it]52it [02:36,  2.75s/it]53it [02:39,  2.64s/it]54it [02:41,  2.62s/it]55it [02:44,  2.68s/it]56it [02:47,  2.74s/it]57it [02:50,  2.80s/it]58it [02:53,  2.91s/it]59it [02:56,  2.94s/it]60it [02:59,  2.93s/it]61it [03:02,  2.95s/it]62it [03:05,  2.88s/it]63it [03:08,  2.90s/it]64it [03:10,  2.89s/it]65it [03:13,  2.91s/it]66it [03:16,  2.87s/it]67it [03:19,  2.84s/it]68it [03:21,  2.74s/it]69it [03:24,  2.71s/it]70it [03:27,  2.63s/it]71it [03:29,  2.63s/it]72it [03:32,  2.61s/it]73it [03:34,  2.54s/it]74it [03:37,  2.56s/it]75it [03:39,  2.60s/it]76it [03:42,  2.71s/it]77it [03:45,  2.78s/it]78it [03:48,  2.83s/it]79it [03:51,  2.70s/it]80it [03:53,  2.62s/it]81it [03:57,  3.12s/it]82it [04:00,  3.01s/it]83it [04:03,  2.97s/it]84it [04:06,  2.90s/it]85it [04:09,  2.90s/it]86it [04:12,  2.94s/it]87it [04:15,  3.04s/it]88it [04:18,  3.04s/it]89it [04:21,  2.93s/it]90it [04:24,  3.00s/it]91it [04:27,  3.08s/it]92it [04:30,  3.12s/it]93it [04:33,  3.03s/it]94it [04:36,  2.84s/it]95it [04:38,  2.65s/it]96it [04:40,  2.58s/it]97it [04:42,  2.49s/it]98it [04:45,  2.40s/it]99it [04:47,  2.44s/it]100it [04:50,  2.49s/it]101it [04:53,  2.58s/it]102it [04:55,  2.45s/it]103it [04:57,  2.37s/it]104it [04:59,  2.29s/it]105it [05:01,  2.33s/it]106it [05:04,  2.38s/it]107it [05:06,  2.21s/it]108it [05:08,  2.21s/it]109it [05:11,  2.36s/it]110it [05:13,  2.44s/it]111it [05:16,  2.52s/it]112it [05:18,  2.40s/it]113it [05:20,  2.35s/it]114it [05:23,  2.55s/it]115it [05:25,  2.36s/it]116it [05:27,  2.31s/it]117it [05:31,  2.55s/it]118it [05:33,  2.54s/it]119it [05:35,  2.46s/it]120it [05:38,  2.48s/it]121it [05:40,  2.43s/it]122it [05:43,  2.47s/it]123it [05:45,  2.51s/it]124it [05:48,  2.53s/it]125it [05:51,  2.60s/it]126it [05:54,  2.93s/it]127it [05:57,  2.97s/it]128it [06:00,  2.87s/it]129it [06:03,  2.85s/it]130it [06:05,  2.70s/it]131it [06:08,  2.69s/it]132it [06:11,  2.73s/it]133it [06:14,  2.77s/it]134it [06:17,  2.98s/it]135it [06:20,  2.83s/it]136it [06:22,  2.70s/it]137it [06:25,  2.77s/it]138it [06:28,  2.75s/it]139it [06:30,  2.66s/it]140it [06:33,  2.72s/it]141it [06:35,  2.59s/it]142it [06:38,  2.61s/it]143it [06:41,  2.89s/it]144it [06:44,  2.91s/it]145it [06:47,  2.88s/it]146it [06:50,  2.96s/it]147it [06:53,  2.91s/it]148it [06:56,  2.80s/it]149it [06:58,  2.74s/it]150it [07:01,  2.74s/it]151it [07:03,  2.63s/it]152it [07:06,  2.59s/it]153it [07:08,  2.37s/it]154it [07:10,  2.32s/it]155it [07:12,  2.25s/it]156it [07:14,  2.23s/it]157it [07:16,  2.13s/it]158it [07:19,  2.38s/it]159it [07:22,  2.69s/it]160it [07:25,  2.71s/it]161it [07:28,  2.80s/it]162it [07:32,  3.08s/it]163it [07:35,  2.93s/it]164it [07:37,  2.84s/it]165it [07:40,  2.73s/it]166it [07:43,  2.78s/it]167it [07:46,  3.09s/it]168it [07:49,  2.92s/it]169it [07:51,  2.80s/it]170it [07:54,  2.70s/it]171it [07:57,  2.94s/it]172it [08:00,  2.76s/it]173it [08:02,  2.68s/it]174it [08:05,  2.64s/it]175it [08:07,  2.51s/it]176it [08:11,  3.08s/it]177it [08:15,  3.14s/it]178it [08:17,  3.03s/it]179it [08:20,  3.02s/it]180it [08:23,  2.85s/it]181it [08:25,  2.71s/it]182it [08:28,  2.72s/it]183it [08:31,  2.66s/it]184it [08:33,  2.71s/it]185it [08:36,  2.71s/it]186it [08:39,  2.69s/it]187it [08:42,  2.72s/it]188it [08:44,  2.80s/it]189it [08:47,  2.86s/it]190it [08:50,  2.78s/it]191it [08:53,  2.72s/it]192it [08:55,  2.59s/it]193it [08:57,  2.48s/it]194it [09:00,  2.54s/it]195it [09:02,  2.45s/it]196it [09:04,  2.38s/it]197it [09:07,  2.48s/it]198it [09:10,  2.64s/it]199it [09:13,  2.72s/it]200it [09:16,  2.70s/it]201it [09:18,  2.63s/it]202it [09:20,  2.44s/it]203it [09:22,  2.33s/it]203it [09:22,  2.77s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<12:41,  3.03s/it]  2%|▏         | 5/252 [00:03<01:57,  2.09it/s]  4%|▎         | 9/252 [00:03<00:56,  4.33it/s]  5%|▌         | 13/252 [00:03<00:33,  7.04it/s]  7%|▋         | 17/252 [00:04<00:35,  6.65it/s]  8%|▊         | 21/252 [00:04<00:24,  9.29it/s] 10%|▉         | 25/252 [00:04<00:18, 12.28it/s] 12%|█▏        | 29/252 [00:04<00:14, 15.57it/s] 13%|█▎        | 33/252 [00:04<00:11, 18.78it/s] 15%|█▍        | 37/252 [00:04<00:10, 20.88it/s] 16%|█▋        | 41/252 [00:04<00:08, 23.91it/s] 18%|█▊        | 45/252 [00:04<00:07, 25.97it/s] 19%|█▉        | 49/252 [00:05<00:09, 21.70it/s] 21%|██        | 52/252 [00:05<00:18, 10.99it/s] 22%|██▏       | 55/252 [00:06<00:23,  8.36it/s] 23%|██▎       | 59/252 [00:06<00:17, 11.17it/s] 25%|██▌       | 63/252 [00:06<00:13, 14.26it/s] 27%|██▋       | 67/252 [00:06<00:10, 17.54it/s] 28%|██▊       | 70/252 [00:07<00:12, 14.84it/s] 29%|██▉       | 74/252 [00:07<00:09, 18.27it/s] 31%|███       | 78/252 [00:07<00:08, 21.57it/s] 33%|███▎      | 82/252 [00:07<00:06, 24.58it/s] 34%|███▍      | 86/252 [00:07<00:06, 26.35it/s] 36%|███▌      | 90/252 [00:07<00:05, 28.61it/s] 37%|███▋      | 94/252 [00:07<00:05, 30.15it/s] 39%|███▉      | 98/252 [00:08<00:10, 14.13it/s] 40%|████      | 102/252 [00:08<00:08, 17.28it/s] 42%|████▏     | 106/252 [00:08<00:07, 20.43it/s] 44%|████▎     | 110/252 [00:08<00:06, 23.41it/s] 45%|████▌     | 114/252 [00:09<00:09, 14.68it/s] 47%|████▋     | 118/252 [00:09<00:07, 17.81it/s] 48%|████▊     | 122/252 [00:09<00:06, 20.94it/s] 50%|█████     | 126/252 [00:09<00:05, 23.72it/s] 52%|█████▏    | 130/252 [00:09<00:05, 23.49it/s] 53%|█████▎    | 134/252 [00:09<00:04, 25.85it/s] 55%|█████▍    | 138/252 [00:09<00:04, 28.14it/s] 56%|█████▋    | 142/252 [00:10<00:03, 30.03it/s] 58%|█████▊    | 146/252 [00:10<00:05, 19.97it/s] 59%|█████▉    | 149/252 [00:11<00:10,  9.96it/s] 61%|██████    | 153/252 [00:11<00:07, 12.88it/s] 62%|██████▏   | 157/252 [00:11<00:05, 16.05it/s] 64%|██████▍   | 161/252 [00:11<00:04, 19.30it/s] 65%|██████▌   | 164/252 [00:11<00:06, 14.06it/s] 66%|██████▋   | 167/252 [00:12<00:06, 13.95it/s] 68%|██████▊   | 171/252 [00:12<00:04, 17.47it/s] 69%|██████▉   | 175/252 [00:12<00:03, 20.86it/s] 71%|███████   | 179/252 [00:12<00:04, 17.94it/s] 73%|███████▎  | 183/252 [00:12<00:03, 21.20it/s] 74%|███████▍  | 187/252 [00:12<00:02, 24.18it/s] 76%|███████▌  | 191/252 [00:13<00:02, 26.74it/s] 77%|███████▋  | 195/252 [00:13<00:02, 24.55it/s] 79%|███████▊  | 198/252 [00:13<00:03, 17.48it/s] 80%|████████  | 202/252 [00:13<00:02, 20.85it/s] 82%|████████▏ | 206/252 [00:13<00:01, 23.90it/s] 83%|████████▎ | 209/252 [00:13<00:01, 22.35it/s] 84%|████████▍ | 212/252 [00:14<00:02, 17.05it/s] 86%|████████▌ | 216/252 [00:14<00:01, 20.62it/s] 87%|████████▋ | 220/252 [00:14<00:01, 23.84it/s] 88%|████████▊ | 223/252 [00:14<00:01, 14.71it/s] 90%|█████████ | 227/252 [00:15<00:01, 18.01it/s] 91%|█████████▏| 230/252 [00:15<00:01, 19.76it/s] 93%|█████████▎| 234/252 [00:15<00:00, 23.17it/s] 94%|█████████▍| 238/252 [00:15<00:00, 26.10it/s] 96%|█████████▌| 242/252 [00:15<00:00, 20.04it/s] 98%|█████████▊| 246/252 [00:15<00:00, 23.16it/s] 99%|█████████▉| 250/252 [00:15<00:00, 25.94it/s]100%|██████████| 252/252 [00:16<00:00, 15.42it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<14:14,  3.40s/it]  2%|▏         | 5/252 [00:03<02:11,  1.88it/s]  3%|▎         | 8/252 [00:04<01:56,  2.10it/s]  5%|▍         | 12/252 [00:04<01:03,  3.78it/s]  6%|▋         | 16/252 [00:05<00:39,  5.91it/s]  8%|▊         | 20/252 [00:05<00:27,  8.45it/s] 10%|▉         | 24/252 [00:05<00:19, 11.45it/s] 11%|█         | 28/252 [00:05<00:15, 14.66it/s] 13%|█▎        | 32/252 [00:05<00:12, 17.82it/s] 14%|█▍        | 36/252 [00:05<00:10, 21.11it/s] 16%|█▌        | 40/252 [00:05<00:10, 19.63it/s] 17%|█▋        | 44/252 [00:05<00:09, 22.40it/s] 19%|█▊        | 47/252 [00:06<00:10, 18.92it/s] 20%|██        | 51/252 [00:06<00:09, 21.90it/s] 22%|██▏       | 55/252 [00:06<00:07, 24.84it/s] 23%|██▎       | 58/252 [00:06<00:11, 17.37it/s] 25%|██▍       | 62/252 [00:06<00:09, 20.60it/s] 26%|██▌       | 66/252 [00:06<00:07, 23.57it/s] 28%|██▊       | 70/252 [00:07<00:06, 26.10it/s] 29%|██▉       | 74/252 [00:07<00:11, 15.31it/s] 31%|███       | 78/252 [00:07<00:09, 18.50it/s] 33%|███▎      | 82/252 [00:07<00:07, 21.52it/s] 34%|███▍      | 86/252 [00:07<00:07, 23.65it/s] 35%|███▌      | 89/252 [00:08<00:08, 18.59it/s] 37%|███▋      | 93/252 [00:08<00:07, 21.85it/s] 38%|███▊      | 97/252 [00:08<00:06, 24.28it/s] 40%|████      | 101/252 [00:08<00:05, 26.89it/s] 42%|████▏     | 105/252 [00:08<00:06, 23.18it/s] 43%|████▎     | 109/252 [00:08<00:05, 25.74it/s] 45%|████▍     | 113/252 [00:09<00:04, 27.86it/s] 46%|████▋     | 117/252 [00:09<00:05, 26.80it/s] 48%|████▊     | 121/252 [00:09<00:04, 28.72it/s] 50%|████▉     | 125/252 [00:09<00:06, 18.96it/s] 51%|█████     | 129/252 [00:09<00:05, 22.03it/s] 53%|█████▎    | 133/252 [00:09<00:04, 24.58it/s] 54%|█████▍    | 137/252 [00:10<00:04, 27.05it/s] 56%|█████▌    | 141/252 [00:10<00:05, 21.27it/s] 58%|█████▊    | 145/252 [00:10<00:04, 24.10it/s] 59%|█████▉    | 149/252 [00:10<00:03, 26.36it/s] 61%|██████    | 153/252 [00:10<00:03, 28.34it/s] 62%|██████▏   | 157/252 [00:11<00:08, 11.28it/s] 64%|██████▍   | 161/252 [00:11<00:06, 13.95it/s] 65%|██████▌   | 165/252 [00:11<00:05, 16.93it/s] 67%|██████▋   | 169/252 [00:11<00:04, 19.98it/s] 69%|██████▊   | 173/252 [00:12<00:06, 12.17it/s] 70%|███████   | 177/252 [00:12<00:04, 15.06it/s] 72%|███████▏  | 181/252 [00:12<00:03, 18.04it/s] 73%|███████▎  | 185/252 [00:12<00:03, 20.95it/s] 75%|███████▍  | 188/252 [00:13<00:04, 15.60it/s] 76%|███████▌  | 192/252 [00:13<00:03, 18.66it/s] 78%|███████▊  | 196/252 [00:13<00:02, 21.61it/s] 79%|███████▉  | 200/252 [00:13<00:02, 24.36it/s] 81%|████████  | 204/252 [00:13<00:02, 17.33it/s] 83%|████████▎ | 208/252 [00:14<00:02, 20.45it/s] 84%|████████▍ | 212/252 [00:14<00:01, 23.32it/s] 86%|████████▌ | 216/252 [00:14<00:01, 25.97it/s] 87%|████████▋ | 220/252 [00:14<00:01, 22.62it/s] 89%|████████▉ | 224/252 [00:14<00:01, 25.47it/s] 90%|█████████ | 228/252 [00:15<00:01, 17.98it/s] 92%|█████████▏| 232/252 [00:15<00:00, 21.13it/s] 94%|█████████▎| 236/252 [00:15<00:00, 24.06it/s] 95%|█████████▌| 240/252 [00:15<00:00, 26.64it/s] 97%|█████████▋| 244/252 [00:15<00:00, 28.78it/s] 98%|█████████▊| 248/252 [00:15<00:00, 30.53it/s]100%|██████████| 252/252 [00:16<00:00, 15.40it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6257954835891724


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 50.32956099987563
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.10205767325374
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 36.449797274438026


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.57954788208008
=========================          END          =========================
0it [00:00, ?it/s]1it [00:03,  3.21s/it]2it [00:06,  3.04s/it]3it [00:08,  2.71s/it]4it [00:11,  2.93s/it]5it [00:14,  2.90s/it]6it [00:17,  2.99s/it]7it [00:21,  3.09s/it]8it [00:24,  3.12s/it]9it [00:27,  3.05s/it]10it [00:29,  2.95s/it]11it [00:33,  3.15s/it]12it [00:36,  3.16s/it]13it [00:39,  3.11s/it]14it [00:42,  3.18s/it]15it [00:46,  3.26s/it]16it [00:49,  3.19s/it]17it [00:52,  3.04s/it]18it [00:55,  3.09s/it]19it [00:58,  3.14s/it]20it [01:01,  3.11s/it]21it [01:04,  3.15s/it]22it [01:08,  3.24s/it]23it [01:11,  3.15s/it]24it [01:14,  3.18s/it]25it [01:17,  3.11s/it]26it [01:20,  3.12s/it]27it [01:23,  2.98s/it]28it [01:25,  2.71s/it]29it [01:28,  2.89s/it]30it [01:31,  2.96s/it]31it [01:33,  2.68s/it]32it [01:36,  2.73s/it]33it [01:39,  2.65s/it]34it [01:41,  2.59s/it]35it [01:43,  2.52s/it]36it [01:46,  2.45s/it]37it [01:48,  2.42s/it]38it [01:50,  2.35s/it]39it [01:52,  2.25s/it]40it [01:55,  2.48s/it]41it [01:58,  2.55s/it]42it [02:00,  2.54s/it]43it [02:03,  2.56s/it]44it [02:06,  2.58s/it]45it [02:08,  2.53s/it]46it [02:10,  2.47s/it]47it [02:13,  2.44s/it]48it [02:16,  2.62s/it]49it [02:19,  2.73s/it]50it [02:22,  2.74s/it]51it [02:25,  2.98s/it]52it [02:28,  2.93s/it]53it [02:31,  3.02s/it]54it [02:35,  3.40s/it]55it [02:39,  3.40s/it]56it [02:42,  3.26s/it]57it [02:45,  3.23s/it]58it [02:48,  3.18s/it]59it [02:51,  3.18s/it]60it [02:54,  3.15s/it]61it [02:58,  3.19s/it]62it [03:01,  3.33s/it]63it [03:05,  3.40s/it]64it [03:08,  3.36s/it]65it [03:11,  3.32s/it]66it [03:14,  3.28s/it]67it [03:18,  3.26s/it]68it [03:21,  3.20s/it]69it [03:24,  3.26s/it]70it [03:28,  3.44s/it]71it [03:31,  3.38s/it]72it [03:34,  3.33s/it]73it [03:38,  3.27s/it]74it [03:41,  3.28s/it]75it [03:44,  3.33s/it]76it [03:47,  3.11s/it]77it [03:50,  3.13s/it]78it [03:53,  3.21s/it]79it [03:57,  3.16s/it]80it [04:00,  3.27s/it]81it [04:06,  4.00s/it]82it [04:09,  3.69s/it]83it [04:12,  3.46s/it]84it [04:14,  3.23s/it]85it [04:17,  3.15s/it]86it [04:20,  3.13s/it]87it [04:23,  3.12s/it]88it [04:27,  3.09s/it]89it [04:29,  3.04s/it]90it [04:32,  3.01s/it]91it [04:35,  3.04s/it]92it [04:39,  3.11s/it]93it [04:41,  2.95s/it]94it [04:44,  2.90s/it]95it [04:47,  2.87s/it]96it [04:50,  2.95s/it]97it [04:53,  2.97s/it]98it [04:56,  2.90s/it]99it [04:59,  2.93s/it]100it [05:01,  2.79s/it]101it [05:04,  2.89s/it]102it [05:07,  2.69s/it]103it [05:09,  2.66s/it]104it [05:12,  2.58s/it]105it [05:14,  2.63s/it]106it [05:17,  2.70s/it]107it [05:20,  2.75s/it]108it [05:23,  2.73s/it]109it [05:25,  2.68s/it]110it [05:28,  2.58s/it]111it [05:30,  2.61s/it]112it [05:33,  2.72s/it]113it [05:36,  2.67s/it]114it [05:39,  2.85s/it]115it [05:42,  2.85s/it]116it [05:45,  2.89s/it]117it [05:48,  2.83s/it]118it [05:50,  2.69s/it]119it [05:53,  2.69s/it]120it [05:55,  2.63s/it]121it [05:58,  2.55s/it]122it [06:00,  2.51s/it]123it [06:03,  2.65s/it]124it [06:05,  2.57s/it]125it [06:08,  2.47s/it]126it [06:10,  2.55s/it]127it [06:13,  2.58s/it]128it [06:16,  2.58s/it]129it [06:18,  2.62s/it]130it [06:21,  2.61s/it]131it [06:23,  2.56s/it]132it [06:26,  2.58s/it]133it [06:29,  2.62s/it]134it [06:31,  2.65s/it]135it [06:34,  2.56s/it]136it [06:36,  2.45s/it]137it [06:39,  2.74s/it]138it [06:41,  2.51s/it]139it [06:43,  2.38s/it]140it [06:46,  2.34s/it]141it [06:49,  2.70s/it]142it [06:53,  2.95s/it]143it [06:55,  2.83s/it]144it [06:58,  2.72s/it]145it [07:01,  2.80s/it]146it [07:04,  2.83s/it]147it [07:06,  2.64s/it]148it [07:08,  2.62s/it]149it [07:11,  2.62s/it]150it [07:13,  2.59s/it]151it [07:16,  2.59s/it]152it [07:18,  2.53s/it]153it [07:21,  2.49s/it]154it [07:24,  2.66s/it]155it [07:26,  2.48s/it]156it [07:29,  2.58s/it]157it [07:31,  2.52s/it]158it [07:34,  2.60s/it]159it [07:36,  2.54s/it]160it [07:39,  2.70s/it]161it [07:42,  2.73s/it]162it [07:45,  2.69s/it]163it [07:47,  2.67s/it]164it [07:50,  2.51s/it]165it [07:51,  2.18s/it]166it [07:52,  1.96s/it]167it [07:55,  2.05s/it]168it [07:57,  1.98s/it]169it [07:58,  1.91s/it]170it [08:00,  1.88s/it]171it [08:02,  1.87s/it]172it [08:04,  1.91s/it]173it [08:06,  1.91s/it]174it [08:08,  1.91s/it]175it [08:10,  1.98s/it]176it [08:12,  1.94s/it]177it [08:13,  1.74s/it]178it [08:14,  1.50s/it]179it [08:16,  1.65s/it]180it [08:18,  1.71s/it]181it [08:20,  1.77s/it]182it [08:21,  1.64s/it]183it [08:23,  1.62s/it]184it [08:25,  1.85s/it]185it [08:27,  1.99s/it]186it [08:29,  1.99s/it]187it [08:31,  1.98s/it]188it [08:33,  1.94s/it]189it [08:35,  1.80s/it]190it [08:36,  1.79s/it]191it [08:38,  1.81s/it]192it [08:40,  1.88s/it]193it [08:42,  1.96s/it]194it [08:45,  2.03s/it]195it [08:47,  2.11s/it]196it [08:50,  2.30s/it]197it [08:52,  2.30s/it]198it [08:55,  2.44s/it]199it [08:58,  2.55s/it]200it [09:00,  2.47s/it]201it [09:02,  2.43s/it]202it [09:05,  2.52s/it]203it [09:08,  2.64s/it]203it [09:08,  2.70s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<12:57,  3.10s/it]  2%|▏         | 4/252 [00:03<02:34,  1.61it/s]  2%|▏         | 6/252 [00:03<01:38,  2.50it/s]  4%|▍         | 10/252 [00:03<00:46,  5.15it/s]  6%|▌         | 14/252 [00:03<00:28,  8.25it/s]  7%|▋         | 18/252 [00:03<00:19, 11.71it/s]  8%|▊         | 21/252 [00:04<00:22, 10.08it/s] 10%|▉         | 24/252 [00:04<00:23,  9.72it/s] 11%|█         | 28/252 [00:04<00:17, 13.00it/s] 13%|█▎        | 32/252 [00:04<00:13, 16.58it/s] 14%|█▍        | 36/252 [00:04<00:10, 19.67it/s] 16%|█▌        | 40/252 [00:05<00:09, 22.76it/s] 17%|█▋        | 44/252 [00:05<00:08, 25.33it/s] 19%|█▉        | 48/252 [00:05<00:07, 27.30it/s] 21%|██        | 52/252 [00:05<00:06, 29.29it/s] 22%|██▏       | 56/252 [00:05<00:07, 25.25it/s] 23%|██▎       | 59/252 [00:05<00:11, 16.81it/s] 25%|██▌       | 63/252 [00:06<00:09, 19.88it/s] 27%|██▋       | 67/252 [00:06<00:08, 23.00it/s] 28%|██▊       | 70/252 [00:06<00:13, 13.71it/s] 29%|██▉       | 74/252 [00:06<00:10, 16.95it/s] 31%|███       | 78/252 [00:07<00:10, 16.35it/s] 33%|███▎      | 82/252 [00:07<00:08, 19.59it/s] 34%|███▍      | 86/252 [00:07<00:07, 22.56it/s] 36%|███▌      | 90/252 [00:07<00:06, 25.07it/s] 37%|███▋      | 94/252 [00:07<00:08, 18.84it/s] 39%|███▉      | 98/252 [00:07<00:07, 21.50it/s] 40%|████      | 102/252 [00:07<00:06, 24.25it/s] 42%|████▏     | 106/252 [00:08<00:05, 26.79it/s] 44%|████▎     | 110/252 [00:08<00:04, 28.89it/s] 45%|████▌     | 114/252 [00:08<00:04, 29.97it/s] 47%|████▋     | 118/252 [00:08<00:04, 29.49it/s] 48%|████▊     | 122/252 [00:08<00:04, 30.84it/s] 50%|█████     | 126/252 [00:08<00:03, 31.99it/s] 52%|█████▏    | 130/252 [00:08<00:03, 32.76it/s] 53%|█████▎    | 134/252 [00:08<00:03, 33.34it/s] 55%|█████▍    | 138/252 [00:09<00:04, 23.01it/s] 56%|█████▋    | 142/252 [00:09<00:06, 16.38it/s] 58%|█████▊    | 146/252 [00:09<00:05, 19.33it/s] 60%|█████▉    | 150/252 [00:09<00:04, 21.92it/s] 61%|██████    | 153/252 [00:10<00:05, 16.84it/s] 62%|██████▏   | 157/252 [00:10<00:04, 20.20it/s] 63%|██████▎   | 160/252 [00:10<00:04, 21.53it/s] 65%|██████▌   | 164/252 [00:10<00:03, 24.31it/s] 66%|██████▋   | 167/252 [00:10<00:03, 25.52it/s] 68%|██████▊   | 171/252 [00:10<00:02, 28.02it/s] 69%|██████▉   | 175/252 [00:10<00:02, 30.01it/s] 71%|███████   | 179/252 [00:10<00:02, 31.54it/s] 73%|███████▎  | 183/252 [00:11<00:03, 21.67it/s] 74%|███████▍  | 186/252 [00:11<00:05, 11.66it/s] 75%|███████▌  | 190/252 [00:12<00:04, 14.83it/s] 77%|███████▋  | 194/252 [00:12<00:03, 18.06it/s] 79%|███████▊  | 198/252 [00:12<00:02, 20.41it/s] 80%|████████  | 202/252 [00:12<00:02, 23.47it/s] 82%|████████▏ | 206/252 [00:12<00:01, 26.18it/s] 83%|████████▎ | 210/252 [00:12<00:01, 28.31it/s] 85%|████████▍ | 214/252 [00:12<00:01, 25.76it/s] 87%|████████▋ | 218/252 [00:12<00:01, 28.10it/s] 88%|████████▊ | 222/252 [00:13<00:01, 23.05it/s] 90%|████████▉ | 226/252 [00:13<00:01, 25.76it/s] 91%|█████████ | 229/252 [00:14<00:02,  8.44it/s] 92%|█████████▏| 232/252 [00:14<00:02,  7.02it/s] 94%|█████████▎| 236/252 [00:15<00:01,  9.56it/s] 95%|█████████▌| 240/252 [00:15<00:00, 12.48it/s] 97%|█████████▋| 244/252 [00:15<00:00, 15.68it/s] 98%|█████████▊| 247/252 [00:15<00:00, 10.95it/s]100%|█████████▉| 251/252 [00:15<00:00, 14.12it/s]100%|██████████| 252/252 [00:16<00:00, 14.83it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:10,  3.15s/it]  2%|▏         | 5/252 [00:03<02:02,  2.02it/s]  4%|▎         | 9/252 [00:03<00:58,  4.19it/s]  5%|▌         | 13/252 [00:03<00:35,  6.81it/s]  7%|▋         | 17/252 [00:03<00:23,  9.87it/s]  8%|▊         | 21/252 [00:04<00:23,  9.69it/s] 10%|▉         | 25/252 [00:04<00:17, 12.81it/s] 11%|█         | 28/252 [00:04<00:15, 14.20it/s] 13%|█▎        | 32/252 [00:04<00:12, 17.73it/s] 14%|█▍        | 36/252 [00:04<00:10, 20.91it/s] 15%|█▌        | 39/252 [00:04<00:14, 15.04it/s] 17%|█▋        | 43/252 [00:05<00:11, 18.07it/s] 19%|█▊        | 47/252 [00:05<00:09, 21.35it/s] 20%|██        | 51/252 [00:05<00:08, 23.92it/s] 22%|██▏       | 55/252 [00:05<00:07, 25.57it/s] 23%|██▎       | 59/252 [00:05<00:06, 27.93it/s] 25%|██▌       | 63/252 [00:05<00:06, 28.62it/s] 27%|██▋       | 67/252 [00:05<00:06, 29.89it/s] 28%|██▊       | 71/252 [00:06<00:07, 24.29it/s] 29%|██▉       | 74/252 [00:06<00:13, 13.24it/s] 31%|███       | 78/252 [00:06<00:10, 16.34it/s] 33%|███▎      | 82/252 [00:06<00:08, 19.23it/s] 34%|███▍      | 86/252 [00:06<00:07, 22.37it/s] 35%|███▌      | 89/252 [00:07<00:11, 13.75it/s] 37%|███▋      | 93/252 [00:07<00:09, 16.89it/s] 38%|███▊      | 97/252 [00:07<00:07, 20.18it/s] 40%|████      | 101/252 [00:07<00:06, 23.30it/s] 42%|████▏     | 105/252 [00:07<00:05, 26.01it/s] 43%|████▎     | 109/252 [00:07<00:05, 28.19it/s] 45%|████▍     | 113/252 [00:08<00:05, 24.47it/s] 46%|████▋     | 117/252 [00:08<00:05, 26.83it/s] 48%|████▊     | 121/252 [00:08<00:04, 28.85it/s] 50%|████▉     | 125/252 [00:08<00:06, 18.26it/s] 51%|█████     | 129/252 [00:08<00:05, 20.94it/s] 53%|█████▎    | 133/252 [00:09<00:05, 23.76it/s] 54%|█████▍    | 136/252 [00:09<00:08, 13.76it/s] 56%|█████▌    | 140/252 [00:09<00:06, 17.07it/s] 57%|█████▋    | 144/252 [00:09<00:05, 20.35it/s] 59%|█████▊    | 148/252 [00:09<00:04, 23.22it/s] 60%|██████    | 152/252 [00:10<00:05, 17.33it/s] 62%|██████▏   | 156/252 [00:10<00:04, 20.51it/s] 63%|██████▎   | 160/252 [00:10<00:03, 23.51it/s] 65%|██████▌   | 164/252 [00:10<00:03, 26.18it/s] 67%|██████▋   | 168/252 [00:10<00:03, 25.49it/s] 68%|██████▊   | 171/252 [00:11<00:04, 17.76it/s] 69%|██████▉   | 175/252 [00:11<00:03, 21.10it/s] 71%|███████   | 178/252 [00:12<00:09,  8.03it/s] 72%|███████▏  | 182/252 [00:12<00:06, 10.73it/s] 74%|███████▍  | 186/252 [00:12<00:04, 13.79it/s] 75%|███████▌  | 190/252 [00:12<00:03, 17.03it/s] 77%|███████▋  | 194/252 [00:13<00:07,  8.09it/s] 79%|███████▊  | 198/252 [00:13<00:05, 10.59it/s] 80%|████████  | 202/252 [00:13<00:03, 13.44it/s] 82%|████████▏ | 206/252 [00:14<00:02, 16.52it/s] 83%|████████▎ | 210/252 [00:14<00:02, 19.56it/s] 85%|████████▍ | 214/252 [00:14<00:01, 22.63it/s] 87%|████████▋ | 218/252 [00:15<00:03,  8.57it/s] 88%|████████▊ | 222/252 [00:15<00:02, 11.10it/s] 90%|████████▉ | 226/252 [00:15<00:01, 14.01it/s] 91%|█████████▏| 230/252 [00:15<00:01, 17.13it/s] 93%|█████████▎| 234/252 [00:15<00:00, 18.14it/s] 94%|█████████▍| 238/252 [00:16<00:00, 21.28it/s] 96%|█████████▌| 242/252 [00:16<00:00, 24.20it/s] 98%|█████████▊| 246/252 [00:16<00:00, 26.78it/s] 99%|█████████▉| 250/252 [00:16<00:00, 22.41it/s]100%|██████████| 252/252 [00:17<00:00, 14.76it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6258078813552856


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.37196866061435
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.0621466647464
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 35.820598374315296


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.580787658691406
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.51s/it]2it [00:02,  1.27s/it]3it [00:03,  1.26s/it]4it [00:05,  1.34s/it]5it [00:06,  1.29s/it]6it [00:07,  1.31s/it]7it [00:08,  1.13s/it]8it [00:09,  1.15s/it]9it [00:11,  1.18s/it]10it [00:12,  1.16s/it]11it [00:13,  1.19s/it]12it [00:14,  1.18s/it]13it [00:15,  1.24s/it]14it [00:17,  1.24s/it]15it [00:18,  1.23s/it]16it [00:20,  1.38s/it]17it [00:20,  1.19s/it]18it [00:21,  1.08s/it]19it [00:23,  1.17s/it]20it [00:24,  1.18s/it]21it [00:25,  1.21s/it]22it [00:26,  1.27s/it]23it [00:27,  1.16s/it]24it [00:29,  1.16s/it]25it [00:30,  1.27s/it]26it [00:31,  1.16s/it]27it [00:33,  1.43s/it]28it [00:36,  1.88s/it]29it [00:37,  1.73s/it]30it [00:39,  1.65s/it]31it [00:40,  1.58s/it]32it [00:41,  1.41s/it]33it [00:42,  1.28s/it]34it [00:43,  1.14s/it]35it [00:44,  1.05s/it]36it [00:45,  1.06s/it]37it [00:46,  1.04s/it]38it [00:47,  1.15s/it]39it [00:48,  1.10s/it]40it [00:50,  1.25s/it]41it [00:51,  1.20s/it]42it [00:52,  1.25s/it]43it [00:54,  1.37s/it]44it [00:55,  1.39s/it]45it [00:57,  1.43s/it]46it [00:58,  1.40s/it]47it [01:00,  1.42s/it]48it [01:01,  1.39s/it]49it [01:02,  1.24s/it]50it [01:03,  1.24s/it]51it [01:05,  1.39s/it]52it [01:06,  1.40s/it]53it [01:07,  1.21s/it]54it [01:09,  1.31s/it]55it [01:10,  1.44s/it]56it [01:12,  1.41s/it]57it [01:13,  1.38s/it]58it [01:15,  1.47s/it]59it [01:16,  1.35s/it]60it [01:17,  1.35s/it]61it [01:18,  1.22s/it]62it [01:19,  1.25s/it]63it [01:20,  1.18s/it]64it [01:22,  1.22s/it]65it [01:23,  1.20s/it]66it [01:24,  1.21s/it]67it [01:25,  1.07s/it]68it [01:26,  1.02s/it]69it [01:27,  1.12s/it]70it [01:29,  1.24s/it]71it [01:30,  1.16s/it]72it [01:31,  1.26s/it]73it [01:32,  1.20s/it]74it [01:33,  1.21s/it]75it [01:35,  1.28s/it]76it [01:36,  1.27s/it]77it [01:38,  1.37s/it]78it [01:40,  1.52s/it]79it [01:41,  1.58s/it]80it [01:43,  1.55s/it]81it [01:46,  1.99s/it]82it [01:47,  1.86s/it]83it [01:49,  1.78s/it]84it [01:50,  1.68s/it]85it [01:52,  1.67s/it]86it [01:53,  1.49s/it]87it [01:55,  1.58s/it]88it [01:56,  1.55s/it]89it [01:58,  1.46s/it]90it [01:59,  1.50s/it]91it [02:01,  1.51s/it]92it [02:02,  1.53s/it]93it [02:04,  1.54s/it]94it [02:06,  1.70s/it]95it [02:07,  1.56s/it]96it [02:08,  1.47s/it]97it [02:10,  1.36s/it]98it [02:11,  1.32s/it]99it [02:12,  1.36s/it]100it [02:13,  1.27s/it]101it [02:16,  1.56s/it]102it [02:16,  1.33s/it]103it [02:17,  1.08s/it]104it [02:18,  1.00s/it]105it [02:19,  1.02it/s]106it [02:20,  1.13s/it]107it [02:21,  1.15s/it]108it [02:23,  1.21s/it]109it [02:24,  1.19s/it]110it [02:24,  1.03s/it]111it [02:25,  1.05it/s]112it [02:26,  1.02s/it]113it [02:27,  1.01it/s]114it [02:29,  1.29s/it]115it [02:30,  1.21s/it]116it [02:31,  1.14s/it]117it [02:33,  1.32s/it]118it [02:34,  1.28s/it]119it [02:35,  1.14s/it]120it [02:36,  1.11s/it]121it [02:37,  1.14s/it]122it [02:39,  1.18s/it]123it [02:40,  1.17s/it]124it [02:41,  1.22s/it]125it [02:42,  1.07s/it]126it [02:43,  1.12s/it]127it [02:45,  1.24s/it]128it [02:45,  1.15s/it]129it [02:47,  1.16s/it]130it [02:48,  1.13s/it]131it [02:49,  1.16s/it]132it [02:50,  1.21s/it]133it [02:52,  1.25s/it]134it [02:53,  1.29s/it]135it [02:54,  1.22s/it]136it [02:55,  1.11s/it]137it [02:56,  1.04s/it]138it [02:57,  1.14s/it]139it [02:58,  1.09s/it]140it [02:59,  1.17s/it]141it [03:01,  1.28s/it]142it [03:03,  1.43s/it]143it [03:04,  1.37s/it]144it [03:05,  1.21s/it]145it [03:06,  1.18s/it]146it [03:07,  1.19s/it]147it [03:08,  1.12s/it]148it [03:09,  1.00it/s]149it [03:10,  1.11s/it]150it [03:12,  1.31s/it]151it [03:13,  1.25s/it]152it [03:14,  1.17s/it]153it [03:15,  1.23s/it]154it [03:17,  1.22s/it]155it [03:18,  1.13s/it]156it [03:19,  1.30s/it]157it [03:20,  1.18s/it]158it [03:21,  1.19s/it]159it [03:22,  1.14s/it]160it [03:24,  1.18s/it]161it [03:25,  1.23s/it]162it [03:26,  1.28s/it]163it [03:27,  1.21s/it]164it [03:29,  1.22s/it]165it [03:30,  1.17s/it]166it [03:31,  1.05s/it]167it [03:32,  1.29s/it]168it [03:34,  1.24s/it]169it [03:35,  1.23s/it]170it [03:37,  1.47s/it]171it [03:41,  2.35s/it]172it [03:45,  2.79s/it]173it [03:46,  2.27s/it]174it [03:47,  1.98s/it]175it [03:48,  1.68s/it]176it [03:50,  1.66s/it]177it [03:51,  1.48s/it]178it [03:52,  1.29s/it]179it [03:53,  1.31s/it]180it [03:54,  1.21s/it]181it [03:56,  1.25s/it]182it [03:57,  1.29s/it]183it [03:58,  1.32s/it]184it [03:59,  1.09s/it]185it [04:00,  1.09s/it]186it [04:02,  1.26s/it]187it [04:03,  1.38s/it]188it [04:04,  1.25s/it]189it [04:06,  1.29s/it]190it [04:07,  1.32s/it]191it [04:08,  1.28s/it]192it [04:09,  1.18s/it]193it [04:10,  1.10s/it]194it [04:11,  1.10s/it]195it [04:12,  1.01it/s]196it [04:13,  1.08s/it]197it [04:14,  1.03s/it]198it [04:16,  1.21s/it]199it [04:17,  1.27s/it]200it [04:18,  1.21s/it]201it [04:20,  1.32s/it]202it [04:24,  2.19s/it]203it [04:27,  2.32s/it]203it [04:27,  1.32s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:02,  3.12s/it]  1%|          | 2/252 [00:03<06:32,  1.57s/it]  2%|▏         | 6/252 [00:03<01:34,  2.61it/s]  4%|▍         | 10/252 [00:03<00:47,  5.07it/s]  6%|▌         | 14/252 [00:03<00:29,  7.98it/s]  7%|▋         | 18/252 [00:04<00:20, 11.24it/s]  9%|▊         | 22/252 [00:04<00:15, 14.74it/s] 10%|█         | 26/252 [00:04<00:12, 18.11it/s] 12%|█▏        | 30/252 [00:04<00:10, 21.27it/s] 13%|█▎        | 34/252 [00:04<00:09, 24.00it/s] 15%|█▌        | 38/252 [00:04<00:08, 26.32it/s] 17%|█▋        | 42/252 [00:04<00:07, 28.31it/s] 18%|█▊        | 46/252 [00:04<00:06, 29.63it/s] 20%|█▉        | 50/252 [00:05<00:06, 30.62it/s] 21%|██▏       | 54/252 [00:05<00:06, 30.85it/s] 23%|██▎       | 58/252 [00:05<00:06, 31.63it/s] 25%|██▍       | 62/252 [00:05<00:05, 32.04it/s] 26%|██▌       | 66/252 [00:05<00:05, 32.40it/s] 28%|██▊       | 70/252 [00:05<00:05, 32.18it/s] 29%|██▉       | 74/252 [00:05<00:05, 32.65it/s] 31%|███       | 78/252 [00:05<00:05, 32.70it/s] 33%|███▎      | 82/252 [00:06<00:05, 32.74it/s] 34%|███▍      | 86/252 [00:06<00:04, 33.23it/s] 36%|███▌      | 90/252 [00:06<00:04, 32.55it/s] 37%|███▋      | 94/252 [00:06<00:04, 32.22it/s] 39%|███▉      | 98/252 [00:06<00:04, 31.94it/s] 40%|████      | 102/252 [00:06<00:04, 31.81it/s] 42%|████▏     | 106/252 [00:06<00:04, 32.69it/s] 44%|████▎     | 110/252 [00:06<00:04, 32.51it/s] 45%|████▌     | 114/252 [00:06<00:04, 32.90it/s] 47%|████▋     | 118/252 [00:07<00:04, 33.47it/s] 48%|████▊     | 122/252 [00:07<00:03, 33.92it/s] 50%|█████     | 126/252 [00:07<00:03, 33.55it/s] 52%|█████▏    | 130/252 [00:07<00:03, 33.15it/s] 53%|█████▎    | 134/252 [00:07<00:03, 33.75it/s] 55%|█████▍    | 138/252 [00:07<00:03, 32.67it/s] 56%|█████▋    | 142/252 [00:07<00:03, 32.75it/s] 58%|█████▊    | 146/252 [00:07<00:03, 32.36it/s] 60%|█████▉    | 150/252 [00:08<00:03, 32.14it/s] 61%|██████    | 154/252 [00:08<00:03, 32.45it/s] 63%|██████▎   | 158/252 [00:08<00:02, 32.55it/s] 64%|██████▍   | 162/252 [00:08<00:02, 32.66it/s] 66%|██████▌   | 166/252 [00:08<00:02, 32.83it/s] 67%|██████▋   | 170/252 [00:08<00:02, 32.75it/s] 69%|██████▉   | 174/252 [00:08<00:02, 33.27it/s] 71%|███████   | 178/252 [00:08<00:02, 33.44it/s] 72%|███████▏  | 182/252 [00:09<00:02, 33.34it/s] 74%|███████▍  | 186/252 [00:09<00:02, 31.88it/s] 75%|███████▌  | 190/252 [00:09<00:01, 32.67it/s] 77%|███████▋  | 194/252 [00:09<00:01, 32.55it/s] 79%|███████▊  | 198/252 [00:09<00:01, 32.39it/s] 80%|████████  | 202/252 [00:09<00:01, 32.69it/s] 82%|████████▏ | 206/252 [00:09<00:01, 32.42it/s] 83%|████████▎ | 210/252 [00:09<00:01, 31.96it/s] 85%|████████▍ | 214/252 [00:10<00:01, 32.35it/s] 87%|████████▋ | 218/252 [00:10<00:01, 32.46it/s] 88%|████████▊ | 222/252 [00:10<00:00, 32.76it/s] 90%|████████▉ | 226/252 [00:10<00:00, 33.22it/s] 91%|█████████▏| 230/252 [00:10<00:00, 33.86it/s] 93%|█████████▎| 234/252 [00:10<00:00, 34.21it/s] 94%|█████████▍| 238/252 [00:10<00:00, 34.25it/s] 96%|█████████▌| 242/252 [00:10<00:00, 34.34it/s] 98%|█████████▊| 246/252 [00:10<00:00, 34.54it/s] 99%|█████████▉| 250/252 [00:11<00:00, 34.56it/s]100%|██████████| 252/252 [00:11<00:00, 22.27it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:37,  3.26s/it]  2%|▏         | 5/252 [00:03<02:06,  1.95it/s]  4%|▎         | 9/252 [00:03<00:59,  4.05it/s]  5%|▌         | 13/252 [00:03<00:36,  6.61it/s]  7%|▋         | 17/252 [00:03<00:24,  9.56it/s]  8%|▊         | 21/252 [00:03<00:17, 12.86it/s] 10%|▉         | 25/252 [00:04<00:14, 15.20it/s] 12%|█▏        | 29/252 [00:04<00:12, 18.42it/s] 13%|█▎        | 33/252 [00:04<00:10, 21.24it/s] 15%|█▍        | 37/252 [00:04<00:08, 24.00it/s] 16%|█▋        | 41/252 [00:04<00:08, 26.12it/s] 18%|█▊        | 45/252 [00:04<00:07, 27.68it/s] 19%|█▉        | 49/252 [00:04<00:07, 28.06it/s] 21%|██        | 53/252 [00:04<00:06, 29.75it/s] 23%|██▎       | 57/252 [00:05<00:06, 29.82it/s] 24%|██▍       | 61/252 [00:05<00:06, 30.69it/s] 26%|██▌       | 65/252 [00:05<00:06, 30.81it/s] 27%|██▋       | 69/252 [00:05<00:05, 31.24it/s] 29%|██▉       | 73/252 [00:05<00:05, 30.53it/s] 31%|███       | 77/252 [00:05<00:05, 31.08it/s] 32%|███▏      | 81/252 [00:05<00:05, 31.86it/s] 34%|███▎      | 85/252 [00:05<00:05, 31.98it/s] 35%|███▌      | 89/252 [00:06<00:05, 32.04it/s] 37%|███▋      | 93/252 [00:06<00:04, 32.76it/s] 38%|███▊      | 97/252 [00:06<00:04, 32.95it/s] 40%|████      | 101/252 [00:06<00:04, 33.34it/s] 42%|████▏     | 105/252 [00:06<00:04, 33.51it/s] 43%|████▎     | 109/252 [00:06<00:04, 33.68it/s] 45%|████▍     | 113/252 [00:06<00:04, 32.98it/s] 46%|████▋     | 117/252 [00:06<00:04, 32.71it/s] 48%|████▊     | 121/252 [00:06<00:04, 32.29it/s] 50%|████▉     | 125/252 [00:07<00:04, 31.59it/s] 51%|█████     | 129/252 [00:07<00:03, 32.18it/s] 53%|█████▎    | 133/252 [00:07<00:03, 32.03it/s] 54%|█████▍    | 137/252 [00:07<00:03, 32.64it/s] 56%|█████▌    | 141/252 [00:07<00:03, 32.95it/s] 58%|█████▊    | 145/252 [00:07<00:03, 33.05it/s] 59%|█████▉    | 149/252 [00:07<00:03, 32.04it/s] 61%|██████    | 153/252 [00:07<00:03, 31.37it/s] 62%|██████▏   | 157/252 [00:08<00:02, 31.68it/s] 64%|██████▍   | 161/252 [00:08<00:02, 31.96it/s] 65%|██████▌   | 165/252 [00:08<00:02, 29.22it/s] 67%|██████▋   | 169/252 [00:08<00:02, 30.14it/s] 69%|██████▊   | 173/252 [00:08<00:02, 30.16it/s] 70%|███████   | 177/252 [00:08<00:02, 30.84it/s] 72%|███████▏  | 181/252 [00:08<00:02, 31.49it/s] 73%|███████▎  | 185/252 [00:09<00:02, 31.55it/s] 75%|███████▌  | 189/252 [00:09<00:01, 31.96it/s] 77%|███████▋  | 193/252 [00:09<00:01, 32.39it/s] 78%|███████▊  | 197/252 [00:09<00:01, 32.88it/s] 80%|███████▉  | 201/252 [00:09<00:01, 33.46it/s] 81%|████████▏ | 205/252 [00:09<00:01, 33.34it/s] 83%|████████▎ | 209/252 [00:09<00:01, 33.62it/s] 85%|████████▍ | 213/252 [00:09<00:01, 33.57it/s] 86%|████████▌ | 217/252 [00:09<00:01, 33.30it/s] 88%|████████▊ | 221/252 [00:10<00:00, 33.53it/s] 89%|████████▉ | 225/252 [00:10<00:00, 33.41it/s] 91%|█████████ | 229/252 [00:10<00:00, 33.41it/s] 92%|█████████▏| 233/252 [00:10<00:00, 33.45it/s] 94%|█████████▍| 237/252 [00:10<00:00, 33.95it/s] 96%|█████████▌| 241/252 [00:10<00:00, 34.35it/s] 97%|█████████▋| 245/252 [00:10<00:00, 34.68it/s] 99%|█████████▉| 249/252 [00:10<00:00, 34.86it/s]100%|██████████| 252/252 [00:11<00:00, 22.56it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6260204315185547


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.832110434025616
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.21476992674494
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 35.86488367577564


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.60204315185547
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.14s/it]2it [00:02,  1.02it/s]3it [00:03,  1.19s/it]4it [00:04,  1.21s/it]5it [00:05,  1.12s/it]6it [00:06,  1.07s/it]7it [00:07,  1.00s/it]8it [00:08,  1.06it/s]9it [00:09,  1.14it/s]10it [00:10,  1.09it/s]11it [00:10,  1.17it/s]12it [00:11,  1.20it/s]13it [00:12,  1.14it/s]14it [00:13,  1.17it/s]15it [00:14,  1.20it/s]16it [00:15,  1.01s/it]17it [00:16,  1.08it/s]18it [00:16,  1.18it/s]19it [00:18,  1.02it/s]20it [00:19,  1.05s/it]21it [00:20,  1.02s/it]22it [00:21,  1.01s/it]23it [00:22,  1.07it/s]24it [00:22,  1.14it/s]25it [00:23,  1.21it/s]26it [00:24,  1.04it/s]27it [00:26,  1.12s/it]28it [00:27,  1.15s/it]29it [00:28,  1.16s/it]30it [00:29,  1.12s/it]31it [00:30,  1.09s/it]32it [00:31,  1.05s/it]33it [00:32,  1.00it/s]34it [00:33,  1.05it/s]35it [00:34,  1.12it/s]36it [00:35,  1.06it/s]37it [00:36,  1.06it/s]38it [00:37,  1.06s/it]39it [00:38,  1.01it/s]40it [00:39,  1.05it/s]41it [00:40,  1.11it/s]42it [00:40,  1.16it/s]43it [00:41,  1.14it/s]44it [00:42,  1.17it/s]45it [00:43,  1.08it/s]46it [00:44,  1.12it/s]47it [00:45,  1.17it/s]48it [00:45,  1.20it/s]49it [00:46,  1.22it/s]50it [00:47,  1.25it/s]51it [00:48,  1.05it/s]52it [00:49,  1.08it/s]53it [00:50,  1.02it/s]54it [00:52,  1.12s/it]55it [00:53,  1.15s/it]56it [00:54,  1.12s/it]57it [00:55,  1.09s/it]58it [00:56,  1.07s/it]59it [00:57,  1.07s/it]60it [00:58,  1.01it/s]61it [00:59,  1.04s/it]62it [01:00,  1.06s/it]63it [01:01,  1.04s/it]64it [01:02,  1.00it/s]65it [01:03,  1.03s/it]66it [01:04,  1.06it/s]67it [01:05,  1.09it/s]68it [01:06,  1.17it/s]69it [01:06,  1.18it/s]70it [01:07,  1.22it/s]71it [01:08,  1.23it/s]72it [01:09,  1.27it/s]73it [01:09,  1.23it/s]74it [01:10,  1.20it/s]75it [01:11,  1.14it/s]76it [01:12,  1.10it/s]77it [01:13,  1.19it/s]78it [01:14,  1.15it/s]79it [01:15,  1.09it/s]80it [01:16,  1.03it/s]81it [01:19,  1.56s/it]82it [01:20,  1.38s/it]83it [01:21,  1.22s/it]84it [01:22,  1.18s/it]85it [01:23,  1.19s/it]86it [01:24,  1.06s/it]87it [01:25,  1.09s/it]88it [01:26,  1.01s/it]89it [01:27,  1.01s/it]90it [01:28,  1.00it/s]91it [01:29,  1.01s/it]92it [01:30,  1.05it/s]93it [01:31,  1.07it/s]94it [01:32,  1.06it/s]95it [01:33,  1.04it/s]96it [01:34,  1.03s/it]97it [01:34,  1.09it/s]98it [01:35,  1.19it/s]99it [01:36,  1.16it/s]100it [01:37,  1.21it/s]101it [01:38,  1.06it/s]102it [01:39,  1.16it/s]103it [01:39,  1.21it/s]104it [01:40,  1.25it/s]105it [01:41,  1.22it/s]106it [01:42,  1.05it/s]107it [01:43,  1.08it/s]108it [01:44,  1.08it/s]109it [01:45,  1.05it/s]110it [01:46,  1.13it/s]111it [01:47,  1.17it/s]112it [01:47,  1.20it/s]113it [01:48,  1.23it/s]114it [01:50,  1.13s/it]115it [01:51,  1.01s/it]116it [01:51,  1.17it/s]117it [01:53,  1.11s/it]118it [01:54,  1.04s/it]119it [01:54,  1.11it/s]120it [01:55,  1.10it/s]121it [01:56,  1.17it/s]122it [01:57,  1.12it/s]123it [01:58,  1.10it/s]124it [01:59,  1.13it/s]125it [01:59,  1.20it/s]126it [02:01,  1.06it/s]127it [02:01,  1.14it/s]128it [02:02,  1.20it/s]129it [02:03,  1.05it/s]130it [02:04,  1.07it/s]131it [02:05,  1.07it/s]132it [02:06,  1.09it/s]133it [02:07,  1.06it/s]134it [02:08,  1.01it/s]135it [02:09,  1.02it/s]136it [02:10,  1.04it/s]137it [02:11,  1.01it/s]138it [02:12,  1.04it/s]139it [02:13,  1.11it/s]140it [02:14,  1.11it/s]141it [02:14,  1.15it/s]142it [02:15,  1.09it/s]143it [02:17,  1.02s/it]144it [02:17,  1.05it/s]145it [02:18,  1.09it/s]146it [02:19,  1.06it/s]147it [02:20,  1.01s/it]148it [02:21,  1.00s/it]149it [02:22,  1.02it/s]150it [02:24,  1.10s/it]151it [02:25,  1.14s/it]152it [02:26,  1.05s/it]153it [02:27,  1.02s/it]154it [02:28,  1.08s/it]155it [02:29,  1.03s/it]156it [02:30,  1.07s/it]157it [02:31,  1.04it/s]158it [02:32,  1.07it/s]159it [02:33,  1.07it/s]160it [02:34,  1.04it/s]161it [02:34,  1.08it/s]162it [02:36,  1.00it/s]163it [02:37,  1.02it/s]164it [02:37,  1.09it/s]165it [02:38,  1.05it/s]166it [02:39,  1.18it/s]167it [02:41,  1.14s/it]168it [02:42,  1.09s/it]169it [02:42,  1.10it/s]170it [02:43,  1.07it/s]171it [02:45,  1.05s/it]172it [02:46,  1.05s/it]173it [02:46,  1.04it/s]174it [02:47,  1.02it/s]175it [02:49,  1.02s/it]176it [02:50,  1.12s/it]177it [02:51,  1.06s/it]178it [02:52,  1.02it/s]179it [02:53,  1.08s/it]180it [02:54,  1.17s/it]181it [02:55,  1.08s/it]182it [02:56,  1.01it/s]183it [02:57,  1.04it/s]184it [02:57,  1.15it/s]185it [02:58,  1.18it/s]186it [02:59,  1.13it/s]187it [03:00,  1.13it/s]188it [03:01,  1.18it/s]189it [03:02,  1.26it/s]190it [03:02,  1.30it/s]191it [03:03,  1.12it/s]192it [03:04,  1.29it/s]193it [03:05,  1.19it/s]194it [03:06,  1.12it/s]195it [03:07,  1.05it/s]196it [03:08,  1.07it/s]197it [03:08,  1.22it/s]198it [03:09,  1.21it/s]199it [03:10,  1.27it/s]200it [03:11,  1.17it/s]201it [03:12,  1.11it/s]202it [03:13,  1.11it/s]203it [03:14,  1.08it/s]203it [03:14,  1.04it/s]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<12:52,  3.08s/it]  1%|          | 3/252 [00:03<03:57,  1.05it/s]  3%|▎         | 7/252 [00:03<01:19,  3.07it/s]  4%|▍         | 11/252 [00:03<00:43,  5.56it/s]  6%|▌         | 15/252 [00:03<00:27,  8.52it/s]  8%|▊         | 19/252 [00:03<00:19, 11.74it/s]  9%|▉         | 23/252 [00:04<00:15, 15.18it/s] 11%|█         | 27/252 [00:04<00:12, 18.55it/s] 12%|█▏        | 31/252 [00:04<00:10, 21.50it/s] 14%|█▍        | 35/252 [00:04<00:08, 24.35it/s] 15%|█▌        | 39/252 [00:04<00:07, 26.85it/s] 17%|█▋        | 43/252 [00:04<00:07, 26.81it/s] 19%|█▊        | 47/252 [00:04<00:07, 28.83it/s] 20%|██        | 51/252 [00:04<00:06, 30.18it/s] 22%|██▏       | 55/252 [00:05<00:06, 31.11it/s] 23%|██▎       | 59/252 [00:05<00:06, 30.20it/s] 25%|██▌       | 63/252 [00:05<00:06, 30.65it/s] 27%|██▋       | 67/252 [00:05<00:05, 31.02it/s] 28%|██▊       | 71/252 [00:05<00:06, 29.95it/s] 30%|██▉       | 75/252 [00:05<00:06, 28.86it/s] 31%|███       | 78/252 [00:05<00:06, 28.84it/s] 33%|███▎      | 82/252 [00:05<00:05, 29.81it/s] 34%|███▍      | 86/252 [00:06<00:05, 30.34it/s] 36%|███▌      | 90/252 [00:06<00:05, 30.02it/s] 37%|███▋      | 94/252 [00:06<00:05, 28.25it/s] 39%|███▉      | 98/252 [00:06<00:05, 29.08it/s] 40%|████      | 101/252 [00:06<00:05, 27.98it/s] 42%|████▏     | 105/252 [00:06<00:05, 28.35it/s] 43%|████▎     | 108/252 [00:06<00:05, 27.33it/s] 44%|████▍     | 111/252 [00:07<00:05, 27.00it/s] 46%|████▌     | 115/252 [00:07<00:04, 28.55it/s] 47%|████▋     | 118/252 [00:07<00:04, 28.91it/s] 48%|████▊     | 122/252 [00:07<00:04, 30.11it/s] 50%|█████     | 126/252 [00:07<00:04, 31.46it/s] 52%|█████▏    | 130/252 [00:07<00:03, 32.11it/s] 53%|█████▎    | 134/252 [00:07<00:03, 32.64it/s] 55%|█████▍    | 138/252 [00:07<00:03, 31.10it/s] 56%|█████▋    | 142/252 [00:07<00:03, 31.21it/s] 58%|█████▊    | 146/252 [00:08<00:03, 31.42it/s] 60%|█████▉    | 150/252 [00:08<00:03, 31.29it/s] 61%|██████    | 154/252 [00:08<00:03, 26.88it/s] 63%|██████▎   | 158/252 [00:08<00:03, 28.38it/s] 64%|██████▍   | 162/252 [00:08<00:03, 29.45it/s] 66%|██████▌   | 166/252 [00:08<00:02, 30.61it/s] 67%|██████▋   | 170/252 [00:08<00:02, 27.44it/s] 69%|██████▉   | 174/252 [00:09<00:02, 28.45it/s] 71%|███████   | 178/252 [00:09<00:02, 29.54it/s] 72%|███████▏  | 182/252 [00:09<00:02, 30.95it/s] 74%|███████▍  | 186/252 [00:09<00:03, 19.34it/s] 75%|███████▌  | 190/252 [00:09<00:02, 22.26it/s] 77%|███████▋  | 194/252 [00:09<00:02, 24.66it/s] 79%|███████▊  | 198/252 [00:10<00:02, 26.65it/s] 80%|████████  | 202/252 [00:10<00:01, 28.37it/s] 82%|████████▏ | 206/252 [00:10<00:01, 28.98it/s] 83%|████████▎ | 210/252 [00:10<00:01, 30.35it/s] 85%|████████▍ | 214/252 [00:10<00:01, 31.14it/s] 87%|████████▋ | 218/252 [00:10<00:01, 24.71it/s] 88%|████████▊ | 222/252 [00:10<00:01, 27.07it/s] 90%|████████▉ | 226/252 [00:11<00:00, 29.10it/s] 91%|█████████▏| 230/252 [00:11<00:00, 30.75it/s] 93%|█████████▎| 234/252 [00:11<00:00, 31.87it/s] 94%|█████████▍| 238/252 [00:11<00:00, 32.88it/s] 96%|█████████▌| 242/252 [00:11<00:00, 22.53it/s] 98%|█████████▊| 246/252 [00:11<00:00, 25.12it/s] 99%|█████████▉| 250/252 [00:11<00:00, 27.28it/s]100%|██████████| 252/252 [00:12<00:00, 20.35it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:12,  3.16s/it]  2%|▏         | 5/252 [00:03<02:07,  1.94it/s]  4%|▎         | 9/252 [00:03<01:00,  4.02it/s]  5%|▌         | 13/252 [00:03<00:36,  6.55it/s]  7%|▋         | 17/252 [00:03<00:24,  9.50it/s]  8%|▊         | 21/252 [00:03<00:19, 11.95it/s] 10%|▉         | 25/252 [00:04<00:14, 15.20it/s] 12%|█▏        | 29/252 [00:04<00:12, 18.05it/s] 13%|█▎        | 32/252 [00:04<00:11, 19.75it/s] 14%|█▍        | 36/252 [00:04<00:09, 22.93it/s] 16%|█▌        | 40/252 [00:04<00:08, 25.61it/s] 17%|█▋        | 44/252 [00:04<00:07, 27.72it/s] 19%|█▉        | 48/252 [00:04<00:06, 29.16it/s] 21%|██        | 52/252 [00:04<00:06, 30.14it/s] 22%|██▏       | 56/252 [00:05<00:07, 26.18it/s] 23%|██▎       | 59/252 [00:05<00:07, 26.45it/s] 25%|██▍       | 62/252 [00:05<00:06, 27.26it/s] 26%|██▌       | 66/252 [00:05<00:06, 28.97it/s] 28%|██▊       | 70/252 [00:05<00:05, 30.53it/s] 29%|██▉       | 74/252 [00:05<00:05, 30.62it/s] 31%|███       | 78/252 [00:05<00:05, 31.72it/s] 33%|███▎      | 82/252 [00:05<00:05, 32.47it/s] 34%|███▍      | 86/252 [00:06<00:05, 31.96it/s] 36%|███▌      | 90/252 [00:06<00:05, 31.51it/s] 37%|███▋      | 94/252 [00:06<00:04, 32.17it/s] 39%|███▉      | 98/252 [00:06<00:04, 32.32it/s] 40%|████      | 102/252 [00:06<00:04, 32.90it/s] 42%|████▏     | 106/252 [00:06<00:04, 33.41it/s] 44%|████▎     | 110/252 [00:06<00:04, 32.92it/s] 45%|████▌     | 114/252 [00:06<00:04, 33.28it/s] 47%|████▋     | 118/252 [00:06<00:03, 33.68it/s] 48%|████▊     | 122/252 [00:07<00:03, 32.99it/s] 50%|█████     | 126/252 [00:07<00:03, 33.06it/s] 52%|█████▏    | 130/252 [00:07<00:08, 14.47it/s] 53%|█████▎    | 134/252 [00:07<00:06, 17.40it/s] 55%|█████▍    | 138/252 [00:08<00:05, 20.28it/s] 56%|█████▋    | 142/252 [00:08<00:04, 22.54it/s] 58%|█████▊    | 146/252 [00:08<00:04, 24.69it/s] 60%|█████▉    | 150/252 [00:08<00:03, 26.72it/s] 61%|██████    | 154/252 [00:08<00:03, 28.63it/s] 63%|██████▎   | 158/252 [00:08<00:03, 29.87it/s] 64%|██████▍   | 162/252 [00:08<00:02, 31.06it/s] 66%|██████▌   | 166/252 [00:08<00:02, 32.09it/s] 67%|██████▋   | 170/252 [00:09<00:02, 28.02it/s] 69%|██████▊   | 173/252 [00:09<00:02, 28.28it/s] 70%|███████   | 177/252 [00:09<00:02, 29.29it/s] 72%|███████▏  | 181/252 [00:09<00:02, 29.30it/s] 73%|███████▎  | 185/252 [00:09<00:02, 30.63it/s] 75%|███████▌  | 189/252 [00:09<00:02, 30.99it/s] 77%|███████▋  | 193/252 [00:09<00:01, 31.44it/s] 78%|███████▊  | 197/252 [00:09<00:01, 31.59it/s] 80%|███████▉  | 201/252 [00:10<00:01, 32.16it/s] 81%|████████▏ | 205/252 [00:10<00:01, 28.31it/s] 83%|████████▎ | 209/252 [00:10<00:01, 29.54it/s] 85%|████████▍ | 213/252 [00:10<00:01, 30.35it/s] 86%|████████▌ | 217/252 [00:10<00:01, 31.39it/s] 88%|████████▊ | 221/252 [00:10<00:01, 21.62it/s] 89%|████████▉ | 225/252 [00:11<00:01, 24.27it/s] 91%|█████████ | 229/252 [00:11<00:00, 26.67it/s] 92%|█████████▏| 233/252 [00:11<00:00, 28.46it/s] 94%|█████████▍| 237/252 [00:11<00:00, 30.22it/s] 96%|█████████▌| 241/252 [00:11<00:00, 31.59it/s] 97%|█████████▋| 245/252 [00:11<00:00, 32.63it/s] 99%|█████████▉| 249/252 [00:12<00:00, 15.00it/s]100%|██████████| 252/252 [00:12<00:00, 19.56it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6252211928367615


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.52120383036936
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.12500227127803
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 35.829995757715174


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.522117614746094
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.05s/it]2it [00:03,  1.91s/it]3it [00:05,  1.97s/it]4it [00:08,  2.03s/it]5it [00:09,  1.84s/it]6it [00:11,  1.99s/it]7it [00:13,  1.90s/it]8it [00:15,  1.95s/it]9it [00:17,  1.93s/it]10it [00:19,  1.98s/it]11it [00:21,  1.88s/it]12it [00:22,  1.67s/it]13it [00:24,  1.71s/it]14it [00:26,  1.79s/it]15it [00:28,  1.84s/it]16it [00:30,  1.89s/it]17it [00:32,  1.96s/it]18it [00:34,  1.98s/it]19it [00:36,  2.10s/it]20it [00:38,  2.07s/it]21it [00:40,  2.11s/it]22it [00:42,  2.09s/it]23it [00:45,  2.10s/it]24it [00:47,  2.21s/it]25it [00:49,  2.16s/it]26it [00:51,  2.15s/it]27it [00:54,  2.23s/it]28it [00:55,  2.07s/it]29it [00:57,  2.03s/it]30it [00:59,  1.91s/it]31it [01:01,  1.95s/it]32it [01:03,  1.88s/it]33it [01:05,  1.92s/it]34it [01:07,  2.04s/it]35it [01:09,  2.06s/it]36it [01:11,  2.04s/it]37it [01:14,  2.21s/it]38it [01:16,  2.22s/it]39it [01:18,  2.09s/it]40it [01:19,  1.96s/it]41it [01:21,  1.85s/it]42it [01:23,  1.80s/it]43it [01:24,  1.81s/it]44it [01:26,  1.81s/it]45it [01:28,  1.91s/it]46it [01:31,  1.98s/it]47it [01:32,  1.90s/it]48it [01:34,  1.85s/it]49it [01:36,  1.94s/it]50it [01:38,  2.01s/it]51it [01:40,  2.04s/it]52it [01:42,  1.95s/it]53it [01:44,  1.84s/it]54it [01:46,  1.86s/it]55it [01:48,  1.91s/it]56it [01:49,  1.85s/it]57it [01:51,  1.73s/it]58it [01:53,  1.84s/it]59it [01:55,  1.87s/it]60it [01:57,  1.83s/it]61it [01:58,  1.72s/it]62it [02:00,  1.86s/it]63it [02:02,  1.97s/it]64it [02:04,  1.97s/it]65it [02:06,  1.91s/it]66it [02:08,  1.88s/it]67it [02:10,  1.88s/it]68it [02:12,  2.01s/it]69it [02:14,  1.99s/it]70it [02:16,  1.96s/it]71it [02:18,  1.93s/it]72it [02:19,  1.80s/it]73it [02:21,  1.82s/it]74it [02:23,  1.86s/it]75it [02:25,  1.96s/it]76it [02:27,  1.96s/it]77it [02:29,  1.91s/it]78it [02:31,  1.91s/it]79it [02:33,  1.78s/it]80it [02:34,  1.62s/it]81it [02:37,  2.00s/it]82it [02:38,  1.86s/it]83it [02:39,  1.67s/it]84it [02:41,  1.55s/it]85it [02:42,  1.47s/it]86it [02:43,  1.34s/it]87it [02:45,  1.57s/it]88it [02:47,  1.60s/it]89it [02:48,  1.32s/it]90it [02:48,  1.22s/it]91it [02:49,  1.10s/it]92it [02:50,  1.01it/s]93it [02:51,  1.06it/s]94it [02:52,  1.11s/it]95it [02:53,  1.00s/it]96it [02:54,  1.08it/s]97it [02:55,  1.10s/it]98it [02:56,  1.08s/it]99it [02:58,  1.24s/it]100it [03:00,  1.34s/it]101it [03:02,  1.52s/it]102it [03:03,  1.39s/it]103it [03:04,  1.25s/it]104it [03:05,  1.18s/it]105it [03:06,  1.30s/it]106it [03:08,  1.44s/it]107it [03:10,  1.55s/it]108it [03:11,  1.51s/it]109it [03:13,  1.48s/it]110it [03:13,  1.32s/it]111it [03:14,  1.23s/it]112it [03:16,  1.19s/it]113it [03:17,  1.13s/it]114it [03:19,  1.52s/it]115it [03:20,  1.37s/it]116it [03:21,  1.27s/it]117it [03:23,  1.46s/it]118it [03:24,  1.32s/it]119it [03:26,  1.48s/it]120it [03:27,  1.54s/it]121it [03:29,  1.55s/it]122it [03:31,  1.58s/it]123it [03:32,  1.53s/it]124it [03:34,  1.60s/it]125it [03:36,  1.68s/it]126it [03:38,  1.90s/it]127it [03:40,  1.97s/it]128it [03:42,  1.96s/it]129it [03:44,  1.84s/it]130it [03:46,  1.90s/it]131it [03:48,  1.94s/it]132it [03:50,  1.99s/it]133it [03:53,  2.17s/it]134it [03:56,  2.43s/it]135it [03:58,  2.44s/it]136it [04:00,  2.39s/it]137it [04:03,  2.39s/it]138it [04:06,  2.54s/it]139it [04:08,  2.49s/it]140it [04:10,  2.47s/it]141it [04:13,  2.55s/it]142it [04:16,  2.57s/it]143it [04:18,  2.52s/it]144it [04:20,  2.40s/it]145it [04:23,  2.34s/it]146it [04:25,  2.35s/it]147it [04:27,  2.28s/it]148it [04:29,  2.29s/it]149it [04:31,  2.23s/it]150it [04:35,  2.54s/it]151it [04:37,  2.47s/it]152it [04:39,  2.32s/it]153it [04:41,  2.23s/it]154it [04:43,  2.28s/it]155it [04:46,  2.29s/it]156it [04:48,  2.41s/it]157it [04:51,  2.33s/it]158it [04:53,  2.41s/it]159it [04:56,  2.52s/it]160it [04:59,  2.70s/it]161it [05:02,  2.66s/it]162it [05:05,  2.75s/it]163it [05:07,  2.61s/it]164it [05:09,  2.39s/it]165it [05:11,  2.24s/it]166it [05:13,  2.18s/it]167it [05:16,  2.42s/it]168it [05:18,  2.41s/it]169it [05:20,  2.32s/it]170it [05:23,  2.36s/it]171it [05:26,  2.64s/it]172it [05:28,  2.50s/it]173it [05:30,  2.37s/it]174it [05:33,  2.43s/it]175it [05:35,  2.30s/it]176it [05:37,  2.26s/it]177it [05:39,  2.28s/it]178it [05:41,  2.08s/it]179it [05:43,  2.19s/it]180it [05:45,  2.17s/it]181it [05:48,  2.22s/it]182it [05:50,  2.16s/it]183it [05:52,  2.11s/it]184it [05:54,  2.12s/it]185it [05:56,  2.23s/it]186it [05:58,  2.18s/it]187it [06:00,  2.06s/it]188it [06:02,  1.94s/it]189it [06:04,  1.90s/it]190it [06:05,  1.80s/it]191it [06:07,  1.70s/it]192it [06:09,  1.89s/it]193it [06:12,  2.25s/it]194it [06:16,  2.65s/it]195it [06:18,  2.51s/it]196it [06:20,  2.35s/it]197it [06:22,  2.43s/it]198it [06:24,  2.24s/it]199it [06:26,  2.18s/it]200it [06:28,  2.11s/it]201it [06:30,  2.04s/it]202it [06:32,  1.98s/it]203it [06:34,  1.89s/it]203it [06:34,  1.94s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:50,  3.31s/it]  2%|▏         | 5/252 [00:03<02:08,  1.93it/s]  3%|▎         | 7/252 [00:03<01:34,  2.59it/s]  4%|▍         | 11/252 [00:03<00:48,  4.98it/s]  6%|▌         | 15/252 [00:04<00:30,  7.81it/s]  8%|▊         | 19/252 [00:04<00:21, 11.05it/s]  9%|▉         | 23/252 [00:04<00:16, 14.02it/s] 11%|█         | 27/252 [00:04<00:13, 17.20it/s] 12%|█▏        | 31/252 [00:04<00:10, 20.31it/s] 14%|█▍        | 35/252 [00:04<00:09, 22.94it/s] 15%|█▌        | 39/252 [00:04<00:08, 25.06it/s] 17%|█▋        | 43/252 [00:04<00:07, 26.78it/s] 19%|█▊        | 47/252 [00:05<00:07, 28.19it/s] 20%|██        | 51/252 [00:05<00:06, 29.43it/s] 22%|██▏       | 55/252 [00:05<00:06, 30.30it/s] 23%|██▎       | 59/252 [00:05<00:06, 31.18it/s] 25%|██▌       | 63/252 [00:05<00:05, 31.92it/s] 27%|██▋       | 67/252 [00:06<00:11, 15.66it/s] 28%|██▊       | 71/252 [00:06<00:09, 18.56it/s] 30%|██▉       | 75/252 [00:06<00:08, 21.47it/s] 31%|███▏      | 79/252 [00:06<00:07, 23.75it/s] 33%|███▎      | 82/252 [00:06<00:06, 24.95it/s] 34%|███▍      | 86/252 [00:06<00:06, 27.19it/s] 36%|███▌      | 90/252 [00:06<00:05, 29.02it/s] 37%|███▋      | 94/252 [00:06<00:05, 30.49it/s] 39%|███▉      | 98/252 [00:07<00:11, 13.93it/s] 40%|████      | 102/252 [00:07<00:08, 16.86it/s] 42%|████▏     | 105/252 [00:07<00:07, 18.86it/s] 43%|████▎     | 108/252 [00:07<00:07, 19.74it/s] 44%|████▍     | 112/252 [00:07<00:06, 22.74it/s] 46%|████▌     | 115/252 [00:08<00:08, 16.08it/s] 47%|████▋     | 119/252 [00:08<00:06, 19.03it/s] 48%|████▊     | 122/252 [00:08<00:06, 20.83it/s] 50%|█████     | 126/252 [00:08<00:05, 23.80it/s] 52%|█████▏    | 130/252 [00:08<00:04, 26.28it/s] 53%|█████▎    | 134/252 [00:08<00:04, 27.85it/s] 55%|█████▍    | 138/252 [00:09<00:04, 26.99it/s] 56%|█████▋    | 142/252 [00:09<00:03, 29.09it/s] 58%|█████▊    | 146/252 [00:09<00:03, 30.75it/s] 60%|█████▉    | 150/252 [00:09<00:04, 25.37it/s] 61%|██████    | 154/252 [00:09<00:05, 16.37it/s] 63%|██████▎   | 158/252 [00:10<00:04, 19.58it/s] 64%|██████▍   | 162/252 [00:10<00:03, 22.65it/s] 66%|██████▌   | 166/252 [00:10<00:03, 25.37it/s] 67%|██████▋   | 170/252 [00:10<00:05, 15.05it/s] 69%|██████▉   | 174/252 [00:10<00:04, 18.21it/s] 71%|███████   | 178/252 [00:11<00:03, 21.32it/s] 72%|███████▏  | 181/252 [00:11<00:03, 18.79it/s] 73%|███████▎  | 185/252 [00:11<00:06, 10.91it/s] 75%|███████▌  | 189/252 [00:12<00:04, 13.91it/s] 77%|███████▋  | 193/252 [00:12<00:03, 17.12it/s] 78%|███████▊  | 197/252 [00:12<00:02, 20.31it/s] 80%|███████▉  | 201/252 [00:12<00:02, 17.42it/s] 81%|████████▏ | 205/252 [00:12<00:02, 20.61it/s] 83%|████████▎ | 209/252 [00:12<00:01, 23.62it/s] 85%|████████▍ | 213/252 [00:12<00:01, 26.28it/s] 86%|████████▌ | 217/252 [00:13<00:02, 14.94it/s] 88%|████████▊ | 221/252 [00:13<00:01, 17.80it/s] 89%|████████▉ | 225/252 [00:13<00:01, 20.54it/s] 91%|█████████ | 229/252 [00:13<00:00, 23.24it/s] 92%|█████████▏| 233/252 [00:13<00:00, 25.61it/s] 94%|█████████▍| 237/252 [00:14<00:00, 26.64it/s] 96%|█████████▌| 241/252 [00:14<00:00, 15.37it/s] 97%|█████████▋| 245/252 [00:14<00:00, 18.53it/s] 99%|█████████▉| 249/252 [00:14<00:00, 21.28it/s]100%|██████████| 252/252 [00:15<00:00, 18.44it/s]100%|██████████| 252/252 [00:15<00:00, 16.05it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:41,  3.27s/it]  1%|          | 3/252 [00:03<04:01,  1.03it/s]  3%|▎         | 7/252 [00:03<01:21,  3.01it/s]  4%|▍         | 11/252 [00:03<00:44,  5.43it/s]  6%|▌         | 15/252 [00:03<00:28,  8.27it/s]  8%|▊         | 19/252 [00:04<00:20, 11.47it/s]  9%|▉         | 23/252 [00:04<00:15, 14.87it/s] 11%|█         | 27/252 [00:04<00:12, 18.14it/s] 12%|█▏        | 31/252 [00:04<00:10, 21.29it/s] 14%|█▍        | 35/252 [00:04<00:09, 23.93it/s] 15%|█▌        | 39/252 [00:04<00:08, 26.30it/s] 17%|█▋        | 43/252 [00:04<00:07, 28.01it/s] 19%|█▊        | 47/252 [00:04<00:07, 28.51it/s] 20%|██        | 51/252 [00:05<00:06, 29.82it/s] 22%|██▏       | 55/252 [00:05<00:06, 30.38it/s] 23%|██▎       | 59/252 [00:05<00:06, 30.96it/s] 25%|██▌       | 63/252 [00:05<00:06, 30.91it/s] 27%|██▋       | 67/252 [00:05<00:05, 31.73it/s] 28%|██▊       | 71/252 [00:05<00:05, 31.82it/s] 30%|██▉       | 75/252 [00:05<00:05, 31.72it/s] 31%|███▏      | 79/252 [00:05<00:05, 32.32it/s] 33%|███▎      | 83/252 [00:06<00:05, 32.59it/s] 35%|███▍      | 87/252 [00:06<00:05, 32.95it/s] 36%|███▌      | 91/252 [00:06<00:04, 32.89it/s] 38%|███▊      | 95/252 [00:06<00:04, 32.97it/s] 39%|███▉      | 99/252 [00:06<00:04, 33.28it/s] 41%|████      | 103/252 [00:06<00:04, 32.91it/s] 42%|████▏     | 107/252 [00:06<00:04, 32.98it/s] 44%|████▍     | 111/252 [00:06<00:04, 33.29it/s] 46%|████▌     | 115/252 [00:07<00:04, 33.21it/s] 47%|████▋     | 119/252 [00:07<00:04, 32.73it/s] 49%|████▉     | 123/252 [00:07<00:03, 33.05it/s] 50%|█████     | 127/252 [00:07<00:03, 32.75it/s] 52%|█████▏    | 131/252 [00:07<00:03, 32.79it/s] 54%|█████▎    | 135/252 [00:07<00:03, 31.62it/s] 55%|█████▌    | 139/252 [00:07<00:03, 32.30it/s] 57%|█████▋    | 143/252 [00:07<00:03, 32.42it/s] 58%|█████▊    | 147/252 [00:08<00:03, 32.51it/s] 60%|█████▉    | 151/252 [00:08<00:03, 32.64it/s] 62%|██████▏   | 155/252 [00:08<00:03, 31.98it/s] 63%|██████▎   | 159/252 [00:08<00:02, 32.34it/s] 65%|██████▍   | 163/252 [00:08<00:02, 32.65it/s] 66%|██████▋   | 167/252 [00:08<00:02, 32.94it/s] 68%|██████▊   | 171/252 [00:08<00:02, 31.89it/s] 69%|██████▉   | 175/252 [00:08<00:02, 31.93it/s] 71%|███████   | 179/252 [00:09<00:02, 32.17it/s] 73%|███████▎  | 183/252 [00:09<00:02, 32.51it/s] 74%|███████▍  | 187/252 [00:09<00:01, 32.94it/s] 76%|███████▌  | 191/252 [00:09<00:01, 33.48it/s] 77%|███████▋  | 195/252 [00:09<00:01, 33.20it/s] 79%|███████▉  | 199/252 [00:09<00:01, 32.99it/s] 81%|████████  | 203/252 [00:09<00:01, 33.03it/s] 82%|████████▏ | 207/252 [00:09<00:01, 33.29it/s] 84%|████████▎ | 211/252 [00:09<00:01, 33.66it/s] 85%|████████▌ | 215/252 [00:10<00:01, 33.72it/s] 87%|████████▋ | 219/252 [00:10<00:00, 33.07it/s] 88%|████████▊ | 223/252 [00:10<00:00, 32.59it/s] 90%|█████████ | 227/252 [00:10<00:00, 33.35it/s] 92%|█████████▏| 231/252 [00:10<00:00, 33.84it/s] 93%|█████████▎| 235/252 [00:10<00:00, 34.11it/s] 95%|█████████▍| 239/252 [00:10<00:00, 34.50it/s] 96%|█████████▋| 243/252 [00:10<00:00, 34.74it/s] 98%|█████████▊| 247/252 [00:11<00:00, 34.96it/s]100%|█████████▉| 251/252 [00:11<00:00, 35.12it/s]100%|██████████| 252/252 [00:11<00:00, 22.23it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6251515746116638


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.68287526427061
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.14018564712384
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 36.24988162447252


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.51515579223633
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.56s/it]2it [00:02,  1.27s/it]3it [00:03,  1.06s/it]4it [00:04,  1.11it/s]5it [00:04,  1.14it/s]6it [00:06,  1.03it/s]7it [00:07,  1.09s/it]8it [00:08,  1.06s/it]9it [00:09,  1.03s/it]10it [00:09,  1.30it/s]11it [00:10,  1.34it/s]12it [00:10,  1.36it/s]13it [00:11,  1.23it/s]14it [00:12,  1.20it/s]15it [00:13,  1.33it/s]16it [00:14,  1.05it/s]17it [00:15,  1.02it/s]18it [00:16,  1.03it/s]19it [00:17,  1.02it/s]20it [00:18,  1.05it/s]21it [00:19,  1.10it/s]22it [00:20,  1.11it/s]23it [00:21,  1.18it/s]24it [00:21,  1.22it/s]25it [00:22,  1.25it/s]26it [00:23,  1.06it/s]27it [00:24,  1.02it/s]28it [00:25,  1.09it/s]29it [00:26,  1.04it/s]30it [00:27,  1.06it/s]31it [00:28,  1.12it/s]32it [00:29,  1.22it/s]33it [00:29,  1.26it/s]34it [00:30,  1.34it/s]35it [00:31,  1.35it/s]36it [00:32,  1.23it/s]37it [00:32,  1.32it/s]38it [00:34,  1.09it/s]39it [00:34,  1.16it/s]40it [00:35,  1.22it/s]41it [00:36,  1.21it/s]42it [00:37,  1.23it/s]43it [00:37,  1.26it/s]44it [00:38,  1.19it/s]45it [00:40,  1.06it/s]46it [00:41,  1.01it/s]47it [00:42,  1.00s/it]48it [00:43,  1.00s/it]49it [00:43,  1.07it/s]50it [00:44,  1.15it/s]51it [00:46,  1.08s/it]52it [00:47,  1.01s/it]53it [00:47,  1.06it/s]54it [00:49,  1.11s/it]55it [00:50,  1.13s/it]56it [00:51,  1.11s/it]57it [00:52,  1.08s/it]58it [00:53,  1.07s/it]59it [00:54,  1.04s/it]60it [00:55,  1.06it/s]61it [00:56,  1.07it/s]62it [00:57,  1.06it/s]63it [00:58,  1.11it/s]64it [00:58,  1.22it/s]65it [00:59,  1.24it/s]66it [01:00,  1.30it/s]67it [01:01,  1.27it/s]68it [01:01,  1.30it/s]69it [01:02,  1.29it/s]70it [01:03,  1.29it/s]71it [01:04,  1.26it/s]72it [01:04,  1.24it/s]73it [01:05,  1.16it/s]74it [01:06,  1.13it/s]75it [01:07,  1.17it/s]76it [01:08,  1.15it/s]77it [01:09,  1.18it/s]78it [01:10,  1.07it/s]79it [01:13,  1.58s/it]80it [01:16,  1.86s/it]81it [01:21,  2.79s/it]82it [01:23,  2.73s/it]83it [01:25,  2.45s/it]84it [01:26,  2.02s/it]85it [01:28,  2.03s/it]86it [01:32,  2.48s/it]87it [01:35,  2.78s/it]88it [01:39,  3.11s/it]89it [01:42,  2.99s/it]90it [01:45,  3.00s/it]91it [01:48,  3.05s/it]92it [01:50,  2.78s/it]93it [01:53,  2.84s/it]94it [01:56,  2.79s/it]95it [01:59,  3.03s/it]96it [02:03,  3.30s/it]97it [02:06,  3.27s/it]98it [02:10,  3.26s/it]99it [02:13,  3.35s/it]100it [02:16,  3.32s/it]101it [02:20,  3.50s/it]102it [02:24,  3.40s/it]103it [02:27,  3.51s/it]104it [02:30,  3.33s/it]105it [02:33,  3.25s/it]106it [02:36,  3.10s/it]107it [02:39,  2.95s/it]108it [02:42,  3.08s/it]109it [02:43,  2.35s/it]110it [02:43,  1.88s/it]111it [02:45,  1.67s/it]112it [02:46,  1.59s/it]113it [02:48,  1.75s/it]114it [02:51,  2.19s/it]115it [02:55,  2.66s/it]116it [02:59,  2.93s/it]117it [03:02,  3.04s/it]118it [03:05,  3.19s/it]119it [03:07,  2.77s/it]120it [03:09,  2.55s/it]121it [03:11,  2.38s/it]122it [03:13,  2.16s/it]123it [03:14,  1.96s/it]124it [03:16,  1.85s/it]125it [03:17,  1.65s/it]126it [03:19,  1.78s/it]127it [03:21,  1.66s/it]128it [03:21,  1.31s/it]129it [03:23,  1.54s/it]130it [03:25,  1.62s/it]131it [03:27,  1.85s/it]132it [03:30,  2.09s/it]133it [03:33,  2.49s/it]134it [03:36,  2.62s/it]135it [03:40,  2.77s/it]136it [03:43,  3.08s/it]137it [03:46,  3.07s/it]138it [03:48,  2.71s/it]139it [03:49,  2.18s/it]140it [03:50,  1.81s/it]141it [03:51,  1.48s/it]142it [03:52,  1.51s/it]143it [03:55,  1.84s/it]144it [03:57,  1.87s/it]145it [03:59,  1.93s/it]146it [04:01,  2.02s/it]147it [04:04,  2.35s/it]148it [04:06,  2.26s/it]149it [04:08,  1.96s/it]150it [04:09,  1.80s/it]151it [04:11,  1.71s/it]152it [04:11,  1.43s/it]153it [04:13,  1.37s/it]154it [04:16,  1.90s/it]155it [04:18,  2.02s/it]156it [04:19,  1.78s/it]157it [04:20,  1.46s/it]158it [04:21,  1.23s/it]159it [04:22,  1.10s/it]160it [04:22,  1.02s/it]161it [04:23,  1.03it/s]162it [04:24,  1.04s/it]163it [04:25,  1.02s/it]164it [04:26,  1.00it/s]165it [04:27,  1.08it/s]166it [04:28,  1.17it/s]167it [04:29,  1.06s/it]168it [04:30,  1.02it/s]169it [04:31,  1.12it/s]170it [04:32,  1.18it/s]171it [04:33,  1.04s/it]172it [04:34,  1.03it/s]173it [04:35,  1.06s/it]174it [04:36,  1.05s/it]175it [04:37,  1.01it/s]176it [04:38,  1.10s/it]177it [04:39,  1.00s/it]178it [04:40,  1.03s/it]179it [04:41,  1.06s/it]180it [04:42,  1.03it/s]181it [04:43,  1.07it/s]182it [04:44,  1.14it/s]183it [04:44,  1.23it/s]184it [04:45,  1.30it/s]185it [04:46,  1.29it/s]186it [04:46,  1.35it/s]187it [04:47,  1.33it/s]188it [04:48,  1.18it/s]189it [04:49,  1.09it/s]190it [04:50,  1.15it/s]191it [04:51,  1.17it/s]192it [04:52,  1.22it/s]193it [04:53,  1.20it/s]194it [04:53,  1.19it/s]195it [04:54,  1.17it/s]196it [04:55,  1.16it/s]197it [04:56,  1.18it/s]198it [04:58,  1.05s/it]199it [04:59,  1.06s/it]200it [05:00,  1.03s/it]201it [05:01,  1.03s/it]202it [05:01,  1.04it/s]203it [05:02,  1.01it/s]203it [05:02,  1.49s/it]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:00,  3.11s/it]  2%|▏         | 5/252 [00:03<02:00,  2.05it/s]  4%|▎         | 9/252 [00:03<00:57,  4.22it/s]  5%|▌         | 13/252 [00:03<00:34,  6.86it/s]  7%|▋         | 17/252 [00:03<00:24,  9.74it/s]  8%|▊         | 20/252 [00:03<00:20, 11.11it/s] 10%|▉         | 24/252 [00:03<00:15, 14.59it/s] 11%|█         | 28/252 [00:04<00:12, 18.18it/s] 13%|█▎        | 32/252 [00:04<00:10, 21.28it/s] 14%|█▍        | 36/252 [00:04<00:08, 24.23it/s] 16%|█▌        | 40/252 [00:04<00:07, 26.78it/s] 17%|█▋        | 44/252 [00:04<00:07, 28.22it/s] 19%|█▉        | 48/252 [00:04<00:06, 29.61it/s] 21%|██        | 52/252 [00:04<00:06, 29.49it/s] 22%|██▏       | 56/252 [00:04<00:06, 29.82it/s] 24%|██▍       | 60/252 [00:04<00:06, 31.02it/s] 25%|██▌       | 64/252 [00:05<00:06, 31.32it/s] 27%|██▋       | 68/252 [00:05<00:05, 32.24it/s] 29%|██▊       | 72/252 [00:05<00:05, 32.29it/s] 30%|███       | 76/252 [00:05<00:05, 32.93it/s] 32%|███▏      | 80/252 [00:05<00:05, 33.02it/s] 33%|███▎      | 84/252 [00:05<00:05, 32.69it/s] 35%|███▍      | 88/252 [00:05<00:04, 33.26it/s] 37%|███▋      | 92/252 [00:05<00:04, 33.13it/s] 38%|███▊      | 96/252 [00:06<00:04, 33.51it/s] 40%|███▉      | 100/252 [00:06<00:04, 33.49it/s] 41%|████▏     | 104/252 [00:06<00:04, 33.77it/s] 43%|████▎     | 108/252 [00:06<00:04, 33.88it/s] 44%|████▍     | 112/252 [00:06<00:04, 34.05it/s] 46%|████▌     | 116/252 [00:06<00:04, 33.94it/s] 48%|████▊     | 120/252 [00:06<00:03, 33.85it/s] 49%|████▉     | 124/252 [00:06<00:03, 33.76it/s] 51%|█████     | 128/252 [00:07<00:03, 33.61it/s] 52%|█████▏    | 132/252 [00:07<00:03, 33.57it/s] 54%|█████▍    | 136/252 [00:07<00:03, 32.76it/s] 56%|█████▌    | 140/252 [00:07<00:03, 33.31it/s] 57%|█████▋    | 144/252 [00:07<00:03, 33.53it/s] 59%|█████▊    | 148/252 [00:07<00:03, 33.80it/s] 60%|██████    | 152/252 [00:07<00:02, 33.97it/s] 62%|██████▏   | 156/252 [00:07<00:02, 33.68it/s] 63%|██████▎   | 160/252 [00:07<00:02, 34.06it/s] 65%|██████▌   | 164/252 [00:08<00:02, 33.28it/s] 67%|██████▋   | 168/252 [00:08<00:04, 18.79it/s] 68%|██████▊   | 172/252 [00:08<00:03, 21.78it/s] 70%|██████▉   | 176/252 [00:08<00:03, 24.38it/s] 71%|███████▏  | 180/252 [00:08<00:02, 26.36it/s] 73%|███████▎  | 184/252 [00:09<00:02, 28.19it/s] 75%|███████▍  | 188/252 [00:09<00:02, 29.78it/s] 76%|███████▌  | 192/252 [00:09<00:01, 30.71it/s] 78%|███████▊  | 196/252 [00:09<00:01, 31.19it/s] 79%|███████▉  | 200/252 [00:09<00:01, 31.84it/s] 81%|████████  | 204/252 [00:09<00:01, 32.04it/s] 83%|████████▎ | 208/252 [00:09<00:01, 32.54it/s] 84%|████████▍ | 212/252 [00:09<00:01, 32.92it/s] 86%|████████▌ | 216/252 [00:09<00:01, 33.06it/s] 87%|████████▋ | 220/252 [00:10<00:00, 33.34it/s] 89%|████████▉ | 224/252 [00:10<00:00, 33.72it/s] 90%|█████████ | 228/252 [00:10<00:00, 33.49it/s] 92%|█████████▏| 232/252 [00:10<00:00, 33.30it/s] 94%|█████████▎| 236/252 [00:10<00:00, 33.87it/s] 95%|█████████▌| 240/252 [00:10<00:00, 34.36it/s] 97%|█████████▋| 244/252 [00:10<00:00, 34.70it/s] 98%|█████████▊| 248/252 [00:10<00:00, 34.45it/s]100%|██████████| 252/252 [00:11<00:00, 22.48it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631430983543396
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:03<13:08,  3.14s/it]  1%|          | 3/252 [00:03<03:38,  1.14it/s]  2%|▏         | 5/252 [00:03<01:53,  2.18it/s]  3%|▎         | 7/252 [00:03<01:10,  3.47it/s]  4%|▎         | 9/252 [00:03<00:49,  4.92it/s]  4%|▍         | 11/252 [00:03<00:37,  6.47it/s]  5%|▌         | 13/252 [00:03<00:28,  8.25it/s]  6%|▌         | 15/252 [00:04<00:24,  9.63it/s]  7%|▋         | 17/252 [00:04<00:21, 11.03it/s]  8%|▊         | 19/252 [00:04<00:19, 12.22it/s]  8%|▊         | 21/252 [00:04<00:17, 13.31it/s]  9%|▉         | 23/252 [00:04<00:16, 13.84it/s] 10%|▉         | 25/252 [00:04<00:15, 14.36it/s] 11%|█         | 27/252 [00:04<00:14, 15.24it/s] 12%|█▏        | 29/252 [00:04<00:14, 15.38it/s] 12%|█▏        | 31/252 [00:05<00:14, 15.56it/s] 13%|█▎        | 34/252 [00:05<00:13, 16.76it/s] 14%|█▍        | 36/252 [00:05<00:13, 16.56it/s] 15%|█▌        | 38/252 [00:05<00:13, 16.23it/s] 16%|█▌        | 40/252 [00:05<00:12, 16.83it/s] 17%|█▋        | 42/252 [00:05<00:12, 16.58it/s] 17%|█▋        | 44/252 [00:05<00:12, 16.40it/s] 18%|█▊        | 46/252 [00:05<00:12, 16.54it/s] 19%|█▉        | 48/252 [00:06<00:12, 16.67it/s] 20%|█▉        | 50/252 [00:06<00:12, 16.38it/s] 21%|██        | 52/252 [00:06<00:12, 16.26it/s] 21%|██▏       | 54/252 [00:06<00:11, 16.54it/s] 22%|██▏       | 56/252 [00:06<00:12, 16.29it/s] 23%|██▎       | 58/252 [00:06<00:12, 16.10it/s] 24%|██▍       | 60/252 [00:06<00:11, 17.08it/s] 25%|██▍       | 62/252 [00:06<00:11, 16.75it/s] 25%|██▌       | 64/252 [00:07<00:11, 16.33it/s] 26%|██▌       | 66/252 [00:07<00:11, 16.37it/s] 27%|██▋       | 68/252 [00:07<00:11, 16.56it/s] 28%|██▊       | 70/252 [00:07<00:11, 16.19it/s] 29%|██▊       | 72/252 [00:07<00:11, 16.05it/s] 29%|██▉       | 74/252 [00:07<00:10, 16.83it/s] 30%|███       | 76/252 [00:07<00:10, 16.27it/s] 31%|███       | 78/252 [00:07<00:10, 16.19it/s] 32%|███▏      | 81/252 [00:08<00:09, 17.80it/s] 33%|███▎      | 83/252 [00:08<00:09, 17.19it/s] 34%|███▍      | 86/252 [00:08<00:08, 19.93it/s] 36%|███▌      | 90/252 [00:08<00:06, 23.79it/s] 37%|███▋      | 94/252 [00:08<00:05, 26.95it/s] 39%|███▉      | 98/252 [00:08<00:05, 29.22it/s] 40%|████      | 102/252 [00:08<00:04, 30.62it/s] 42%|████▏     | 106/252 [00:08<00:04, 31.81it/s] 44%|████▎     | 110/252 [00:08<00:04, 32.60it/s] 45%|████▌     | 114/252 [00:09<00:04, 32.84it/s] 47%|████▋     | 118/252 [00:09<00:04, 33.42it/s] 48%|████▊     | 122/252 [00:09<00:03, 33.59it/s] 50%|█████     | 126/252 [00:09<00:03, 33.36it/s] 52%|█████▏    | 130/252 [00:09<00:03, 33.72it/s] 53%|█████▎    | 134/252 [00:09<00:03, 33.28it/s] 55%|█████▍    | 138/252 [00:09<00:03, 33.52it/s] 56%|█████▋    | 142/252 [00:09<00:03, 33.30it/s] 58%|█████▊    | 146/252 [00:10<00:03, 33.54it/s] 60%|█████▉    | 150/252 [00:10<00:03, 32.44it/s] 61%|██████    | 154/252 [00:10<00:02, 32.91it/s] 63%|██████▎   | 158/252 [00:10<00:02, 33.33it/s] 64%|██████▍   | 162/252 [00:10<00:02, 33.36it/s] 66%|██████▌   | 166/252 [00:10<00:02, 33.04it/s] 67%|██████▋   | 170/252 [00:10<00:02, 33.36it/s] 69%|██████▉   | 174/252 [00:10<00:02, 33.34it/s] 71%|███████   | 178/252 [00:11<00:02, 33.13it/s] 72%|███████▏  | 182/252 [00:11<00:02, 33.07it/s] 74%|███████▍  | 186/252 [00:11<00:01, 33.47it/s] 75%|███████▌  | 190/252 [00:11<00:02, 26.91it/s] 77%|███████▋  | 193/252 [00:11<00:02, 26.96it/s] 78%|███████▊  | 197/252 [00:11<00:01, 28.61it/s] 80%|███████▉  | 201/252 [00:11<00:01, 29.85it/s] 81%|████████▏ | 205/252 [00:11<00:01, 31.06it/s] 83%|████████▎ | 209/252 [00:12<00:01, 31.69it/s] 85%|████████▍ | 213/252 [00:12<00:01, 32.10it/s] 86%|████████▌ | 217/252 [00:12<00:01, 32.87it/s] 88%|████████▊ | 221/252 [00:12<00:00, 32.15it/s] 89%|████████▉ | 225/252 [00:12<00:00, 32.10it/s] 91%|█████████ | 229/252 [00:12<00:00, 32.90it/s] 92%|█████████▏| 233/252 [00:12<00:00, 33.41it/s] 94%|█████████▍| 237/252 [00:12<00:00, 33.38it/s] 96%|█████████▌| 241/252 [00:13<00:00, 33.42it/s] 97%|█████████▋| 245/252 [00:13<00:00, 33.03it/s] 99%|█████████▉| 249/252 [00:13<00:00, 33.14it/s]100%|██████████| 252/252 [00:13<00:00, 18.65it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6260507106781006


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.3143081665039
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.471458773784356
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.10877162393712
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 35.765911444377565


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.605072021484375
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.06s/it]2it [00:01,  1.05it/s]3it [00:02,  1.17it/s]4it [00:03,  1.21it/s]5it [00:04,  1.20it/s]6it [00:05,  1.13it/s]7it [00:05,  1.29it/s]8it [00:06,  1.23it/s]9it [00:07,  1.23it/s]10it [00:08,  1.29it/s]11it [00:09,  1.29it/s]12it [00:09,  1.32it/s]13it [00:10,  1.24it/s]14it [00:11,  1.25it/s]15it [00:12,  1.26it/s]16it [00:13,  1.04it/s]17it [00:14,  1.11it/s]18it [00:14,  1.21it/s]19it [00:15,  1.21it/s]20it [00:16,  1.22it/s]21it [00:17,  1.17it/s]22it [00:18,  1.21it/s]23it [00:18,  1.29it/s]24it [00:19,  1.33it/s]25it [00:20,  1.29it/s]26it [00:21,  1.02it/s]27it [00:23,  1.17s/it]28it [00:24,  1.10s/it]29it [00:25,  1.11s/it]30it [00:26,  1.09s/it]31it [00:27,  1.02s/it]32it [00:28,  1.02s/it]33it [00:29,  1.05it/s]34it [00:30,  1.02s/it]35it [00:30,  1.34it/s]36it [00:31,  1.44it/s]37it [00:32,  1.09it/s]38it [00:34,  1.11s/it]39it [00:34,  1.00s/it]40it [00:35,  1.09it/s]41it [00:36,  1.13it/s]42it [00:37,  1.19it/s]43it [00:37,  1.22it/s]44it [00:38,  1.20it/s]45it [00:39,  1.10it/s]46it [00:40,  1.14it/s]47it [00:41,  1.18it/s]48it [00:42,  1.23it/s]49it [00:42,  1.32it/s]50it [00:43,  1.32it/s]51it [00:44,  1.10it/s]52it [00:45,  1.13it/s]53it [00:46,  1.16it/s]54it [00:47,  1.01s/it]55it [00:48,  1.06it/s]56it [00:49,  1.14it/s]57it [00:50,  1.18it/s]58it [00:51,  1.12it/s]59it [00:52,  1.07it/s]60it [00:53,  1.08it/s]61it [00:53,  1.11it/s]62it [00:54,  1.07it/s]63it [00:55,  1.11it/s]64it [00:56,  1.20it/s]65it [00:57,  1.27it/s]66it [00:57,  1.29it/s]67it [00:58,  1.27it/s]68it [00:59,  1.21it/s]69it [01:00,  1.23it/s]70it [01:01,  1.27it/s]71it [01:01,  1.28it/s]72it [01:02,  1.28it/s]73it [01:03,  1.33it/s]74it [01:04,  1.37it/s]75it [01:04,  1.31it/s]76it [01:05,  1.23it/s]77it [01:06,  1.27it/s]78it [01:07,  1.27it/s]79it [01:08,  1.29it/s]80it [01:09,  1.19it/s]81it [01:12,  1.49s/it]82it [01:12,  1.31s/it]83it [01:13,  1.15s/it]84it [01:14,  1.04s/it]85it [01:15,  1.04s/it]86it [01:16,  1.06it/s]87it [01:17,  1.04s/it]88it [01:18,  1.06it/s]89it [01:19,  1.14it/s]90it [01:19,  1.12it/s]91it [01:20,  1.08it/s]92it [01:21,  1.09it/s]93it [01:22,  1.10it/s]94it [01:23,  1.09it/s]95it [01:24,  1.13it/s]96it [01:25,  1.20it/s]97it [01:25,  1.26it/s]98it [01:26,  1.29it/s]99it [01:27,  1.19it/s]100it [01:28,  1.25it/s]101it [01:29,  1.04s/it]102it [01:30,  1.03it/s]103it [01:31,  1.10it/s]104it [01:32,  1.16it/s]105it [01:33,  1.16it/s]106it [01:34,  1.00it/s]107it [01:35,  1.00it/s]108it [01:36,  1.04it/s]109it [01:37,  1.08it/s]110it [01:37,  1.13it/s]111it [01:38,  1.15it/s]112it [01:39,  1.15it/s]113it [01:40,  1.21it/s]114it [01:42,  1.16s/it]115it [01:43,  1.04s/it]116it [01:43,  1.05it/s]117it [01:45,  1.11s/it]118it [01:46,  1.09s/it]119it [01:47,  1.02it/s]120it [01:47,  1.09it/s]121it [01:48,  1.16it/s]122it [01:49,  1.24it/s]123it [01:50,  1.24it/s]124it [01:50,  1.24it/s]125it [01:51,  1.25it/s]126it [01:52,  1.11it/s]127it [01:53,  1.16it/s]128it [01:54,  1.17it/s]129it [01:55,  1.15it/s]130it [01:56,  1.20it/s]131it [01:56,  1.18it/s]132it [01:57,  1.13it/s]133it [01:58,  1.11it/s]134it [02:00,  1.09s/it]135it [02:01,  1.03s/it]136it [02:02,  1.01it/s]137it [02:03,  1.03it/s]138it [02:03,  1.08it/s]139it [02:04,  1.08it/s]140it [02:05,  1.15it/s]141it [02:06,  1.19it/s]142it [02:07,  1.11it/s]143it [02:08,  1.03it/s]144it [02:09,  1.09it/s]145it [02:10,  1.09it/s]146it [02:11,  1.02s/it]147it [02:12,  1.00s/it]148it [02:13,  1.11it/s]149it [02:13,  1.18it/s]150it [02:15,  1.04s/it]151it [02:16,  1.07s/it]152it [02:17,  1.02it/s]153it [02:18,  1.02it/s]154it [02:19,  1.09s/it]155it [02:20,  1.04s/it]156it [02:21,  1.10s/it]157it [02:22,  1.01s/it]158it [02:23,  1.03it/s]159it [02:24,  1.00s/it]160it [02:25,  1.02s/it]161it [02:26,  1.05it/s]162it [02:27,  1.02it/s]163it [02:28,  1.05it/s]164it [02:29,  1.03it/s]165it [02:30,  1.10it/s]166it [02:30,  1.13it/s]167it [02:32,  1.15s/it]168it [02:33,  1.10s/it]169it [02:34,  1.05it/s]170it [02:35,  1.06it/s]171it [02:36,  1.01it/s]172it [02:37,  1.07it/s]173it [02:37,  1.08it/s]174it [02:38,  1.06it/s]175it [02:39,  1.10it/s]176it [02:41,  1.04s/it]177it [02:41,  1.02it/s]178it [02:42,  1.05it/s]179it [02:44,  1.04s/it]180it [02:44,  1.06it/s]181it [02:45,  1.14it/s]182it [02:46,  1.16it/s]183it [02:47,  1.22it/s]184it [02:47,  1.26it/s]185it [02:48,  1.25it/s]186it [02:49,  1.25it/s]187it [02:50,  1.24it/s]188it [02:50,  1.29it/s]189it [02:51,  1.26it/s]190it [02:52,  1.25it/s]191it [02:53,  1.25it/s]192it [02:54,  1.25it/s]193it [02:55,  1.24it/s]194it [02:55,  1.24it/s]195it [02:56,  1.21it/s]196it [02:57,  1.20it/s]197it [02:58,  1.24it/s]198it [02:59,  1.24it/s]199it [02:59,  1.28it/s]200it [03:00,  1.28it/s]201it [03:01,  1.23it/s]202it [03:02,  1.22it/s]203it [03:03,  1.18it/s]203it [03:03,  1.11it/s]
Number of selected candidates = 177
---> Each Classifier' shapes
	 GT_classifier = 196
	 ViLang_guessed = 177
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:02<12:31,  2.99s/it]  2%|▏         | 4/252 [00:03<02:29,  1.66it/s]  3%|▎         | 8/252 [00:03<01:01,  3.94it/s]  5%|▍         | 12/252 [00:03<00:35,  6.70it/s]  6%|▋         | 16/252 [00:03<00:23,  9.90it/s]  8%|▊         | 20/252 [00:03<00:18, 12.29it/s] 10%|▉         | 24/252 [00:03<00:14, 15.73it/s] 11%|█         | 28/252 [00:03<00:11, 18.99it/s] 13%|█▎        | 32/252 [00:04<00:09, 22.05it/s] 14%|█▍        | 36/252 [00:04<00:10, 20.69it/s] 16%|█▌        | 40/252 [00:04<00:08, 23.65it/s] 17%|█▋        | 44/252 [00:04<00:08, 25.67it/s] 19%|█▉        | 48/252 [00:04<00:07, 27.40it/s] 21%|██        | 52/252 [00:04<00:06, 28.71it/s] 22%|██▏       | 56/252 [00:04<00:06, 30.03it/s] 24%|██▍       | 60/252 [00:04<00:06, 30.98it/s] 25%|██▌       | 64/252 [00:05<00:05, 32.02it/s] 27%|██▋       | 68/252 [00:05<00:05, 32.59it/s] 29%|██▊       | 72/252 [00:05<00:06, 26.43it/s] 30%|███       | 76/252 [00:05<00:06, 27.51it/s] 32%|███▏      | 80/252 [00:05<00:05, 29.19it/s] 33%|███▎      | 84/252 [00:05<00:05, 30.72it/s] 35%|███▍      | 88/252 [00:05<00:06, 26.52it/s] 37%|███▋      | 92/252 [00:06<00:05, 28.23it/s] 38%|███▊      | 96/252 [00:06<00:05, 29.73it/s] 40%|███▉      | 100/252 [00:06<00:05, 28.13it/s] 41%|████▏     | 104/252 [00:06<00:05, 29.51it/s] 43%|████▎     | 108/252 [00:06<00:04, 30.21it/s] 44%|████▍     | 112/252 [00:06<00:04, 31.29it/s] 46%|████▌     | 116/252 [00:06<00:04, 32.19it/s] 48%|████▊     | 120/252 [00:07<00:05, 24.69it/s] 49%|████▉     | 124/252 [00:07<00:04, 26.91it/s] 51%|█████     | 128/252 [00:07<00:04, 28.39it/s] 52%|█████▏    | 132/252 [00:07<00:04, 29.80it/s] 54%|█████▍    | 136/252 [00:07<00:04, 25.53it/s] 56%|█████▌    | 140/252 [00:07<00:04, 27.37it/s] 57%|█████▋    | 143/252 [00:07<00:03, 27.61it/s] 58%|█████▊    | 147/252 [00:08<00:03, 29.01it/s] 60%|█████▉    | 151/252 [00:08<00:03, 30.56it/s] 62%|██████▏   | 155/252 [00:08<00:03, 31.45it/s] 63%|██████▎   | 159/252 [00:08<00:02, 32.00it/s] 65%|██████▍   | 163/252 [00:08<00:02, 32.25it/s] 66%|██████▋   | 167/252 [00:08<00:02, 32.81it/s] 68%|██████▊   | 171/252 [00:08<00:02, 33.05it/s] 69%|██████▉   | 175/252 [00:08<00:02, 33.38it/s] 71%|███████   | 179/252 [00:08<00:02, 33.58it/s] 73%|███████▎  | 183/252 [00:09<00:02, 32.05it/s] 74%|███████▍  | 187/252 [00:09<00:01, 32.74it/s] 76%|███████▌  | 191/252 [00:09<00:01, 33.07it/s] 77%|███████▋  | 195/252 [00:09<00:01, 33.30it/s] 79%|███████▉  | 199/252 [00:09<00:01, 32.69it/s] 81%|████████  | 203/252 [00:09<00:01, 32.56it/s] 82%|████████▏ | 207/252 [00:09<00:01, 32.97it/s] 84%|████████▎ | 211/252 [00:09<00:01, 33.56it/s] 85%|████████▌ | 215/252 [00:10<00:01, 33.67it/s] 87%|████████▋ | 219/252 [00:10<00:00, 33.41it/s] 88%|████████▊ | 223/252 [00:10<00:00, 33.40it/s] 90%|█████████ | 227/252 [00:10<00:00, 33.88it/s] 92%|█████████▏| 231/252 [00:10<00:00, 33.74it/s] 93%|█████████▎| 235/252 [00:10<00:00, 34.23it/s] 95%|█████████▍| 239/252 [00:10<00:00, 34.57it/s] 96%|█████████▋| 243/252 [00:10<00:00, 34.87it/s] 98%|█████████▊| 247/252 [00:10<00:00, 34.78it/s]100%|█████████▉| 251/252 [00:11<00:00, 34.99it/s]100%|██████████| 252/252 [00:11<00:00, 22.36it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6631431579589844
---> Evaluating
  0%|          | 0/252 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  0%|          | 1/252 [00:02<12:04,  2.89s/it]  1%|          | 3/252 [00:02<03:16,  1.27it/s]  3%|▎         | 7/252 [00:03<01:06,  3.66it/s]  4%|▍         | 10/252 [00:03<00:44,  5.49it/s]  6%|▌         | 14/252 [00:03<00:27,  8.78it/s]  7%|▋         | 18/252 [00:03<00:19, 12.28it/s]  9%|▊         | 22/252 [00:03<00:14, 15.94it/s] 10%|█         | 26/252 [00:03<00:11, 19.35it/s] 12%|█▏        | 30/252 [00:03<00:09, 22.60it/s] 13%|█▎        | 34/252 [00:03<00:08, 25.41it/s] 15%|█▌        | 38/252 [00:04<00:07, 27.45it/s] 17%|█▋        | 42/252 [00:04<00:07, 29.35it/s] 18%|█▊        | 46/252 [00:04<00:06, 29.87it/s] 20%|█▉        | 50/252 [00:04<00:06, 31.12it/s] 21%|██▏       | 54/252 [00:04<00:07, 24.87it/s] 23%|██▎       | 58/252 [00:04<00:07, 26.78it/s] 25%|██▍       | 62/252 [00:04<00:06, 28.60it/s] 26%|██▌       | 66/252 [00:05<00:06, 30.28it/s] 28%|██▊       | 70/252 [00:05<00:05, 31.20it/s] 29%|██▉       | 74/252 [00:05<00:05, 31.21it/s] 31%|███       | 78/252 [00:05<00:05, 31.74it/s] 33%|███▎      | 82/252 [00:05<00:05, 32.26it/s] 34%|███▍      | 86/252 [00:05<00:05, 32.26it/s] 36%|███▌      | 90/252 [00:05<00:05, 32.18it/s] 37%|███▋      | 94/252 [00:05<00:04, 32.06it/s] 39%|███▉      | 98/252 [00:06<00:04, 32.86it/s] 40%|████      | 102/252 [00:06<00:04, 33.44it/s] 42%|████▏     | 106/252 [00:06<00:04, 33.25it/s] 44%|████▎     | 110/252 [00:06<00:04, 33.26it/s] 45%|████▌     | 114/252 [00:06<00:04, 33.29it/s] 47%|████▋     | 118/252 [00:06<00:04, 33.49it/s] 48%|████▊     | 122/252 [00:06<00:03, 33.28it/s] 50%|█████     | 126/252 [00:06<00:03, 33.44it/s] 52%|█████▏    | 130/252 [00:06<00:03, 33.36it/s] 53%|█████▎    | 134/252 [00:07<00:04, 27.73it/s] 55%|█████▍    | 138/252 [00:07<00:03, 29.16it/s] 56%|█████▋    | 142/252 [00:07<00:03, 30.38it/s] 58%|█████▊    | 146/252 [00:07<00:03, 30.94it/s] 60%|█████▉    | 150/252 [00:07<00:03, 31.54it/s] 61%|██████    | 154/252 [00:07<00:03, 32.27it/s] 63%|██████▎   | 158/252 [00:07<00:02, 32.84it/s] 64%|██████▍   | 162/252 [00:08<00:02, 33.12it/s] 66%|██████▌   | 166/252 [00:08<00:02, 33.42it/s] 67%|██████▋   | 170/252 [00:08<00:02, 33.39it/s] 69%|██████▉   | 174/252 [00:08<00:02, 33.12it/s] 71%|███████   | 178/252 [00:08<00:02, 32.91it/s] 72%|███████▏  | 182/252 [00:08<00:02, 33.24it/s] 74%|███████▍  | 186/252 [00:08<00:01, 33.61it/s] 75%|███████▌  | 190/252 [00:08<00:01, 33.89it/s] 77%|███████▋  | 194/252 [00:08<00:01, 34.17it/s] 79%|███████▊  | 198/252 [00:09<00:01, 33.76it/s] 80%|████████  | 202/252 [00:09<00:01, 34.12it/s] 82%|████████▏ | 206/252 [00:09<00:01, 33.33it/s] 83%|████████▎ | 210/252 [00:09<00:01, 33.15it/s] 85%|████████▍ | 214/252 [00:09<00:01, 33.55it/s] 87%|████████▋ | 218/252 [00:09<00:01, 33.77it/s] 88%|████████▊ | 222/252 [00:09<00:00, 34.04it/s] 90%|████████▉ | 226/252 [00:09<00:00, 34.13it/s] 91%|█████████▏| 230/252 [00:10<00:00, 34.37it/s] 93%|█████████▎| 234/252 [00:10<00:00, 34.53it/s] 94%|█████████▍| 238/252 [00:10<00:00, 34.58it/s] 96%|█████████▌| 242/252 [00:10<00:00, 34.68it/s] 98%|█████████▊| 246/252 [00:10<00:00, 34.91it/s] 99%|█████████▉| 250/252 [00:10<00:00, 35.06it/s]100%|██████████| 252/252 [00:10<00:00, 22.95it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([8041, 768])
gt_feats.shape torch.Size([8041, 768])
Semantic similarity score = 0.6276611685752869


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 63.16378559880612
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 80.95202793343623
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 47.87494235507517


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 66.31431579589844
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 49.794801641586865
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 76.0071619520328
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 36.17769605662848


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 62.766117095947266
=========================          END          =========================


========================= ViLang Final Results of 10 runs, w/ random imgs per class=========================


[Clustering]
Clustering ACC: 49.72267130953861
Semantic ACC:   62.56841278076172
=========================          END          =========================
