Namespace(config_file_env='./configs/env_machine.yml', config_file_expt='./configs/expts/bird200_all.yml', alpha=0.7, N_tta=10, num_per_category='random', num_runs=10)
Number of GPUs: 1
Device ID: 0 Device Name: NVIDIA RTX A6000
['White-winged Crossbill', 'Green-winged Teal', 'Little Blue Heron', 'Horned Puffin', 'Green-backed Kingfisher', 'Common Pochard (Aythya ferina)', 'Common grackle', 'Common Blackbird (Turdus merula)', 'Brown-headed Cowbird', 'Red-winged blackbird', 'Pomarine Skua', 'Black-bellied Whistling-Duck', 'Blackburnian Warbler', 'Eurasian Wigeon', 'Blue Jay', 'Pied-billed Grebe', 'Sora (Porzana carolina)', 'Pine warbler', 'Black-billed Magpie', 'Ruby-crowned Kinglet', 'Brown Pelican', 'Red-naped Sapsucker', 'House Finch', 'Mallard Duck', 'Rock Sandpiper', 'Black-headed Grosbeak', 'Pica hudsonia', 'House Wren', 'Black-throated warbler', 'Pied flycatcher', 'Boat-tailed Grackle (Quiscalus major)', 'Snowy Egret', 'Black Oystercatcher', 'Willow Flycatcher', "St. Paul's Rock Wren", 'Golden-crowned Kinglet', 'Sandpiper', 'Prothonotary Warbler', 'Carolina Chickadee', 'Black-throated Green Warbler', 'Black-billed Magpie (Pica hudsonia)', 'Red-breasted Merganser', 'Gray Kingbird', 'Rock Dove', 'Slender-billed Gull', 'Blue-crowned Conure', 'Spruce Grouse', 'House Crow', 'Black Guillemot', 'Red-and-black Flycatcher', 'Heron', 'Red Poll', 'Canada Warbler', 'Northwestern Crow', 'Slender-billed Wren', 'Blackbird', 'Rock Sparrow', 'Red Crested Finch', 'Pyrrhuloxia (Cardinalis sinuatus)', 'Pigeon Guillemot', 'Eastern Meadowlark (Sturnella magna)', 'Green-cheeked Amazon', 'American Wigeon', 'American Goldfinch (Spinus tristis)', 'Surf Scoter', 'White-throated Sparrow', 'Green Jay', 'Brown Wren', 'Red Phalarope', 'Black-and-white Warbling Finch', 'Blue-crowned Parakeet', 'Ruddy Turnstone', 'Great Black-backed Gull', 'Mountain Bluebird', 'Tree Wren', 'Sparkling Violetear', 'Black-footed Albatross (Phoebastria nigripes)', 'Belted Kingfisher', 'Common Goldeneye', 'Green-cheeked Amazon Parrot', 'Black-throated Blue Warbler', 'South Polar Skua', 'Pileated Woodpecker', 'Common Grackle', 'Pine Warbler', 'Black-capped Chickadee', 'Black-necked Stilt', 'Fox Sparrow', 'Bobolink', 'Yellow-breasted Bunting', 'Common Kingfisher', 'Raven', 'Brown-striped Snowbird', 'Western Bluebird', 'Black-throated sparrow', "Wilson's Warbler", 'American White Pelican', 'White-headed Blackbird', 'Summer Tanager', 'Sandhill Crane', 'Grassland Sparrow', 'Adélie Penguin', 'Harlequin Duck', 'Golden-winged Warbler (Vermivora chrysoptera)', 'Green Heron', 'Eastern Kingbird', 'Pied Wagtail (Motacilla alba)', 'Kingfisher', 'Pied Kingfisher (Ceryle rudis)', 'Black-and-white Flycatcher', 'Western tanager', 'Yellow-breasted Flycatcher', 'Mountain Chickadee', 'Warbler', 'Eurasian Tree Sparrow', 'Northwestern Crow (Corvus caurinus)', 'Great Black-backed Gull (Larus marinus)', 'Pica pica', 'Northern Cardinal (Cardinalis cardinalis)', 'Spotted Sandpiper', 'Yellow-bellied Flycatcher', 'Red-necked Phalarope', 'Pacific Ocean bird', 'Semipalmated Sandpiper', 'Desert Sparrow', 'Herring Gull', 'Savannah Sparrow', 'Virginia Rail (Rallus limicola)', 'Rockhopper Penguin', 'Light-mantled Albatross', 'Great White Pelican', 'Scarlet Tanager (Piranga olivacea)', 'Red-bellied Woodpecker', 'Red-winged Blackbird', 'Black Swan', 'Black Tern', 'Little Egret', 'Ring-billed Gull', 'Palm Warbler', 'Magellanic Oystercatcher', 'Pyrrhuloxia', 'Yellow-rumped Warbler', 'Elegant Tern', 'Black Phoebe', 'Pied Avocet', 'Pelican', 'Sanderling (Calidris alba)', 'Tufted Duck', 'Brown-headed Nuthatch', 'Rufous Hummingbird', 'Green honeycreeper', 'Great Skua', 'Downy flippy', 'Black-footed Albatross', 'Arctic Tern', 'Herring Gull (Larus argentatus)', 'Common Loon', 'Oystercatcher', 'Splotchy-breasted Sparrow', 'Fish Crow (Corvus ossifragus)', 'Mallard (Anas platyrhynchos)', 'Northern Pintail (Anas acuta)', 'Canyon Wren', 'Bluebird', 'Rose-breasted Grosbeak', 'Laughing Gull', 'Red-necked Grebe', 'Gentoo Penguin', "Wilson's Warbler (Cardellina pusilla)", 'Green-backed Firecrown', 'Great Blue Heron', 'Yellow-bellied Warbler', 'Cyanocorax species', 'Eastern Meadowlark', 'Tufted Titmouse', 'American Avocet', 'Eastern Bluebird', 'Cackling Goose', 'Mallard', 'Red-headed Woodpecker', 'European Pied Flycatcher', 'Green Honeycreeper', "Anna's Hummingbird", 'Pied Flycatcher', 'California Gull', 'Green Warbler', 'Common Chiffchaff', 'Blackpoll Warbler', 'Green-backed Honeyeater', 'Swallow', 'Neotropic Cormorant', 'Carolina Wren', 'European Starling', 'Blue-winged Warbler', 'Northern Mockingbird', 'European Shag', 'Common Kingfisher (Alcedo atthis)', 'American Wigeon (Anas americana)', 'Least Tern', 'Black and White Warbler', 'Cardinal', 'Lesser Goldfinch', 'Prairie Warbler', 'Francolinus francolinus', 'Green-and-white Hummingbird', 'Red-crowned Finch', 'Mountain Bird', 'Brown-crowned Hummingbird', 'Green-backed Flycatcher', 'Black-throated Gray Warbler', 'Green-backed Goldfinch', 'European Robin', 'Australian Pelican', 'White-winged Tern', 'Yellow-billed Magpie', 'Summer tanager', 'Pink Robin', 'Gray-headed Woodpecker', 'Carrion Crow', 'Cedar Waxwing', 'Great Frigatebird', 'Pomarine Jaeger', 'Green-backed Tit', 'Mandarin Duck', 'Crested Finch', 'Eastern Black and White Warbler', 'Black-bellied Plover', 'Common Goldeneye (Bucephala clangula)', 'Laysan Albatross', 'Scarlet Tanager', 'Pied Kingfisher', 'Vermilion Cardinal', 'Black-crowned Night-Heron', 'Blue-headed Parrot', 'White-bellied Forestbird', 'Green-cheeked Parakeet', 'Wandering Albatross', 'Red-throated Pipit', 'Crow', 'Gadwall (Mareca strepera)', 'Common Raven', 'Yellow-throated Vireo (Vireo flavifrons)', 'Spotted Towhee', 'White-crowned Sparrow (Zonotrichia leucophrys)', 'Black-and-white warbler', 'House Sparrow', 'Black-backed Woodpecker', 'Swan', 'Gray Catbird', 'Indigo Bunting', 'Cyanocitta cristata', 'Black-bellied Plover (Pluvialis squatarola)', 'Brown-headed Sandpiper', 'Tree Swallow', 'Black-necked Grebe', 'White-breasted Nuthatch', 'American Coot', "Bonaparte's Gull", 'Green-headed Tanager (Tangara seledon)', 'Skua', 'Surfbird', 'Cormorant', 'Green-winged Macaw', 'Black-crowned Night Heron', 'Ruby-throated Hummingbird', 'Black-headed Chickadee', 'Fish Crow', 'Lapland Longspur', 'Rusty Blackbird', 'South American Skua', 'Northern Gannet', 'Pica nuttalli', 'Marsh Wren', 'Red-headed Blackbird (Agelaius phoeniceus)', 'Sanderling', 'Bald Eagle', 'Blue-headed Warbler', 'Snow Bunting', 'Glaucous Gull', 'African Black Oystercatcher', 'Common Yellowthroat', 'Black-throated Sparrow', 'Grey-headed Albatross', 'Black-necked Swan (Cygnus melancoryphus)', 'Sandwich Tern', 'Downy Woodpecker', 'Red-legged Kittiwake', 'Common Moorhen (Gallinula chloropus)', 'Magellanic Penguin', 'Violet-crowned Woodnymph', 'Brown Thrasher', 'Red-breasted Blackbird', 'Vermilion Flycatcher (Pyrocephalus rubinus)', 'Blue-footed Booby', 'Rosefinch', 'Blue Grosbeak', 'Ivory Gull', 'Northern Cardinal', 'Guillemot', 'American Crow (Corvus brachyrhynchos)', 'Red-billed Blue Magpie', 'Northern Flicker', 'Mallard duck', 'Goldfinch', 'Black-bellied Whistling-Duck (Dendrocygna autumnalis)', 'Scarlet tanager', 'Wood Duck', 'Blue-crowned Motmot', 'Black-winged Stilt', 'Magpie', 'Seagull', 'Black-and-white Mannikin', 'Curve-billed Thrasher', 'Black Skimmer (Rynchops niger)', 'Black-and-white Munia', 'Royal Tern', 'Yellow Warbler', 'Tufted Puffin', 'Brown Creeper', 'Black-necked Stork', 'Brown-cheeked Flycatcher', 'Orange-bellied Hummingbird', 'Barred Sparrow', 'Collared Kingfisher (Todiramphus chloris)', 'Western Meadowlark', 'American Goldfinch', 'Black and white warbler', 'Yellow-headed Blackbird', 'American Yellow Warbler', 'Slender-billed Wingtail', 'Florida Scrub-jay', 'Common Crow', 'Atlantic Puffin', 'Olive-backed sunbird', 'Black Scoter', 'White-headed Duck (Oxyura leucocephala)', 'Green-breasted mango', 'Black-throated green warbler', 'American Oystercatcher', 'Red-breasted Nuthatch', 'Tree Sparrow', 'Black-necked Crane', 'Double-crested Cormorant', 'Green-cheeked warbler', 'Vermilion Cardinal (Cardinalis phoeniceus)', "Costa's Hummingbird", 'Albatross', 'Red-winged Blackbird (Agelaius phoeniceus)', 'Great Egret', "Forster's Tern", 'Eastern Wood-Pewee', 'Stygia albacore', 'Bufflehead', 'American Kestrel', 'Common Raven (Corvus corax)', 'White-winged Black-Tyrant', 'Blue-crowned Manakin', 'Black-and-white Warbler', 'Black-chinned Hummingbird', 'Yellow Warbler (Setophaga petechia)', 'Green Kingfisher', 'Black-tailed Gull', 'Green jay', 'Black Turnstone', 'Common Crow (Corvus brachyrhynchos)', 'Osprey', 'Green-winged warbler', 'Black-and-White Warbler', 'Black-capped francolin', 'Orange-crowned Warbler', 'Mew Gull', 'Song Sparrow', 'Painted Bunting', 'Tufted duck', 'Hawk', 'Pine Siskin', 'Black-necked Swan', 'Yellow-throated Warbler', 'Brown-crested Flycatcher', 'Common Teal', 'Western Gull', 'Antarctic Skua', 'Chipping Sparrow', 'White-crowned Sparrow', 'Skua, Duck, Sandpiper', 'Common Eider', 'Green-breasted Mango', 'Eastern Phoebe', 'Nashville Warbler', 'Black-capped chickadee', 'Black-necked Stilt (Himantopus mexicanus)', 'European Starling (Sturnus vulgaris)', 'Mute Swan', 'Blue-gray Gnatcatcher', 'Baltimore Oriole', 'Striped-breasted Hummingbird', 'Northern Shoveler (Spatula clypeata)', 'Gray-cheeked Thrush', 'Black-legged Kittiwake', 'American Robin', 'Willow Warbler', 'Swan Goose', 'Brown Skua', 'Dark-eyed Junco', 'Belted Kingfisher (Megaceryle alcyon)', 'Green-crowned Brilliant', 'Northern Shoveler', 'Red Crossbill', 'Black Skimmer', 'Rose-breasted Grosbeak (Pheucticus ludovicianus)', 'White-necked Swan', 'Pied Wagtail', 'Gray-headed Chickadee', 'Brown-capped Pygmy Woodpecker', 'Sparrow', 'American Crow', "Bewick's Wren", 'Meadowlark', 'Hairy Woodpecker', 'Black-headed Gull', 'European Goldfinch', 'Green-backed Sparrow', 'Common Tern', 'Pied Oystercatcher', 'Pink-headed Warbler', 'Rock Pigeon', 'Duck', 'Great Cormorant', 'Saguaro Cactus Wren', 'Black-crowned Sparrow', 'Gadwall', 'Green-throated Mountaingem', 'Vermilion Flycatcher', 'Black-browed Albatross', 'Yellow-breasted chat', 'Western Gull (Larus occidentalis)', 'Northern Wheatear']
0it [00:00, ?it/s]1it [00:02,  2.44s/it]2it [00:03,  1.44s/it]3it [00:03,  1.02s/it]4it [00:04,  1.06it/s]5it [00:05,  1.06it/s]6it [00:06,  1.10it/s]7it [00:07,  1.04s/it]8it [00:08,  1.15s/it]9it [00:10,  1.17s/it]10it [00:11,  1.21s/it]11it [00:12,  1.24s/it]12it [00:14,  1.31s/it]13it [00:16,  1.56s/it]14it [00:18,  1.61s/it]15it [00:19,  1.63s/it]16it [00:21,  1.69s/it]17it [00:24,  1.96s/it]18it [00:26,  2.12s/it]19it [00:28,  2.00s/it]20it [00:30,  1.97s/it]21it [00:32,  2.00s/it]22it [00:34,  2.06s/it]23it [00:36,  2.14s/it]24it [00:40,  2.44s/it]25it [00:42,  2.46s/it]26it [00:44,  2.36s/it]27it [00:47,  2.40s/it]28it [00:49,  2.42s/it]29it [00:52,  2.48s/it]30it [00:55,  2.57s/it]31it [00:57,  2.59s/it]32it [01:00,  2.59s/it]33it [01:02,  2.61s/it]34it [01:05,  2.69s/it]35it [01:08,  2.58s/it]36it [01:10,  2.60s/it]37it [01:13,  2.64s/it]38it [01:15,  2.57s/it]39it [01:18,  2.59s/it]40it [01:21,  2.72s/it]41it [01:24,  2.70s/it]42it [01:27,  2.77s/it]43it [01:30,  2.80s/it]44it [01:32,  2.79s/it]45it [01:35,  2.88s/it]46it [01:38,  2.80s/it]47it [01:41,  2.84s/it]48it [01:44,  2.83s/it]49it [01:47,  2.96s/it]50it [01:50,  2.93s/it]51it [01:52,  2.73s/it]52it [01:55,  2.78s/it]53it [01:58,  2.86s/it]54it [02:01,  2.76s/it]55it [02:03,  2.71s/it]56it [02:06,  2.69s/it]57it [02:08,  2.66s/it]58it [02:11,  2.61s/it]59it [02:13,  2.55s/it]60it [02:16,  2.59s/it]61it [02:18,  2.51s/it]62it [02:21,  2.45s/it]63it [02:23,  2.47s/it]64it [02:26,  2.50s/it]65it [02:28,  2.47s/it]66it [02:31,  2.48s/it]67it [02:34,  2.59s/it]68it [02:36,  2.60s/it]69it [02:39,  2.61s/it]70it [02:41,  2.62s/it]71it [02:44,  2.66s/it]72it [02:47,  2.73s/it]73it [02:50,  2.71s/it]74it [02:52,  2.72s/it]75it [02:55,  2.57s/it]76it [02:57,  2.58s/it]77it [03:00,  2.68s/it]78it [03:03,  2.67s/it]79it [03:05,  2.65s/it]80it [03:08,  2.63s/it]81it [03:11,  2.60s/it]82it [03:11,  1.97s/it]83it [03:13,  1.90s/it]84it [03:14,  1.82s/it]85it [03:16,  1.71s/it]86it [03:17,  1.68s/it]87it [03:19,  1.67s/it]88it [03:21,  1.60s/it]89it [03:22,  1.58s/it]90it [03:24,  1.65s/it]91it [03:26,  1.64s/it]92it [03:27,  1.62s/it]93it [03:29,  1.68s/it]94it [03:31,  1.71s/it]95it [03:32,  1.60s/it]96it [03:34,  1.60s/it]97it [03:35,  1.61s/it]98it [03:37,  1.50s/it]99it [03:38,  1.52s/it]100it [03:40,  1.56s/it]101it [03:41,  1.53s/it]102it [03:43,  1.50s/it]103it [03:44,  1.44s/it]104it [03:45,  1.39s/it]105it [03:47,  1.52s/it]106it [03:48,  1.47s/it]107it [03:50,  1.43s/it]108it [03:51,  1.43s/it]109it [03:53,  1.44s/it]110it [03:54,  1.43s/it]111it [03:55,  1.42s/it]112it [03:57,  1.41s/it]113it [03:58,  1.47s/it]114it [04:00,  1.51s/it]115it [04:02,  1.55s/it]116it [04:03,  1.54s/it]117it [04:04,  1.44s/it]118it [04:06,  1.45s/it]119it [04:07,  1.49s/it]120it [04:09,  1.44s/it]121it [04:10,  1.42s/it]122it [04:12,  1.46s/it]123it [04:13,  1.49s/it]124it [04:15,  1.53s/it]125it [04:16,  1.50s/it]126it [04:17,  1.40s/it]127it [04:19,  1.36s/it]128it [04:20,  1.36s/it]129it [04:21,  1.31s/it]130it [04:23,  1.38s/it]131it [04:24,  1.41s/it]132it [04:26,  1.40s/it]133it [04:27,  1.38s/it]134it [04:28,  1.36s/it]135it [04:30,  1.40s/it]136it [04:31,  1.23s/it]137it [04:32,  1.11s/it]138it [04:32,  1.07it/s]139it [04:33,  1.12it/s]140it [04:34,  1.04it/s]141it [04:35,  1.06it/s]142it [04:36,  1.04s/it]143it [04:37,  1.08s/it]144it [04:39,  1.19s/it]145it [04:40,  1.21s/it]146it [04:41,  1.20s/it]147it [04:42,  1.22s/it]148it [04:44,  1.26s/it]149it [04:45,  1.27s/it]150it [04:46,  1.31s/it]151it [04:48,  1.25s/it]152it [04:49,  1.27s/it]153it [04:50,  1.34s/it]154it [04:52,  1.30s/it]155it [04:53,  1.37s/it]156it [04:55,  1.40s/it]157it [04:56,  1.43s/it]158it [04:57,  1.32s/it]159it [04:58,  1.23s/it]160it [04:59,  1.11s/it]161it [05:00,  1.06s/it]162it [05:01,  1.01it/s]163it [05:02,  1.01it/s]164it [05:03,  1.01it/s]165it [05:03,  1.20it/s]166it [05:04,  1.19it/s]167it [05:05,  1.39it/s]168it [05:05,  1.35it/s]169it [05:06,  1.56it/s]170it [05:07,  1.45it/s]171it [05:07,  1.85it/s]172it [05:07,  2.34it/s]173it [05:07,  2.55it/s]174it [05:08,  2.09it/s]175it [05:08,  2.27it/s]176it [05:09,  2.50it/s]177it [05:10,  1.62it/s]178it [05:11,  1.06it/s]179it [05:13,  1.13s/it]180it [05:14,  1.06s/it]181it [05:15,  1.03it/s]182it [05:15,  1.08it/s]183it [05:16,  1.27it/s]184it [05:17,  1.15it/s]185it [05:17,  1.46it/s]186it [05:18,  1.35it/s]187it [05:19,  1.26it/s]188it [05:20,  1.00s/it]189it [05:22,  1.18s/it]190it [05:23,  1.13s/it]191it [05:24,  1.17s/it]192it [05:26,  1.25s/it]193it [05:27,  1.29s/it]194it [05:28,  1.19s/it]195it [05:29,  1.12s/it]196it [05:30,  1.00s/it]197it [05:31,  1.03it/s]198it [05:32,  1.00it/s]199it [05:33,  1.00s/it]200it [05:34,  1.03it/s]201it [05:35,  1.03it/s]202it [05:36,  1.01s/it]203it [05:37,  1.07it/s]204it [05:38,  1.03it/s]205it [05:38,  1.08it/s]206it [05:40,  1.10s/it]207it [05:42,  1.33s/it]208it [05:43,  1.33s/it]209it [05:44,  1.32s/it]210it [05:46,  1.33s/it]211it [05:47,  1.28s/it]212it [05:48,  1.31s/it]213it [05:50,  1.29s/it]213it [05:50,  1.64s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]  4%|▍         | 1/23 [00:19<07:02, 19.19s/it]  9%|▊         | 2/23 [00:19<02:53,  8.27s/it] 13%|█▎        | 3/23 [00:22<01:55,  5.77s/it] 17%|█▋        | 4/23 [00:22<01:07,  3.57s/it] 22%|██▏       | 5/23 [00:23<00:42,  2.36s/it] 26%|██▌       | 6/23 [00:23<00:27,  1.63s/it] 30%|███       | 7/23 [00:23<00:18,  1.16s/it] 35%|███▍      | 8/23 [00:23<00:12,  1.17it/s] 39%|███▉      | 9/23 [00:23<00:09,  1.53it/s] 43%|████▎     | 10/23 [00:24<00:06,  1.94it/s] 48%|████▊     | 11/23 [00:24<00:05,  2.35it/s] 52%|█████▏    | 12/23 [00:24<00:03,  2.76it/s] 57%|█████▋    | 13/23 [00:24<00:03,  3.18it/s] 61%|██████    | 14/23 [00:24<00:02,  3.56it/s] 65%|██████▌   | 15/23 [00:25<00:02,  3.85it/s] 70%|██████▉   | 16/23 [00:25<00:01,  4.10it/s] 74%|███████▍  | 17/23 [00:32<00:14,  2.38s/it] 78%|███████▊  | 18/23 [00:33<00:10,  2.04s/it] 83%|████████▎ | 19/23 [00:37<00:09,  2.38s/it] 87%|████████▋ | 20/23 [00:37<00:05,  1.73s/it] 91%|█████████▏| 21/23 [00:37<00:02,  1.27s/it] 96%|█████████▌| 22/23 [00:37<00:00,  1.05it/s]100%|██████████| 23/23 [00:37<00:00,  1.38it/s]100%|██████████| 23/23 [00:38<00:00,  1.65s/it]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:03<01:07,  3.06s/it]  9%|▊         | 2/23 [00:03<00:28,  1.38s/it] 13%|█▎        | 3/23 [00:03<00:18,  1.06it/s] 17%|█▋        | 4/23 [00:03<00:12,  1.53it/s] 22%|██▏       | 5/23 [00:04<00:08,  2.04it/s] 26%|██▌       | 6/23 [00:04<00:06,  2.54it/s] 30%|███       | 7/23 [00:04<00:05,  3.01it/s] 35%|███▍      | 8/23 [00:04<00:04,  3.42it/s] 39%|███▉      | 9/23 [00:04<00:03,  3.76it/s] 43%|████▎     | 10/23 [00:05<00:03,  4.03it/s] 48%|████▊     | 11/23 [00:05<00:02,  4.24it/s] 52%|█████▏    | 12/23 [00:05<00:02,  4.42it/s] 57%|█████▋    | 13/23 [00:05<00:02,  4.54it/s] 61%|██████    | 14/23 [00:05<00:01,  4.60it/s] 65%|██████▌   | 15/23 [00:06<00:01,  4.67it/s] 70%|██████▉   | 16/23 [00:06<00:01,  4.73it/s] 74%|███████▍  | 17/23 [00:06<00:01,  4.76it/s] 78%|███████▊  | 18/23 [00:06<00:01,  4.79it/s] 83%|████████▎ | 19/23 [00:06<00:00,  4.81it/s] 87%|████████▋ | 20/23 [00:07<00:00,  4.81it/s] 91%|█████████▏| 21/23 [00:07<00:00,  4.84it/s] 96%|█████████▌| 22/23 [00:07<00:00,  4.85it/s]100%|██████████| 23/23 [00:07<00:00,  5.46it/s]100%|██████████| 23/23 [00:07<00:00,  2.93it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6499395966529846


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.666896789782534
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.52429412752059
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.157563807514315


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 64.99395751953125
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.07s/it]2it [00:02,  1.18s/it]3it [00:04,  1.59s/it]4it [00:07,  2.20s/it]5it [00:10,  2.54s/it]6it [00:13,  2.62s/it]7it [00:15,  2.58s/it]8it [00:18,  2.56s/it]9it [00:20,  2.49s/it]10it [00:23,  2.54s/it]11it [00:26,  2.59s/it]12it [00:27,  2.33s/it]13it [00:29,  2.09s/it]14it [00:31,  2.14s/it]15it [00:34,  2.28s/it]16it [00:36,  2.36s/it]17it [00:39,  2.51s/it]18it [00:42,  2.56s/it]19it [00:45,  2.61s/it]20it [00:48,  2.74s/it]21it [00:49,  2.37s/it]22it [00:52,  2.49s/it]23it [00:55,  2.54s/it]24it [00:57,  2.46s/it]25it [01:00,  2.53s/it]26it [01:02,  2.48s/it]27it [01:04,  2.31s/it]28it [01:06,  2.33s/it]29it [01:09,  2.44s/it]30it [01:12,  2.58s/it]31it [01:14,  2.46s/it]32it [01:16,  2.48s/it]33it [01:18,  2.32s/it]34it [01:21,  2.42s/it]35it [01:24,  2.50s/it]36it [01:26,  2.49s/it]37it [01:29,  2.54s/it]38it [01:31,  2.47s/it]39it [01:34,  2.45s/it]40it [01:36,  2.51s/it]41it [01:39,  2.51s/it]42it [01:42,  2.59s/it]43it [01:44,  2.57s/it]44it [01:47,  2.76s/it]45it [01:50,  2.67s/it]46it [01:53,  2.86s/it]47it [01:55,  2.63s/it]48it [01:58,  2.78s/it]49it [02:01,  2.80s/it]50it [02:04,  2.73s/it]51it [02:06,  2.74s/it]52it [02:09,  2.71s/it]53it [02:12,  2.66s/it]54it [02:14,  2.62s/it]55it [02:16,  2.53s/it]56it [02:18,  2.32s/it]57it [02:21,  2.30s/it]58it [02:22,  2.10s/it]59it [02:23,  1.86s/it]60it [02:26,  1.95s/it]61it [02:28,  2.10s/it]62it [02:31,  2.28s/it]63it [02:33,  2.26s/it]64it [02:35,  2.32s/it]65it [02:38,  2.33s/it]66it [02:40,  2.35s/it]67it [02:43,  2.51s/it]68it [02:46,  2.73s/it]69it [02:49,  2.55s/it]70it [02:51,  2.41s/it]71it [02:53,  2.43s/it]72it [02:55,  2.40s/it]73it [02:58,  2.58s/it]74it [03:01,  2.54s/it]75it [03:03,  2.42s/it]76it [03:05,  2.27s/it]77it [03:07,  2.16s/it]78it [03:09,  2.07s/it]79it [03:11,  2.17s/it]80it [03:13,  2.24s/it]81it [03:16,  2.39s/it]82it [03:19,  2.51s/it]83it [03:21,  2.50s/it]84it [03:24,  2.54s/it]85it [03:26,  2.46s/it]86it [03:29,  2.59s/it]87it [03:32,  2.61s/it]88it [03:34,  2.49s/it]89it [03:37,  2.61s/it]90it [03:40,  2.68s/it]91it [03:43,  2.92s/it]92it [03:46,  2.81s/it]93it [03:48,  2.63s/it]94it [03:51,  2.61s/it]95it [03:53,  2.52s/it]96it [03:56,  2.56s/it]97it [03:58,  2.43s/it]98it [04:01,  2.70s/it]99it [04:03,  2.52s/it]100it [04:06,  2.45s/it]101it [04:08,  2.38s/it]102it [04:11,  2.56s/it]103it [04:13,  2.57s/it]104it [04:16,  2.49s/it]105it [04:18,  2.55s/it]106it [04:21,  2.62s/it]107it [04:23,  2.56s/it]108it [04:27,  2.71s/it]109it [04:29,  2.74s/it]110it [04:32,  2.85s/it]111it [04:35,  2.74s/it]112it [04:38,  2.89s/it]113it [04:41,  2.93s/it]114it [04:44,  2.97s/it]115it [04:47,  2.89s/it]116it [04:50,  2.94s/it]117it [04:53,  2.83s/it]118it [04:56,  2.97s/it]119it [04:58,  2.78s/it]120it [05:01,  2.63s/it]121it [05:03,  2.58s/it]122it [05:06,  2.69s/it]123it [05:09,  2.67s/it]124it [05:11,  2.62s/it]125it [05:14,  2.60s/it]126it [05:16,  2.57s/it]127it [05:19,  2.58s/it]128it [05:22,  2.85s/it]129it [05:25,  2.95s/it]130it [05:28,  2.76s/it]131it [05:31,  2.78s/it]132it [05:33,  2.67s/it]133it [05:36,  2.64s/it]134it [05:38,  2.70s/it]135it [05:41,  2.81s/it]136it [05:44,  2.71s/it]137it [05:46,  2.65s/it]138it [05:49,  2.68s/it]139it [05:52,  2.66s/it]140it [05:54,  2.59s/it]141it [05:57,  2.79s/it]142it [06:00,  2.73s/it]143it [06:02,  2.66s/it]144it [06:05,  2.63s/it]145it [06:08,  2.60s/it]146it [06:10,  2.50s/it]147it [06:13,  2.55s/it]148it [06:15,  2.66s/it]149it [06:18,  2.65s/it]150it [06:21,  2.82s/it]151it [06:24,  2.73s/it]152it [06:26,  2.57s/it]153it [06:28,  2.47s/it]154it [06:30,  2.40s/it]155it [06:33,  2.53s/it]156it [06:36,  2.57s/it]157it [06:38,  2.48s/it]158it [06:41,  2.53s/it]159it [06:44,  2.63s/it]160it [06:46,  2.66s/it]161it [06:50,  2.83s/it]162it [06:53,  2.91s/it]163it [06:55,  2.78s/it]164it [06:58,  2.83s/it]165it [07:01,  2.79s/it]166it [07:03,  2.71s/it]167it [07:06,  2.80s/it]168it [07:09,  2.77s/it]169it [07:12,  2.68s/it]170it [07:14,  2.52s/it]171it [07:17,  2.69s/it]172it [07:19,  2.66s/it]173it [07:22,  2.53s/it]174it [07:25,  2.74s/it]175it [07:28,  2.73s/it]176it [07:30,  2.74s/it]177it [07:33,  2.70s/it]178it [07:36,  2.81s/it]179it [07:39,  2.76s/it]180it [07:42,  2.80s/it]181it [07:44,  2.75s/it]182it [07:47,  2.70s/it]183it [07:50,  2.75s/it]184it [07:53,  2.90s/it]185it [07:56,  2.93s/it]186it [07:59,  2.83s/it]187it [08:02,  2.95s/it]188it [08:05,  3.02s/it]189it [08:08,  2.94s/it]190it [08:10,  2.89s/it]191it [08:14,  3.00s/it]192it [08:17,  2.98s/it]193it [08:20,  2.96s/it]194it [08:23,  3.01s/it]195it [08:25,  2.89s/it]196it [08:28,  2.73s/it]197it [08:31,  2.82s/it]198it [08:34,  3.10s/it]199it [08:38,  3.10s/it]200it [08:41,  3.15s/it]201it [08:44,  3.07s/it]202it [08:47,  3.09s/it]203it [08:51,  3.35s/it]204it [08:55,  3.64s/it]205it [08:59,  3.58s/it]206it [09:02,  3.54s/it]207it [09:06,  3.53s/it]208it [09:09,  3.50s/it]209it [09:12,  3.39s/it]210it [09:15,  3.38s/it]211it [09:19,  3.35s/it]212it [09:21,  3.14s/it]213it [09:24,  3.03s/it]213it [09:24,  2.65s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:06<02:20,  6.37s/it]  9%|▊         | 2/23 [00:06<00:58,  2.80s/it] 13%|█▎        | 3/23 [00:07<00:33,  1.68s/it] 17%|█▋        | 4/23 [00:07<00:22,  1.17s/it] 22%|██▏       | 5/23 [00:07<00:15,  1.14it/s] 26%|██▌       | 6/23 [00:07<00:11,  1.54it/s] 30%|███       | 7/23 [00:08<00:08,  1.99it/s] 35%|███▍      | 8/23 [00:08<00:06,  2.46it/s] 39%|███▉      | 9/23 [00:08<00:04,  2.91it/s] 43%|████▎     | 10/23 [00:08<00:03,  3.32it/s] 48%|████▊     | 11/23 [00:08<00:03,  3.69it/s] 52%|█████▏    | 12/23 [00:09<00:02,  3.99it/s] 57%|█████▋    | 13/23 [00:09<00:02,  4.22it/s] 61%|██████    | 14/23 [00:09<00:02,  4.41it/s] 65%|██████▌   | 15/23 [00:09<00:01,  4.54it/s] 70%|██████▉   | 16/23 [00:10<00:01,  4.65it/s] 74%|███████▍  | 17/23 [00:10<00:01,  4.72it/s] 78%|███████▊  | 18/23 [00:10<00:01,  4.77it/s] 83%|████████▎ | 19/23 [00:10<00:00,  4.81it/s] 87%|████████▋ | 20/23 [00:10<00:00,  4.84it/s] 91%|█████████▏| 21/23 [00:11<00:00,  4.86it/s] 96%|█████████▌| 22/23 [00:11<00:00,  4.87it/s]100%|██████████| 23/23 [00:11<00:00,  5.48it/s]100%|██████████| 23/23 [00:11<00:00,  1.97it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:03<01:26,  3.93s/it]  9%|▊         | 2/23 [00:04<00:46,  2.23s/it] 13%|█▎        | 3/23 [00:05<00:26,  1.30s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.15it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.59it/s] 26%|██▌       | 6/23 [00:05<00:08,  2.06it/s] 30%|███       | 7/23 [00:05<00:06,  2.54it/s] 35%|███▍      | 8/23 [00:06<00:05,  3.00it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.41it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.77it/s] 48%|████▊     | 11/23 [00:06<00:02,  4.05it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.27it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.44it/s] 61%|██████    | 14/23 [00:07<00:01,  4.58it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.67it/s] 70%|██████▉   | 16/23 [00:07<00:01,  4.74it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.77it/s] 78%|███████▊  | 18/23 [00:08<00:01,  3.74it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.02it/s] 87%|████████▋ | 20/23 [00:08<00:00,  3.74it/s] 91%|█████████▏| 21/23 [00:09<00:00,  3.19it/s] 96%|█████████▌| 22/23 [00:09<00:00,  2.90it/s]100%|██████████| 23/23 [00:10<00:00,  3.16it/s]100%|██████████| 23/23 [00:10<00:00,  2.19it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6511660814285278


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.54608215395236
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.19499355875686
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.196385714740206


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.11660766601562
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.96s/it]2it [00:06,  3.46s/it]3it [00:10,  3.58s/it]4it [00:13,  3.42s/it]5it [00:17,  3.58s/it]6it [00:21,  3.74s/it]7it [00:24,  3.60s/it]8it [00:27,  3.39s/it]9it [00:30,  3.20s/it]10it [00:33,  3.13s/it]11it [00:36,  3.06s/it]12it [00:39,  2.89s/it]13it [00:42,  3.17s/it]14it [00:45,  3.08s/it]15it [00:48,  2.88s/it]16it [00:50,  2.76s/it]17it [00:52,  2.62s/it]18it [00:56,  2.89s/it]19it [00:59,  3.06s/it]20it [01:03,  3.21s/it]21it [01:06,  3.22s/it]22it [01:09,  3.11s/it]23it [01:12,  3.05s/it]24it [01:15,  3.20s/it]25it [01:19,  3.16s/it]26it [01:22,  3.13s/it]27it [01:25,  3.14s/it]28it [01:28,  3.25s/it]29it [01:31,  3.24s/it]30it [01:34,  3.17s/it]31it [01:38,  3.15s/it]32it [01:41,  3.18s/it]33it [01:44,  3.20s/it]34it [01:47,  3.15s/it]35it [01:50,  3.01s/it]36it [01:53,  3.02s/it]37it [01:56,  3.17s/it]38it [01:59,  3.04s/it]39it [02:02,  2.96s/it]40it [02:05,  2.96s/it]41it [02:08,  2.91s/it]42it [02:10,  2.87s/it]43it [02:13,  2.82s/it]44it [02:17,  3.04s/it]45it [02:19,  2.97s/it]46it [02:23,  3.00s/it]47it [02:25,  2.95s/it]48it [02:28,  2.84s/it]49it [02:30,  2.69s/it]50it [02:33,  2.72s/it]51it [02:36,  2.76s/it]52it [02:40,  3.15s/it]53it [02:44,  3.43s/it]54it [02:48,  3.54s/it]55it [02:50,  3.21s/it]56it [02:52,  2.86s/it]57it [02:54,  2.55s/it]58it [02:57,  2.52s/it]59it [02:59,  2.51s/it]60it [03:02,  2.61s/it]61it [03:05,  2.83s/it]62it [03:08,  2.69s/it]63it [03:10,  2.60s/it]64it [03:13,  2.73s/it]65it [03:15,  2.64s/it]66it [03:18,  2.58s/it]67it [03:20,  2.48s/it]68it [03:23,  2.68s/it]69it [03:26,  2.66s/it]70it [03:28,  2.61s/it]71it [03:31,  2.66s/it]72it [03:34,  2.74s/it]73it [03:37,  2.74s/it]74it [03:40,  2.89s/it]75it [03:42,  2.71s/it]76it [03:45,  2.67s/it]77it [03:48,  2.70s/it]78it [03:51,  2.76s/it]79it [03:53,  2.72s/it]80it [03:56,  2.71s/it]81it [03:59,  2.70s/it]82it [04:01,  2.55s/it]83it [04:04,  2.67s/it]84it [04:07,  2.71s/it]85it [04:09,  2.75s/it]86it [04:12,  2.57s/it]87it [04:15,  2.74s/it]88it [04:17,  2.57s/it]89it [04:19,  2.51s/it]90it [04:22,  2.47s/it]91it [04:24,  2.51s/it]92it [04:27,  2.53s/it]93it [04:30,  2.59s/it]94it [04:32,  2.47s/it]95it [04:34,  2.44s/it]96it [04:37,  2.49s/it]97it [04:39,  2.33s/it]98it [04:42,  2.48s/it]99it [04:44,  2.49s/it]100it [04:46,  2.38s/it]101it [04:49,  2.65s/it]102it [04:52,  2.75s/it]103it [04:55,  2.74s/it]104it [04:58,  2.65s/it]105it [05:00,  2.55s/it]106it [05:02,  2.48s/it]107it [05:05,  2.52s/it]108it [05:07,  2.48s/it]109it [05:10,  2.42s/it]110it [05:13,  2.78s/it]111it [05:16,  2.84s/it]112it [05:19,  2.76s/it]113it [05:21,  2.73s/it]114it [05:25,  2.92s/it]115it [05:27,  2.74s/it]116it [05:29,  2.63s/it]117it [05:32,  2.60s/it]118it [05:35,  2.60s/it]119it [05:37,  2.61s/it]120it [05:40,  2.56s/it]121it [05:42,  2.57s/it]122it [05:45,  2.57s/it]123it [05:48,  2.73s/it]124it [05:51,  2.71s/it]125it [05:53,  2.74s/it]126it [05:56,  2.85s/it]127it [06:00,  3.08s/it]128it [06:03,  2.91s/it]129it [06:05,  2.90s/it]130it [06:09,  2.97s/it]131it [06:12,  3.06s/it]132it [06:15,  3.02s/it]133it [06:17,  2.93s/it]134it [06:20,  2.74s/it]135it [06:23,  2.86s/it]136it [06:26,  2.82s/it]137it [06:29,  2.88s/it]138it [06:31,  2.85s/it]139it [06:35,  3.10s/it]140it [06:38,  3.14s/it]141it [06:41,  3.01s/it]142it [06:44,  2.94s/it]143it [06:48,  3.28s/it]144it [06:51,  3.21s/it]145it [06:54,  3.11s/it]146it [06:56,  2.91s/it]147it [06:59,  2.83s/it]148it [07:02,  2.92s/it]149it [07:05,  2.82s/it]150it [07:08,  2.97s/it]151it [07:11,  3.10s/it]152it [07:15,  3.11s/it]153it [07:18,  3.31s/it]154it [07:22,  3.46s/it]155it [07:25,  3.38s/it]156it [07:28,  3.27s/it]157it [07:32,  3.39s/it]158it [07:36,  3.44s/it]159it [07:39,  3.50s/it]160it [07:42,  3.27s/it]161it [07:45,  3.17s/it]162it [07:48,  3.12s/it]163it [07:50,  2.93s/it]164it [07:53,  2.91s/it]165it [07:57,  3.19s/it]166it [08:00,  3.07s/it]167it [08:03,  3.12s/it]168it [08:06,  2.96s/it]169it [08:09,  3.03s/it]170it [08:12,  3.14s/it]171it [08:15,  2.96s/it]172it [08:18,  3.08s/it]173it [08:21,  3.01s/it]174it [08:24,  3.10s/it]175it [08:27,  2.95s/it]176it [08:30,  3.03s/it]177it [08:34,  3.30s/it]178it [08:37,  3.31s/it]179it [08:40,  3.19s/it]180it [08:43,  3.09s/it]181it [08:47,  3.24s/it]182it [08:50,  3.19s/it]183it [08:53,  3.12s/it]184it [08:56,  3.09s/it]185it [08:59,  3.03s/it]186it [09:01,  2.95s/it]187it [09:05,  3.16s/it]188it [09:08,  3.08s/it]189it [09:12,  3.23s/it]190it [09:15,  3.28s/it]191it [09:18,  3.10s/it]192it [09:20,  3.02s/it]193it [09:23,  2.97s/it]194it [09:28,  3.38s/it]195it [09:31,  3.26s/it]196it [09:34,  3.32s/it]197it [09:37,  3.25s/it]198it [09:41,  3.28s/it]199it [09:43,  3.03s/it]200it [09:47,  3.28s/it]201it [09:51,  3.45s/it]202it [09:53,  3.24s/it]203it [09:56,  3.05s/it]204it [09:59,  3.02s/it]205it [10:02,  2.96s/it]206it [10:05,  3.11s/it]207it [10:08,  3.10s/it]208it [10:11,  3.09s/it]209it [10:15,  3.12s/it]210it [10:18,  3.14s/it]211it [10:22,  3.39s/it]212it [10:25,  3.22s/it]213it [10:27,  3.10s/it]213it [10:27,  2.95s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<01:53,  5.15s/it]  9%|▊         | 2/23 [00:05<00:47,  2.24s/it] 13%|█▎        | 3/23 [00:05<00:26,  1.31s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.14it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.58it/s] 26%|██▌       | 6/23 [00:06<00:08,  2.05it/s] 30%|███       | 7/23 [00:06<00:06,  2.53it/s] 35%|███▍      | 8/23 [00:06<00:05,  2.99it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.41it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.76it/s] 48%|████▊     | 11/23 [00:07<00:02,  4.04it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.23it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.41it/s] 61%|██████    | 14/23 [00:07<00:01,  4.54it/s] 65%|██████▌   | 15/23 [00:08<00:01,  4.64it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.71it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.76it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.80it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.82it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.85it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.85it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.85it/s]100%|██████████| 23/23 [00:09<00:00,  5.46it/s]100%|██████████| 23/23 [00:09<00:00,  2.35it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:04<01:30,  4.12s/it]  9%|▊         | 2/23 [00:04<00:40,  1.94s/it] 13%|█▎        | 3/23 [00:04<00:23,  1.15s/it] 17%|█▋        | 4/23 [00:04<00:14,  1.28it/s] 22%|██▏       | 5/23 [00:05<00:10,  1.75it/s] 26%|██▌       | 6/23 [00:05<00:07,  2.24it/s] 30%|███       | 7/23 [00:05<00:05,  2.72it/s] 35%|███▍      | 8/23 [00:05<00:04,  3.17it/s] 39%|███▉      | 9/23 [00:05<00:03,  3.55it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.88it/s] 48%|████▊     | 11/23 [00:06<00:02,  4.14it/s] 52%|█████▏    | 12/23 [00:06<00:02,  4.35it/s] 57%|█████▋    | 13/23 [00:06<00:02,  4.45it/s] 61%|██████    | 14/23 [00:07<00:01,  4.57it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.67it/s] 70%|██████▉   | 16/23 [00:07<00:01,  4.74it/s] 74%|███████▍  | 17/23 [00:07<00:01,  4.79it/s] 78%|███████▊  | 18/23 [00:07<00:01,  4.82it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.84it/s] 87%|████████▋ | 20/23 [00:08<00:00,  4.86it/s] 91%|█████████▏| 21/23 [00:08<00:00,  4.87it/s] 96%|█████████▌| 22/23 [00:08<00:00,  4.88it/s]100%|██████████| 23/23 [00:08<00:00,  5.49it/s]100%|██████████| 23/23 [00:08<00:00,  2.58it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6527702808380127


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.90852606144287
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.57404687174355
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.311290974087296


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.27703094482422
=========================          END          =========================
0it [00:00, ?it/s]1it [00:03,  3.58s/it]2it [00:06,  3.24s/it]3it [00:09,  3.28s/it]4it [00:13,  3.33s/it]5it [00:16,  3.24s/it]6it [00:19,  3.20s/it]7it [00:22,  3.19s/it]8it [00:25,  3.17s/it]9it [00:29,  3.21s/it]10it [00:32,  3.20s/it]11it [00:35,  3.34s/it]12it [00:40,  3.60s/it]13it [00:43,  3.48s/it]14it [00:46,  3.43s/it]15it [00:49,  3.35s/it]16it [00:53,  3.34s/it]17it [00:56,  3.37s/it]18it [00:59,  3.18s/it]19it [01:03,  3.35s/it]20it [01:06,  3.47s/it]21it [01:10,  3.67s/it]22it [01:14,  3.69s/it]23it [01:18,  3.60s/it]24it [01:21,  3.42s/it]25it [01:24,  3.28s/it]26it [01:27,  3.39s/it]27it [01:30,  3.25s/it]28it [01:33,  3.06s/it]29it [01:35,  2.96s/it]30it [01:38,  2.97s/it]31it [01:43,  3.41s/it]32it [01:47,  3.58s/it]33it [01:50,  3.47s/it]34it [01:53,  3.41s/it]35it [01:57,  3.34s/it]36it [01:59,  3.23s/it]37it [02:02,  3.12s/it]38it [02:05,  3.04s/it]39it [02:08,  3.02s/it]40it [02:11,  2.99s/it]41it [02:14,  3.07s/it]42it [02:18,  3.10s/it]43it [02:20,  3.06s/it]44it [02:24,  3.12s/it]45it [02:27,  3.09s/it]46it [02:30,  3.16s/it]47it [02:33,  3.20s/it]48it [02:37,  3.23s/it]49it [02:39,  3.08s/it]50it [02:42,  2.93s/it]51it [02:44,  2.76s/it]52it [02:47,  2.80s/it]53it [02:50,  2.89s/it]54it [02:53,  2.85s/it]55it [02:56,  2.75s/it]56it [02:58,  2.70s/it]57it [03:01,  2.59s/it]58it [03:03,  2.62s/it]59it [03:06,  2.58s/it]60it [03:08,  2.57s/it]61it [03:11,  2.58s/it]62it [03:14,  2.62s/it]63it [03:16,  2.68s/it]64it [03:19,  2.79s/it]65it [03:23,  3.00s/it]66it [03:26,  2.97s/it]67it [03:29,  3.11s/it]68it [03:32,  3.13s/it]69it [03:35,  3.06s/it]70it [03:39,  3.29s/it]71it [03:43,  3.37s/it]72it [03:46,  3.26s/it]73it [03:49,  3.25s/it]74it [03:53,  3.41s/it]75it [03:56,  3.43s/it]76it [03:59,  3.11s/it]77it [04:01,  2.95s/it]78it [04:04,  2.94s/it]79it [04:07,  3.00s/it]80it [04:11,  3.22s/it]81it [04:14,  3.13s/it]82it [04:16,  2.97s/it]83it [04:20,  3.09s/it]84it [04:22,  2.96s/it]85it [04:25,  2.88s/it]86it [04:28,  3.02s/it]87it [04:33,  3.39s/it]88it [04:37,  3.51s/it]89it [04:39,  3.20s/it]90it [04:42,  3.10s/it]91it [04:45,  3.04s/it]92it [04:48,  2.98s/it]93it [04:50,  2.85s/it]94it [04:53,  2.91s/it]95it [04:56,  2.89s/it]96it [04:59,  2.89s/it]97it [05:02,  2.80s/it]98it [05:05,  3.03s/it]99it [05:09,  3.25s/it]100it [05:12,  3.18s/it]101it [05:14,  2.99s/it]102it [05:17,  2.86s/it]103it [05:20,  2.85s/it]104it [05:23,  2.81s/it]105it [05:26,  2.88s/it]106it [05:28,  2.79s/it]107it [05:31,  2.83s/it]108it [05:33,  2.68s/it]109it [05:36,  2.67s/it]110it [05:39,  2.74s/it]111it [05:42,  2.71s/it]112it [05:45,  2.79s/it]113it [05:48,  2.90s/it]114it [05:51,  2.90s/it]115it [05:54,  2.93s/it]116it [05:57,  2.94s/it]117it [06:00,  3.03s/it]118it [06:03,  3.04s/it]119it [06:06,  3.03s/it]120it [06:09,  3.08s/it]121it [06:12,  3.09s/it]122it [06:15,  3.13s/it]123it [06:18,  3.05s/it]124it [06:22,  3.10s/it]125it [06:25,  3.11s/it]126it [06:28,  3.10s/it]127it [06:31,  3.05s/it]128it [06:34,  3.08s/it]129it [06:37,  3.08s/it]130it [06:40,  3.04s/it]131it [06:43,  2.98s/it]132it [06:46,  2.94s/it]133it [06:49,  2.96s/it]134it [06:51,  2.91s/it]135it [06:54,  2.90s/it]136it [06:57,  2.93s/it]137it [07:00,  2.75s/it]138it [07:02,  2.61s/it]139it [07:04,  2.58s/it]140it [07:07,  2.61s/it]141it [07:09,  2.50s/it]142it [07:12,  2.52s/it]143it [07:15,  2.62s/it]144it [07:17,  2.60s/it]145it [07:20,  2.55s/it]146it [07:22,  2.44s/it]147it [07:24,  2.39s/it]148it [07:26,  2.37s/it]149it [07:29,  2.29s/it]150it [07:31,  2.34s/it]151it [07:34,  2.51s/it]152it [07:36,  2.39s/it]153it [07:38,  2.34s/it]154it [07:41,  2.34s/it]155it [07:43,  2.45s/it]156it [07:46,  2.47s/it]157it [07:48,  2.44s/it]158it [07:50,  2.40s/it]159it [07:53,  2.44s/it]160it [07:56,  2.45s/it]161it [07:58,  2.42s/it]162it [08:00,  2.46s/it]163it [08:03,  2.44s/it]164it [08:05,  2.35s/it]165it [08:08,  2.58s/it]166it [08:10,  2.52s/it]167it [08:13,  2.38s/it]168it [08:15,  2.35s/it]169it [08:17,  2.41s/it]170it [08:20,  2.63s/it]171it [08:23,  2.71s/it]172it [08:26,  2.71s/it]173it [08:29,  2.66s/it]174it [08:31,  2.61s/it]175it [08:33,  2.52s/it]176it [08:36,  2.57s/it]177it [08:39,  2.53s/it]178it [08:41,  2.55s/it]179it [08:44,  2.50s/it]180it [08:46,  2.48s/it]181it [08:48,  2.46s/it]182it [08:51,  2.47s/it]183it [08:53,  2.40s/it]184it [08:56,  2.41s/it]185it [08:58,  2.43s/it]186it [09:01,  2.45s/it]187it [09:03,  2.40s/it]188it [09:05,  2.43s/it]189it [09:08,  2.47s/it]190it [09:10,  2.50s/it]191it [09:13,  2.46s/it]192it [09:15,  2.47s/it]193it [09:18,  2.49s/it]194it [09:20,  2.52s/it]195it [09:23,  2.63s/it]196it [09:26,  2.65s/it]197it [09:29,  2.66s/it]198it [09:31,  2.57s/it]199it [09:34,  2.56s/it]200it [09:36,  2.48s/it]201it [09:38,  2.47s/it]202it [09:41,  2.50s/it]203it [09:43,  2.42s/it]204it [09:45,  2.35s/it]205it [09:48,  2.43s/it]206it [09:50,  2.36s/it]207it [09:53,  2.45s/it]208it [09:55,  2.49s/it]209it [09:58,  2.53s/it]210it [10:00,  2.45s/it]211it [10:03,  2.45s/it]212it [10:05,  2.38s/it]213it [10:08,  2.45s/it]213it [10:08,  2.85s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:04<01:49,  4.98s/it]  9%|▊         | 2/23 [00:05<00:45,  2.17s/it] 13%|█▎        | 3/23 [00:05<00:25,  1.28s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.17it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.61it/s] 26%|██▌       | 6/23 [00:06<00:08,  2.09it/s] 30%|███       | 7/23 [00:06<00:06,  2.56it/s] 35%|███▍      | 8/23 [00:06<00:04,  3.02it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.43it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.78it/s] 48%|████▊     | 11/23 [00:07<00:02,  4.06it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.28it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.45it/s] 61%|██████    | 14/23 [00:07<00:01,  4.58it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.67it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.74it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.78it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.77it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.81it/s] 87%|████████▋ | 20/23 [00:08<00:00,  4.83it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.85it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.86it/s]100%|██████████| 23/23 [00:09<00:00,  5.48it/s]100%|██████████| 23/23 [00:09<00:00,  2.38it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<01:50,  5.03s/it]  9%|▊         | 2/23 [00:05<00:46,  2.19s/it] 13%|█▎        | 3/23 [00:05<00:25,  1.29s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.16it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.60it/s] 26%|██▌       | 6/23 [00:06<00:08,  2.07it/s] 30%|███       | 7/23 [00:06<00:06,  2.55it/s] 35%|███▍      | 8/23 [00:06<00:04,  3.01it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.42it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.78it/s] 48%|████▊     | 11/23 [00:07<00:02,  4.06it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.29it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.46it/s] 61%|██████    | 14/23 [00:07<00:01,  4.58it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.67it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.74it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.78it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.82it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.84it/s] 87%|████████▋ | 20/23 [00:08<00:00,  4.86it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.87it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.81it/s]100%|██████████| 23/23 [00:09<00:00,  5.43it/s]100%|██████████| 23/23 [00:09<00:00,  2.39it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6512104272842407


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.874007594062824
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.21441169815797
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.231121633000512


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.12104034423828
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.52s/it]2it [00:05,  2.57s/it]3it [00:07,  2.50s/it]4it [00:10,  2.53s/it]5it [00:12,  2.52s/it]6it [00:14,  2.34s/it]7it [00:16,  2.32s/it]8it [00:19,  2.38s/it]9it [00:21,  2.43s/it]10it [00:24,  2.44s/it]11it [00:26,  2.48s/it]12it [00:29,  2.43s/it]13it [00:31,  2.37s/it]14it [00:33,  2.29s/it]15it [00:35,  2.23s/it]16it [00:38,  2.28s/it]17it [00:40,  2.26s/it]18it [00:42,  2.38s/it]19it [00:45,  2.39s/it]20it [00:47,  2.38s/it]21it [00:49,  2.34s/it]22it [00:52,  2.28s/it]23it [00:54,  2.26s/it]24it [00:56,  2.28s/it]25it [00:58,  2.17s/it]26it [01:00,  2.22s/it]27it [01:03,  2.22s/it]28it [01:05,  2.22s/it]29it [01:07,  2.14s/it]30it [01:09,  2.06s/it]31it [01:11,  2.15s/it]32it [01:13,  2.17s/it]33it [01:15,  2.06s/it]34it [01:17,  2.07s/it]35it [01:19,  2.04s/it]36it [01:21,  2.09s/it]37it [01:24,  2.19s/it]38it [01:26,  2.15s/it]39it [01:28,  2.14s/it]40it [01:30,  2.04s/it]41it [01:32,  2.01s/it]42it [01:34,  1.99s/it]43it [01:35,  1.89s/it]44it [01:37,  1.82s/it]45it [01:39,  1.90s/it]46it [01:42,  2.18s/it]47it [01:45,  2.41s/it]48it [01:47,  2.42s/it]49it [01:50,  2.53s/it]50it [01:53,  2.76s/it]51it [01:55,  2.51s/it]52it [01:58,  2.44s/it]53it [02:00,  2.36s/it]54it [02:02,  2.37s/it]55it [02:05,  2.41s/it]56it [02:06,  2.16s/it]57it [02:08,  2.14s/it]58it [02:10,  2.11s/it]59it [02:12,  2.10s/it]60it [02:14,  2.07s/it]61it [02:17,  2.12s/it]62it [02:19,  2.18s/it]63it [02:21,  2.21s/it]64it [02:24,  2.24s/it]65it [02:26,  2.26s/it]66it [02:28,  2.24s/it]67it [02:31,  2.55s/it]68it [02:35,  2.81s/it]69it [02:37,  2.73s/it]70it [02:39,  2.56s/it]71it [02:42,  2.44s/it]72it [02:44,  2.46s/it]73it [02:47,  2.47s/it]74it [02:49,  2.50s/it]75it [02:52,  2.46s/it]76it [02:53,  2.29s/it]77it [02:56,  2.30s/it]78it [02:58,  2.29s/it]79it [03:00,  2.21s/it]80it [03:03,  2.29s/it]81it [03:05,  2.27s/it]82it [03:07,  2.22s/it]83it [03:09,  2.19s/it]84it [03:11,  2.14s/it]85it [03:13,  2.07s/it]86it [03:15,  2.01s/it]87it [03:17,  1.98s/it]88it [03:19,  2.07s/it]89it [03:21,  2.01s/it]90it [03:23,  2.02s/it]91it [03:25,  2.02s/it]92it [03:27,  2.06s/it]93it [03:29,  2.14s/it]94it [03:32,  2.16s/it]95it [03:34,  2.19s/it]96it [03:36,  2.25s/it]97it [03:38,  2.13s/it]98it [03:40,  2.04s/it]99it [03:41,  1.81s/it]100it [03:43,  1.81s/it]101it [03:45,  1.82s/it]102it [03:46,  1.77s/it]103it [03:48,  1.73s/it]104it [03:51,  1.94s/it]105it [03:53,  1.98s/it]106it [03:55,  2.11s/it]107it [03:58,  2.29s/it]108it [04:00,  2.33s/it]109it [04:02,  2.28s/it]110it [04:06,  2.67s/it]111it [04:10,  3.00s/it]112it [04:13,  3.14s/it]113it [04:16,  3.20s/it]114it [04:18,  2.80s/it]115it [04:20,  2.44s/it]116it [04:21,  2.16s/it]117it [04:23,  2.11s/it]118it [04:25,  1.99s/it]119it [04:27,  1.83s/it]120it [04:28,  1.68s/it]121it [04:29,  1.48s/it]122it [04:30,  1.41s/it]123it [04:31,  1.30s/it]124it [04:33,  1.31s/it]125it [04:34,  1.23s/it]126it [04:35,  1.34s/it]127it [04:37,  1.35s/it]128it [04:38,  1.24s/it]129it [04:39,  1.16s/it]130it [04:40,  1.10s/it]131it [04:41,  1.11s/it]132it [04:42,  1.15s/it]133it [04:43,  1.22s/it]134it [04:45,  1.33s/it]135it [04:46,  1.41s/it]136it [04:48,  1.53s/it]137it [04:51,  1.86s/it]138it [04:55,  2.39s/it]139it [04:58,  2.61s/it]140it [05:01,  2.87s/it]141it [05:03,  2.58s/it]142it [05:04,  2.20s/it]143it [05:06,  1.89s/it]144it [05:07,  1.64s/it]145it [05:07,  1.41s/it]146it [05:09,  1.38s/it]147it [05:10,  1.30s/it]148it [05:11,  1.32s/it]149it [05:13,  1.34s/it]150it [05:14,  1.35s/it]151it [05:16,  1.40s/it]152it [05:17,  1.45s/it]153it [05:19,  1.52s/it]154it [05:20,  1.48s/it]155it [05:22,  1.55s/it]156it [05:24,  1.58s/it]157it [05:25,  1.54s/it]158it [05:27,  1.55s/it]159it [05:28,  1.56s/it]160it [05:30,  1.56s/it]161it [05:31,  1.54s/it]162it [05:33,  1.51s/it]163it [05:35,  1.65s/it]164it [05:36,  1.62s/it]165it [05:38,  1.69s/it]166it [05:40,  1.70s/it]167it [05:42,  1.76s/it]168it [05:43,  1.71s/it]169it [05:45,  1.60s/it]170it [05:46,  1.64s/it]171it [05:47,  1.50s/it]172it [05:49,  1.54s/it]173it [05:51,  1.54s/it]174it [05:52,  1.61s/it]175it [05:54,  1.60s/it]176it [05:56,  1.60s/it]177it [05:57,  1.61s/it]178it [05:59,  1.56s/it]179it [06:00,  1.47s/it]180it [06:02,  1.56s/it]181it [06:03,  1.54s/it]182it [06:04,  1.43s/it]183it [06:06,  1.41s/it]184it [06:07,  1.30s/it]185it [06:08,  1.35s/it]186it [06:09,  1.28s/it]187it [06:10,  1.25s/it]188it [06:12,  1.23s/it]189it [06:13,  1.37s/it]190it [06:15,  1.41s/it]191it [06:17,  1.48s/it]192it [06:18,  1.43s/it]193it [06:19,  1.47s/it]194it [06:21,  1.53s/it]195it [06:23,  1.60s/it]196it [06:25,  1.63s/it]197it [06:26,  1.62s/it]198it [06:28,  1.71s/it]199it [06:30,  1.64s/it]200it [06:32,  1.79s/it]201it [06:33,  1.70s/it]202it [06:34,  1.49s/it]203it [06:35,  1.43s/it]204it [06:37,  1.41s/it]205it [06:38,  1.48s/it]206it [06:40,  1.51s/it]207it [06:42,  1.53s/it]208it [06:43,  1.43s/it]209it [06:44,  1.41s/it]210it [06:46,  1.43s/it]211it [06:47,  1.32s/it]212it [06:48,  1.35s/it]213it [06:50,  1.36s/it]213it [06:50,  1.92s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<01:51,  5.07s/it]  9%|▊         | 2/23 [00:05<00:46,  2.21s/it] 13%|█▎        | 3/23 [00:05<00:25,  1.29s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.16it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.60it/s] 26%|██▌       | 6/23 [00:06<00:08,  2.07it/s] 30%|███       | 7/23 [00:06<00:06,  2.55it/s] 35%|███▍      | 8/23 [00:06<00:04,  3.01it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.42it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.77it/s] 48%|████▊     | 11/23 [00:07<00:02,  4.02it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.26it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.44it/s] 61%|██████    | 14/23 [00:07<00:01,  4.57it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.66it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.73it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.78it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.81it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.77it/s] 87%|████████▋ | 20/23 [00:08<00:00,  4.81it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.83it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.85it/s]100%|██████████| 23/23 [00:09<00:00,  5.46it/s]100%|██████████| 23/23 [00:09<00:00,  2.37it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8058748841285706
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:04<01:49,  4.96s/it]  9%|▊         | 2/23 [00:05<00:45,  2.16s/it] 13%|█▎        | 3/23 [00:05<00:25,  1.27s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.18it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.62it/s] 26%|██▌       | 6/23 [00:05<00:08,  2.09it/s] 30%|███       | 7/23 [00:06<00:06,  2.57it/s] 35%|███▍      | 8/23 [00:06<00:04,  3.02it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.43it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.78it/s] 48%|████▊     | 11/23 [00:07<00:02,  4.03it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.26it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.43it/s] 61%|██████    | 14/23 [00:07<00:01,  4.56it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.65it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.71it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.76it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.79it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.82it/s] 87%|████████▋ | 20/23 [00:08<00:00,  4.83it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.85it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.85it/s]100%|██████████| 23/23 [00:09<00:00,  5.46it/s]100%|██████████| 23/23 [00:09<00:00,  2.40it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6500989198684692


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.2833966171902
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.80968429914056
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.21170049311739


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.58748626708984
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.04556437694166
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.41146837662123
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 30.41873260607066


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.00989532470703
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.54s/it]2it [00:02,  1.43s/it]3it [00:04,  1.58s/it]4it [00:06,  1.68s/it]5it [00:08,  1.65s/it]6it [00:09,  1.66s/it]7it [00:11,  1.59s/it]8it [00:12,  1.57s/it]9it [00:13,  1.43s/it]10it [00:15,  1.42s/it]11it [00:16,  1.41s/it]12it [00:17,  1.33s/it]13it [00:19,  1.36s/it]14it [00:20,  1.47s/it]15it [00:22,  1.59s/it]16it [00:24,  1.56s/it]17it [00:26,  1.61s/it]18it [00:27,  1.62s/it]19it [00:29,  1.65s/it]20it [00:31,  1.64s/it]21it [00:32,  1.64s/it]22it [00:34,  1.62s/it]23it [00:35,  1.62s/it]24it [00:37,  1.56s/it]25it [00:38,  1.46s/it]26it [00:39,  1.40s/it]27it [00:41,  1.49s/it]28it [00:42,  1.47s/it]29it [00:44,  1.52s/it]30it [00:46,  1.54s/it]31it [00:47,  1.49s/it]32it [00:48,  1.47s/it]33it [00:50,  1.48s/it]34it [00:51,  1.50s/it]35it [00:53,  1.51s/it]36it [00:54,  1.51s/it]37it [00:56,  1.51s/it]38it [00:58,  1.52s/it]39it [00:59,  1.49s/it]40it [01:00,  1.50s/it]41it [01:02,  1.41s/it]42it [01:03,  1.39s/it]43it [01:04,  1.25s/it]44it [01:05,  1.28s/it]45it [01:07,  1.31s/it]46it [01:08,  1.34s/it]47it [01:10,  1.38s/it]48it [01:11,  1.49s/it]49it [01:13,  1.55s/it]50it [01:14,  1.52s/it]51it [01:16,  1.49s/it]52it [01:18,  1.55s/it]53it [01:19,  1.45s/it]54it [01:20,  1.45s/it]55it [01:22,  1.40s/it]56it [01:23,  1.46s/it]57it [01:25,  1.56s/it]58it [01:26,  1.40s/it]59it [01:27,  1.24s/it]60it [01:28,  1.30s/it]61it [01:30,  1.34s/it]62it [01:31,  1.46s/it]63it [01:33,  1.55s/it]64it [01:35,  1.55s/it]65it [01:36,  1.38s/it]66it [01:37,  1.50s/it]67it [01:39,  1.41s/it]68it [01:40,  1.30s/it]69it [01:41,  1.31s/it]70it [01:43,  1.35s/it]71it [01:44,  1.39s/it]72it [01:45,  1.36s/it]73it [01:47,  1.39s/it]74it [01:48,  1.34s/it]75it [01:50,  1.42s/it]76it [01:51,  1.51s/it]77it [01:53,  1.54s/it]78it [01:54,  1.50s/it]79it [01:56,  1.50s/it]80it [01:57,  1.40s/it]81it [01:58,  1.39s/it]82it [02:00,  1.49s/it]83it [02:02,  1.49s/it]84it [02:03,  1.47s/it]85it [02:04,  1.47s/it]86it [02:06,  1.44s/it]87it [02:07,  1.48s/it]88it [02:09,  1.57s/it]89it [02:11,  1.51s/it]90it [02:12,  1.48s/it]91it [02:13,  1.47s/it]92it [02:15,  1.47s/it]93it [02:17,  1.57s/it]94it [02:18,  1.54s/it]95it [02:20,  1.54s/it]96it [02:22,  1.67s/it]97it [02:23,  1.71s/it]98it [02:25,  1.62s/it]99it [02:26,  1.61s/it]100it [02:27,  1.44s/it]101it [02:29,  1.43s/it]102it [02:31,  1.53s/it]103it [02:32,  1.59s/it]104it [02:34,  1.67s/it]105it [02:36,  1.62s/it]106it [02:37,  1.63s/it]107it [02:39,  1.61s/it]108it [02:41,  1.66s/it]109it [02:43,  1.72s/it]110it [02:44,  1.68s/it]111it [02:46,  1.60s/it]112it [02:47,  1.64s/it]113it [02:49,  1.65s/it]114it [02:50,  1.61s/it]115it [02:52,  1.47s/it]116it [02:53,  1.42s/it]117it [02:55,  1.54s/it]118it [02:56,  1.58s/it]119it [02:58,  1.56s/it]120it [02:59,  1.49s/it]121it [03:01,  1.51s/it]122it [03:03,  1.57s/it]123it [03:05,  1.75s/it]124it [03:07,  1.79s/it]125it [03:08,  1.78s/it]126it [03:10,  1.72s/it]127it [03:12,  1.68s/it]128it [03:13,  1.63s/it]129it [03:15,  1.69s/it]130it [03:17,  1.76s/it]131it [03:18,  1.67s/it]132it [03:20,  1.57s/it]133it [03:21,  1.56s/it]134it [03:23,  1.56s/it]135it [03:24,  1.62s/it]136it [03:26,  1.60s/it]137it [03:28,  1.63s/it]138it [03:29,  1.60s/it]139it [03:31,  1.64s/it]140it [03:33,  1.63s/it]141it [03:34,  1.57s/it]142it [03:36,  1.62s/it]143it [03:37,  1.65s/it]144it [03:39,  1.62s/it]145it [03:40,  1.54s/it]146it [03:42,  1.59s/it]147it [03:44,  1.58s/it]148it [03:45,  1.59s/it]149it [03:47,  1.54s/it]150it [03:48,  1.57s/it]151it [03:50,  1.56s/it]152it [03:51,  1.59s/it]153it [03:53,  1.57s/it]154it [03:54,  1.48s/it]155it [03:56,  1.45s/it]156it [03:57,  1.46s/it]157it [03:59,  1.50s/it]158it [04:00,  1.55s/it]159it [04:02,  1.49s/it]160it [04:04,  1.61s/it]161it [04:05,  1.61s/it]162it [04:07,  1.55s/it]163it [04:08,  1.52s/it]164it [04:10,  1.52s/it]165it [04:11,  1.48s/it]166it [04:12,  1.44s/it]167it [04:14,  1.58s/it]168it [04:17,  1.98s/it]169it [04:19,  1.84s/it]170it [04:20,  1.69s/it]171it [04:21,  1.58s/it]172it [04:23,  1.52s/it]173it [04:25,  1.62s/it]174it [04:28,  2.04s/it]175it [04:31,  2.36s/it]176it [04:33,  2.20s/it]177it [04:34,  1.96s/it]178it [04:36,  1.87s/it]179it [04:37,  1.72s/it]180it [04:39,  1.66s/it]181it [04:40,  1.61s/it]182it [04:42,  1.60s/it]183it [04:43,  1.63s/it]184it [04:45,  1.64s/it]185it [04:46,  1.60s/it]186it [04:48,  1.51s/it]187it [04:49,  1.48s/it]188it [04:51,  1.58s/it]189it [04:53,  1.57s/it]190it [04:54,  1.57s/it]191it [04:56,  1.58s/it]192it [04:57,  1.61s/it]193it [04:59,  1.55s/it]194it [05:01,  1.66s/it]195it [05:02,  1.65s/it]196it [05:04,  1.66s/it]197it [05:05,  1.53s/it]198it [05:07,  1.58s/it]199it [05:08,  1.52s/it]200it [05:10,  1.46s/it]201it [05:11,  1.37s/it]202it [05:12,  1.29s/it]203it [05:14,  1.39s/it]204it [05:15,  1.47s/it]205it [05:16,  1.26s/it]206it [05:17,  1.16s/it]207it [05:18,  1.19s/it]208it [05:19,  1.20s/it]209it [05:21,  1.23s/it]210it [05:22,  1.31s/it]211it [05:24,  1.36s/it]212it [05:25,  1.38s/it]213it [05:26,  1.39s/it]213it [05:26,  1.53s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<02:03,  5.60s/it]  9%|▊         | 2/23 [00:05<00:50,  2.43s/it] 13%|█▎        | 3/23 [00:06<00:28,  1.41s/it] 17%|█▋        | 4/23 [00:06<00:17,  1.07it/s] 22%|██▏       | 5/23 [00:06<00:12,  1.49it/s] 26%|██▌       | 6/23 [00:06<00:08,  1.95it/s] 30%|███       | 7/23 [00:06<00:06,  2.42it/s] 35%|███▍      | 8/23 [00:07<00:05,  2.88it/s] 39%|███▉      | 9/23 [00:07<00:04,  3.30it/s] 43%|████▎     | 10/23 [00:07<00:03,  3.66it/s] 48%|████▊     | 11/23 [00:07<00:03,  3.96it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.19it/s] 57%|█████▋    | 13/23 [00:08<00:02,  4.37it/s] 61%|██████    | 14/23 [00:08<00:01,  4.50it/s] 65%|██████▌   | 15/23 [00:08<00:01,  4.60it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.67it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.72it/s] 78%|███████▊  | 18/23 [00:09<00:01,  4.76it/s] 83%|████████▎ | 19/23 [00:09<00:00,  4.80it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.82it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.80it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.81it/s]100%|██████████| 23/23 [00:10<00:00,  5.42it/s]100%|██████████| 23/23 [00:10<00:00,  2.24it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:04<01:39,  4.51s/it]  9%|▊         | 2/23 [00:05<00:50,  2.41s/it] 13%|█▎        | 3/23 [00:05<00:28,  1.40s/it] 17%|█▋        | 4/23 [00:05<00:17,  1.07it/s] 22%|██▏       | 5/23 [00:06<00:12,  1.49it/s] 26%|██▌       | 6/23 [00:06<00:08,  1.95it/s] 30%|███       | 7/23 [00:06<00:06,  2.42it/s] 35%|███▍      | 8/23 [00:06<00:05,  2.88it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.30it/s] 43%|████▎     | 10/23 [00:07<00:03,  3.67it/s] 48%|████▊     | 11/23 [00:07<00:03,  3.97it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.20it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.38it/s] 61%|██████    | 14/23 [00:07<00:01,  4.51it/s] 65%|██████▌   | 15/23 [00:08<00:01,  4.62it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.69it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.74it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.78it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.80it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.82it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.83it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.83it/s]100%|██████████| 23/23 [00:09<00:00,  5.43it/s]100%|██████████| 23/23 [00:09<00:00,  2.33it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6523683667182922


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.511563686572316
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.29979272806062
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 30.823590023606933


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.2368392944336
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.92s/it]2it [00:03,  1.65s/it]3it [00:04,  1.57s/it]4it [00:06,  1.61s/it]5it [00:08,  1.76s/it]6it [00:10,  1.75s/it]7it [00:11,  1.73s/it]8it [00:13,  1.78s/it]9it [00:15,  1.79s/it]10it [00:17,  1.65s/it]11it [00:18,  1.50s/it]12it [00:19,  1.43s/it]13it [00:20,  1.39s/it]14it [00:22,  1.37s/it]15it [00:23,  1.34s/it]16it [00:25,  1.44s/it]17it [00:26,  1.39s/it]18it [00:27,  1.33s/it]19it [00:28,  1.30s/it]20it [00:30,  1.43s/it]21it [00:33,  1.88s/it]22it [00:35,  2.03s/it]23it [00:36,  1.75s/it]24it [00:38,  1.78s/it]25it [00:40,  1.84s/it]26it [00:42,  1.90s/it]27it [00:44,  1.93s/it]28it [00:46,  1.87s/it]29it [00:48,  1.98s/it]30it [00:50,  2.03s/it]31it [00:52,  1.96s/it]32it [00:53,  1.77s/it]33it [00:55,  1.73s/it]34it [00:56,  1.61s/it]35it [00:58,  1.47s/it]36it [00:59,  1.34s/it]37it [01:00,  1.27s/it]38it [01:01,  1.33s/it]39it [01:04,  1.76s/it]40it [01:07,  2.20s/it]41it [01:09,  2.07s/it]42it [01:11,  2.01s/it]43it [01:13,  2.14s/it]44it [01:15,  2.06s/it]45it [01:16,  1.85s/it]46it [01:19,  1.93s/it]47it [01:21,  2.07s/it]48it [01:22,  1.88s/it]49it [01:24,  1.64s/it]50it [01:25,  1.47s/it]51it [01:26,  1.35s/it]52it [01:27,  1.23s/it]53it [01:28,  1.13s/it]54it [01:28,  1.07s/it]55it [01:29,  1.00it/s]56it [01:31,  1.07s/it]57it [01:32,  1.10s/it]58it [01:33,  1.18s/it]59it [01:36,  1.63s/it]60it [01:38,  1.79s/it]61it [01:40,  1.98s/it]62it [01:42,  2.01s/it]63it [01:45,  2.29s/it]64it [01:48,  2.46s/it]65it [01:51,  2.49s/it]66it [01:53,  2.49s/it]67it [01:56,  2.47s/it]68it [01:58,  2.50s/it]69it [02:01,  2.55s/it]70it [02:03,  2.41s/it]71it [02:05,  2.29s/it]72it [02:08,  2.37s/it]73it [02:12,  2.89s/it]74it [02:14,  2.81s/it]75it [02:16,  2.49s/it]76it [02:18,  2.39s/it]77it [02:21,  2.57s/it]78it [02:24,  2.65s/it]79it [02:26,  2.58s/it]80it [02:29,  2.42s/it]81it [02:31,  2.39s/it]82it [02:33,  2.27s/it]83it [02:35,  2.19s/it]84it [02:37,  2.23s/it]85it [02:40,  2.28s/it]86it [02:42,  2.26s/it]87it [02:44,  2.17s/it]88it [02:46,  2.29s/it]89it [02:49,  2.35s/it]90it [02:51,  2.35s/it]91it [02:54,  2.41s/it]92it [02:56,  2.46s/it]93it [02:59,  2.43s/it]94it [03:01,  2.34s/it]95it [03:03,  2.40s/it]96it [03:05,  2.34s/it]97it [03:08,  2.30s/it]98it [03:10,  2.31s/it]99it [03:12,  2.36s/it]100it [03:14,  2.24s/it]101it [03:16,  2.14s/it]102it [03:19,  2.15s/it]103it [03:20,  2.07s/it]104it [03:22,  1.99s/it]105it [03:24,  1.92s/it]106it [03:26,  1.92s/it]107it [03:28,  1.94s/it]108it [03:29,  1.84s/it]109it [03:31,  1.87s/it]110it [03:33,  1.78s/it]111it [03:35,  1.78s/it]112it [03:37,  1.85s/it]113it [03:39,  1.90s/it]114it [03:41,  1.93s/it]115it [03:43,  1.94s/it]116it [03:45,  2.11s/it]117it [03:48,  2.29s/it]118it [03:50,  2.28s/it]119it [03:52,  2.21s/it]120it [03:55,  2.36s/it]121it [03:58,  2.55s/it]122it [04:00,  2.43s/it]123it [04:03,  2.61s/it]124it [04:06,  2.69s/it]125it [04:09,  2.75s/it]126it [04:11,  2.68s/it]127it [04:14,  2.74s/it]128it [04:17,  2.70s/it]129it [04:19,  2.61s/it]130it [04:22,  2.53s/it]131it [04:24,  2.46s/it]132it [04:26,  2.44s/it]133it [04:28,  2.34s/it]134it [04:31,  2.38s/it]135it [04:34,  2.48s/it]136it [04:36,  2.49s/it]137it [04:39,  2.47s/it]138it [04:40,  2.21s/it]139it [04:43,  2.42s/it]140it [04:46,  2.44s/it]141it [04:48,  2.40s/it]142it [04:50,  2.35s/it]143it [04:53,  2.37s/it]144it [04:55,  2.50s/it]145it [04:58,  2.50s/it]146it [05:00,  2.53s/it]147it [05:03,  2.41s/it]148it [05:05,  2.29s/it]149it [05:07,  2.26s/it]150it [05:09,  2.25s/it]151it [05:11,  2.26s/it]152it [05:13,  2.21s/it]153it [05:17,  2.75s/it]154it [05:21,  3.16s/it]155it [05:24,  2.94s/it]156it [05:26,  2.80s/it]157it [05:29,  2.87s/it]158it [05:32,  2.83s/it]159it [05:34,  2.57s/it]160it [05:36,  2.49s/it]161it [05:38,  2.28s/it]162it [05:40,  2.21s/it]163it [05:43,  2.26s/it]164it [05:44,  2.12s/it]165it [05:47,  2.26s/it]166it [05:50,  2.40s/it]167it [05:52,  2.43s/it]168it [05:54,  2.34s/it]169it [05:57,  2.44s/it]170it [06:01,  2.85s/it]171it [06:04,  3.01s/it]172it [06:05,  2.45s/it]173it [06:07,  2.07s/it]174it [06:07,  1.70s/it]175it [06:08,  1.49s/it]176it [06:10,  1.40s/it]177it [06:11,  1.41s/it]178it [06:12,  1.42s/it]179it [06:14,  1.48s/it]180it [06:15,  1.44s/it]181it [06:17,  1.37s/it]182it [06:18,  1.25s/it]183it [06:19,  1.16s/it]184it [06:20,  1.34s/it]185it [06:22,  1.56s/it]186it [06:24,  1.56s/it]187it [06:26,  1.63s/it]188it [06:27,  1.58s/it]189it [06:29,  1.60s/it]190it [06:30,  1.58s/it]191it [06:32,  1.69s/it]192it [06:34,  1.56s/it]193it [06:34,  1.34s/it]194it [06:35,  1.22s/it]195it [06:36,  1.13s/it]196it [06:38,  1.16s/it]197it [06:39,  1.17s/it]198it [06:40,  1.21s/it]199it [06:41,  1.24s/it]200it [06:43,  1.23s/it]201it [06:44,  1.21s/it]202it [06:44,  1.05s/it]203it [06:45,  1.01s/it]204it [06:47,  1.12s/it]205it [06:49,  1.41s/it]206it [06:51,  1.57s/it]207it [06:52,  1.62s/it]208it [06:54,  1.53s/it]209it [06:56,  1.76s/it]210it [06:58,  1.96s/it]211it [07:00,  1.81s/it]212it [07:01,  1.59s/it]213it [07:02,  1.42s/it]213it [07:02,  1.98s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<01:50,  5.01s/it]  9%|▊         | 2/23 [00:05<00:45,  2.18s/it] 13%|█▎        | 3/23 [00:05<00:25,  1.29s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.16it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.60it/s] 26%|██▌       | 6/23 [00:06<00:08,  2.06it/s] 30%|███       | 7/23 [00:06<00:06,  2.53it/s] 35%|███▍      | 8/23 [00:06<00:05,  2.98it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.36it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.70it/s] 48%|████▊     | 11/23 [00:07<00:03,  3.98it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.22it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.40it/s] 61%|██████    | 14/23 [00:07<00:01,  4.54it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.59it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.67it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.73it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.77it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.80it/s] 87%|████████▋ | 20/23 [00:08<00:00,  4.82it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.85it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.84it/s]100%|██████████| 23/23 [00:09<00:00,  5.35it/s]100%|██████████| 23/23 [00:09<00:00,  2.36it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<01:52,  5.11s/it]  9%|▊         | 2/23 [00:05<00:52,  2.49s/it] 13%|█▎        | 3/23 [00:05<00:28,  1.45s/it] 17%|█▋        | 4/23 [00:06<00:18,  1.05it/s] 22%|██▏       | 5/23 [00:06<00:12,  1.45it/s] 26%|██▌       | 6/23 [00:06<00:08,  1.91it/s] 30%|███       | 7/23 [00:06<00:06,  2.38it/s] 35%|███▍      | 8/23 [00:07<00:05,  2.85it/s] 39%|███▉      | 9/23 [00:07<00:04,  3.27it/s] 43%|████▎     | 10/23 [00:07<00:03,  3.65it/s] 48%|████▊     | 11/23 [00:07<00:03,  3.95it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.19it/s] 57%|█████▋    | 13/23 [00:08<00:02,  4.38it/s] 61%|██████    | 14/23 [00:08<00:01,  4.52it/s] 65%|██████▌   | 15/23 [00:08<00:01,  4.58it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.66it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.72it/s] 78%|███████▊  | 18/23 [00:09<00:01,  4.77it/s] 83%|████████▎ | 19/23 [00:09<00:00,  4.80it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.82it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.84it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.85it/s]100%|██████████| 23/23 [00:10<00:00,  5.45it/s]100%|██████████| 23/23 [00:10<00:00,  2.25it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6520342826843262


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.804970659302725
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.37212558121306
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.62472765391372


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.20343017578125
=========================          END          =========================
0it [00:00, ?it/s]1it [00:03,  3.30s/it]2it [00:07,  3.60s/it]3it [00:10,  3.43s/it]4it [00:13,  3.28s/it]5it [00:15,  2.75s/it]6it [00:16,  2.42s/it]7it [00:19,  2.51s/it]8it [00:22,  2.54s/it]9it [00:25,  2.65s/it]10it [00:28,  2.76s/it]11it [00:31,  2.89s/it]12it [00:34,  3.07s/it]13it [00:38,  3.38s/it]14it [00:41,  3.10s/it]15it [00:44,  3.07s/it]16it [00:47,  3.19s/it]17it [00:50,  3.07s/it]18it [00:53,  2.93s/it]19it [00:55,  2.84s/it]20it [00:58,  2.82s/it]21it [01:01,  2.73s/it]22it [01:03,  2.65s/it]23it [01:05,  2.55s/it]24it [01:08,  2.52s/it]25it [01:10,  2.52s/it]26it [01:14,  2.71s/it]27it [01:16,  2.57s/it]28it [01:18,  2.54s/it]29it [01:20,  2.40s/it]30it [01:23,  2.39s/it]31it [01:25,  2.31s/it]32it [01:27,  2.29s/it]33it [01:30,  2.36s/it]34it [01:32,  2.46s/it]35it [01:35,  2.60s/it]36it [01:38,  2.57s/it]37it [01:40,  2.42s/it]38it [01:42,  2.44s/it]39it [01:45,  2.48s/it]40it [01:47,  2.45s/it]41it [01:50,  2.42s/it]42it [01:52,  2.33s/it]43it [01:54,  2.26s/it]44it [01:57,  2.39s/it]45it [01:59,  2.51s/it]46it [02:02,  2.55s/it]47it [02:04,  2.54s/it]48it [02:07,  2.53s/it]49it [02:10,  2.57s/it]50it [02:12,  2.52s/it]51it [02:13,  2.19s/it]52it [02:14,  1.74s/it]53it [02:15,  1.47s/it]54it [02:16,  1.31s/it]55it [02:17,  1.23s/it]56it [02:18,  1.29s/it]57it [02:20,  1.36s/it]58it [02:21,  1.30s/it]59it [02:22,  1.31s/it]60it [02:24,  1.26s/it]61it [02:25,  1.26s/it]62it [02:26,  1.25s/it]63it [02:27,  1.11s/it]64it [02:28,  1.03s/it]65it [02:29,  1.23s/it]66it [02:31,  1.47s/it]67it [02:33,  1.55s/it]68it [02:35,  1.57s/it]69it [02:36,  1.40s/it]70it [02:38,  1.58s/it]71it [02:40,  1.73s/it]72it [02:41,  1.68s/it]73it [02:42,  1.47s/it]74it [02:43,  1.30s/it]75it [02:44,  1.12s/it]76it [02:45,  1.02it/s]77it [02:46,  1.13s/it]78it [02:48,  1.27s/it]79it [02:49,  1.29s/it]80it [02:50,  1.31s/it]81it [02:52,  1.29s/it]82it [02:53,  1.28s/it]83it [02:54,  1.14s/it]84it [02:55,  1.03s/it]85it [02:55,  1.02s/it]86it [02:57,  1.24s/it]87it [02:59,  1.49s/it]88it [03:01,  1.55s/it]89it [03:02,  1.40s/it]90it [03:03,  1.29s/it]91it [03:05,  1.52s/it]92it [03:07,  1.66s/it]93it [03:09,  1.64s/it]94it [03:10,  1.42s/it]95it [03:11,  1.28s/it]96it [03:11,  1.14s/it]97it [03:12,  1.05s/it]98it [03:13,  1.03s/it]99it [03:14,  1.08it/s]100it [03:15,  1.14it/s]101it [03:16,  1.06it/s]102it [03:17,  1.08it/s]103it [03:17,  1.18it/s]104it [03:18,  1.18it/s]105it [03:19,  1.10it/s]106it [03:21,  1.31s/it]107it [03:23,  1.43s/it]108it [03:24,  1.28s/it]109it [03:26,  1.37s/it]110it [03:27,  1.41s/it]111it [03:28,  1.34s/it]112it [03:30,  1.39s/it]113it [03:31,  1.39s/it]114it [03:33,  1.54s/it]115it [03:35,  1.56s/it]116it [03:37,  1.64s/it]117it [03:38,  1.65s/it]118it [03:40,  1.63s/it]119it [03:41,  1.61s/it]120it [03:43,  1.52s/it]121it [03:44,  1.48s/it]122it [03:46,  1.56s/it]123it [03:48,  1.61s/it]124it [03:49,  1.51s/it]125it [03:50,  1.52s/it]126it [03:52,  1.49s/it]127it [03:53,  1.54s/it]128it [03:55,  1.54s/it]129it [03:57,  1.53s/it]130it [03:58,  1.52s/it]131it [03:59,  1.48s/it]132it [04:01,  1.46s/it]133it [04:02,  1.38s/it]134it [04:03,  1.30s/it]135it [04:04,  1.31s/it]136it [04:06,  1.32s/it]137it [04:07,  1.35s/it]138it [04:09,  1.34s/it]139it [04:10,  1.29s/it]140it [04:11,  1.26s/it]141it [04:12,  1.31s/it]142it [04:14,  1.34s/it]143it [04:15,  1.19s/it]144it [04:16,  1.20s/it]145it [04:17,  1.27s/it]146it [04:19,  1.52s/it]147it [04:21,  1.54s/it]148it [04:23,  1.57s/it]149it [04:24,  1.51s/it]150it [04:26,  1.56s/it]151it [04:27,  1.56s/it]152it [04:29,  1.49s/it]153it [04:30,  1.43s/it]154it [04:31,  1.34s/it]155it [04:32,  1.33s/it]156it [04:34,  1.45s/it]157it [04:35,  1.43s/it]158it [04:36,  1.28s/it]159it [04:38,  1.34s/it]160it [04:39,  1.35s/it]161it [04:41,  1.46s/it]162it [04:42,  1.50s/it]163it [04:44,  1.47s/it]164it [04:45,  1.51s/it]165it [04:47,  1.49s/it]166it [04:48,  1.51s/it]167it [04:51,  1.70s/it]168it [04:52,  1.60s/it]169it [04:53,  1.55s/it]170it [04:55,  1.46s/it]171it [04:56,  1.47s/it]172it [04:58,  1.56s/it]173it [05:00,  1.59s/it]174it [05:01,  1.65s/it]175it [05:03,  1.58s/it]176it [05:04,  1.45s/it]177it [05:05,  1.36s/it]178it [05:07,  1.50s/it]179it [05:09,  1.64s/it]180it [05:10,  1.60s/it]181it [05:12,  1.53s/it]182it [05:13,  1.54s/it]183it [05:15,  1.63s/it]184it [05:16,  1.35s/it]185it [05:17,  1.41s/it]186it [05:19,  1.52s/it]187it [05:20,  1.45s/it]188it [05:22,  1.49s/it]189it [05:23,  1.47s/it]190it [05:24,  1.28s/it]191it [05:26,  1.30s/it]192it [05:27,  1.33s/it]193it [05:28,  1.31s/it]194it [05:29,  1.26s/it]195it [05:31,  1.38s/it]196it [05:32,  1.31s/it]197it [05:34,  1.38s/it]198it [05:35,  1.32s/it]199it [05:36,  1.33s/it]200it [05:38,  1.36s/it]201it [05:39,  1.34s/it]202it [05:40,  1.28s/it]203it [05:42,  1.35s/it]204it [05:43,  1.46s/it]205it [05:45,  1.43s/it]206it [05:46,  1.37s/it]207it [05:47,  1.39s/it]208it [05:49,  1.34s/it]209it [05:50,  1.37s/it]210it [05:52,  1.39s/it]211it [05:53,  1.40s/it]212it [05:54,  1.39s/it]213it [05:56,  1.42s/it]213it [05:56,  1.67s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<01:57,  5.34s/it]  9%|▊         | 2/23 [00:05<00:49,  2.37s/it] 13%|█▎        | 3/23 [00:05<00:27,  1.38s/it] 17%|█▋        | 4/23 [00:06<00:17,  1.09it/s] 22%|██▏       | 5/23 [00:06<00:11,  1.51it/s] 26%|██▌       | 6/23 [00:06<00:08,  1.97it/s] 30%|███       | 7/23 [00:06<00:06,  2.45it/s] 35%|███▍      | 8/23 [00:06<00:05,  2.91it/s] 39%|███▉      | 9/23 [00:07<00:04,  3.32it/s] 43%|████▎     | 10/23 [00:07<00:03,  3.68it/s] 48%|████▊     | 11/23 [00:07<00:03,  3.99it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.22it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.36it/s] 61%|██████    | 14/23 [00:08<00:01,  4.51it/s] 65%|██████▌   | 15/23 [00:08<00:01,  4.56it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.65it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.72it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.77it/s] 83%|████████▎ | 19/23 [00:09<00:00,  4.81it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.83it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.85it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.64it/s]100%|██████████| 23/23 [00:09<00:00,  5.27it/s]100%|██████████| 23/23 [00:10<00:00,  2.28it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:04<01:44,  4.77s/it]  9%|▊         | 2/23 [00:05<00:47,  2.28s/it] 13%|█▎        | 3/23 [00:05<00:26,  1.33s/it] 17%|█▋        | 4/23 [00:05<00:16,  1.13it/s] 22%|██▏       | 5/23 [00:05<00:11,  1.56it/s] 26%|██▌       | 6/23 [00:06<00:08,  2.03it/s] 30%|███       | 7/23 [00:06<00:06,  2.50it/s] 35%|███▍      | 8/23 [00:06<00:05,  2.95it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.37it/s] 43%|████▎     | 10/23 [00:06<00:03,  3.72it/s] 48%|████▊     | 11/23 [00:07<00:02,  4.01it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.24it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.42it/s] 61%|██████    | 14/23 [00:07<00:01,  4.52it/s] 65%|██████▌   | 15/23 [00:07<00:01,  4.62it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.69it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.75it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.79it/s] 83%|████████▎ | 19/23 [00:08<00:00,  4.78it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.81it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.82it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.83it/s]100%|██████████| 23/23 [00:09<00:00,  5.43it/s]100%|██████████| 23/23 [00:09<00:00,  2.36it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6507960557937622


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.47704521919227
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.33716567229555
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.191668364083995


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.07960510253906
=========================          END          =========================
0it [00:00, ?it/s]1it [00:02,  2.20s/it]2it [00:03,  1.93s/it]3it [00:05,  1.70s/it]4it [00:06,  1.50s/it]5it [00:07,  1.44s/it]6it [00:09,  1.36s/it]7it [00:10,  1.42s/it]8it [00:11,  1.38s/it]9it [00:13,  1.33s/it]10it [00:14,  1.29s/it]11it [00:15,  1.29s/it]12it [00:17,  1.42s/it]13it [00:18,  1.43s/it]14it [00:19,  1.27s/it]15it [00:20,  1.18s/it]16it [00:22,  1.23s/it]17it [00:23,  1.33s/it]18it [00:24,  1.34s/it]19it [00:26,  1.32s/it]20it [00:27,  1.19s/it]21it [00:28,  1.11s/it]22it [00:29,  1.19s/it]23it [00:30,  1.22s/it]24it [00:31,  1.20s/it]25it [00:32,  1.05s/it]26it [00:33,  1.07it/s]27it [00:33,  1.12it/s]28it [00:35,  1.02it/s]29it [00:36,  1.05s/it]30it [00:37,  1.04s/it]31it [00:39,  1.22s/it]32it [00:40,  1.20s/it]33it [00:40,  1.05s/it]34it [00:41,  1.04s/it]35it [00:43,  1.11s/it]36it [00:44,  1.06s/it]37it [00:45,  1.11s/it]38it [00:47,  1.26s/it]39it [00:48,  1.35s/it]40it [00:50,  1.46s/it]41it [00:51,  1.41s/it]42it [00:52,  1.37s/it]43it [00:54,  1.40s/it]44it [00:55,  1.40s/it]45it [00:56,  1.29s/it]46it [00:57,  1.23s/it]47it [00:59,  1.36s/it]48it [01:00,  1.33s/it]49it [01:02,  1.33s/it]50it [01:02,  1.18s/it]51it [01:04,  1.18s/it]52it [01:05,  1.13s/it]53it [01:06,  1.22s/it]54it [01:08,  1.37s/it]55it [01:09,  1.43s/it]56it [01:11,  1.42s/it]57it [01:12,  1.38s/it]58it [01:13,  1.33s/it]59it [01:14,  1.31s/it]60it [01:15,  1.18s/it]61it [01:16,  1.17s/it]62it [01:18,  1.22s/it]63it [01:19,  1.21s/it]64it [01:20,  1.09s/it]65it [01:21,  1.01it/s]66it [01:22,  1.05s/it]67it [01:23,  1.03s/it]68it [01:24,  1.08s/it]69it [01:25,  1.06s/it]70it [01:26,  1.08it/s]71it [01:26,  1.13it/s]72it [01:27,  1.11it/s]73it [01:29,  1.00it/s]74it [01:30,  1.10s/it]75it [01:31,  1.14s/it]76it [01:32,  1.13s/it]77it [01:33,  1.14s/it]78it [01:34,  1.13s/it]79it [01:36,  1.25s/it]80it [01:37,  1.25s/it]81it [01:39,  1.26s/it]82it [01:40,  1.33s/it]83it [01:41,  1.22s/it]84it [01:42,  1.21s/it]85it [01:43,  1.05s/it]86it [01:44,  1.01s/it]87it [01:45,  1.11s/it]88it [01:46,  1.15s/it]89it [01:48,  1.22s/it]90it [01:49,  1.32s/it]91it [01:50,  1.25s/it]92it [01:52,  1.27s/it]93it [01:53,  1.27s/it]94it [01:54,  1.28s/it]95it [01:56,  1.27s/it]96it [01:57,  1.23s/it]97it [01:58,  1.29s/it]98it [01:59,  1.31s/it]99it [02:01,  1.29s/it]100it [02:02,  1.30s/it]101it [02:04,  1.39s/it]102it [02:05,  1.42s/it]103it [02:06,  1.41s/it]104it [02:08,  1.30s/it]105it [02:09,  1.40s/it]106it [02:11,  1.39s/it]107it [02:12,  1.29s/it]108it [02:13,  1.25s/it]109it [02:14,  1.22s/it]110it [02:15,  1.18s/it]111it [02:16,  1.19s/it]112it [02:17,  1.15s/it]113it [02:18,  1.11s/it]114it [02:20,  1.15s/it]115it [02:20,  1.08s/it]116it [02:21,  1.04it/s]117it [02:22,  1.05s/it]118it [02:24,  1.10s/it]119it [02:25,  1.15s/it]120it [02:26,  1.21s/it]121it [02:28,  1.37s/it]122it [02:30,  1.43s/it]123it [02:31,  1.49s/it]124it [02:32,  1.39s/it]125it [02:34,  1.39s/it]126it [02:35,  1.48s/it]127it [02:37,  1.44s/it]128it [02:40,  2.03s/it]129it [02:44,  2.55s/it]130it [02:45,  2.15s/it]131it [02:46,  1.84s/it]132it [02:47,  1.63s/it]133it [02:49,  1.51s/it]134it [02:49,  1.31s/it]135it [02:50,  1.06s/it]136it [02:51,  1.08it/s]137it [02:52,  1.09s/it]138it [02:53,  1.12s/it]139it [02:54,  1.11s/it]140it [02:55,  1.11s/it]141it [02:57,  1.11s/it]142it [02:58,  1.26s/it]143it [03:00,  1.34s/it]144it [03:01,  1.38s/it]145it [03:02,  1.36s/it]146it [03:03,  1.21s/it]147it [03:04,  1.10s/it]148it [03:05,  1.02s/it]149it [03:06,  1.03it/s]150it [03:07,  1.10it/s]151it [03:07,  1.11it/s]152it [03:09,  1.02it/s]153it [03:10,  1.02it/s]154it [03:11,  1.00s/it]155it [03:12,  1.06s/it]156it [03:13,  1.06s/it]157it [03:15,  1.29s/it]158it [03:17,  1.65s/it]159it [03:20,  1.94s/it]160it [03:23,  2.28s/it]161it [03:25,  2.32s/it]162it [03:27,  2.13s/it]163it [03:28,  1.91s/it]164it [03:29,  1.64s/it]165it [03:30,  1.42s/it]166it [03:32,  1.36s/it]167it [03:33,  1.30s/it]168it [03:34,  1.20s/it]169it [03:35,  1.21s/it]170it [03:37,  1.41s/it]171it [03:38,  1.27s/it]172it [03:39,  1.11s/it]173it [03:39,  1.01s/it]174it [03:40,  1.04it/s]175it [03:41,  1.01it/s]176it [03:42,  1.08s/it]177it [03:44,  1.16s/it]178it [03:45,  1.15s/it]179it [03:46,  1.20s/it]180it [03:48,  1.20s/it]181it [03:49,  1.17s/it]182it [03:50,  1.11s/it]183it [03:51,  1.12s/it]184it [03:53,  1.37s/it]185it [03:54,  1.43s/it]186it [03:55,  1.36s/it]187it [03:57,  1.32s/it]188it [03:57,  1.16s/it]189it [03:58,  1.02s/it]190it [04:00,  1.28s/it]191it [04:02,  1.43s/it]192it [04:04,  1.53s/it]193it [04:05,  1.38s/it]194it [04:06,  1.24s/it]195it [04:06,  1.09s/it]196it [04:07,  1.01it/s]197it [04:08,  1.07it/s]198it [04:09,  1.11it/s]199it [04:09,  1.18it/s]200it [04:10,  1.12it/s]201it [04:11,  1.26it/s]202it [04:11,  1.48it/s]203it [04:12,  1.40it/s]204it [04:13,  1.35it/s]205it [04:14,  1.28it/s]206it [04:15,  1.28it/s]207it [04:17,  1.15s/it]208it [04:18,  1.34s/it]209it [04:20,  1.31s/it]210it [04:22,  1.53s/it]211it [04:24,  1.82s/it]212it [04:26,  1.82s/it]213it [04:28,  1.89s/it]213it [04:28,  1.26s/it]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:06<02:18,  6.32s/it]  9%|▊         | 2/23 [00:06<00:57,  2.72s/it] 13%|█▎        | 3/23 [00:06<00:31,  1.57s/it] 17%|█▋        | 4/23 [00:06<00:19,  1.03s/it] 22%|██▏       | 5/23 [00:07<00:13,  1.36it/s] 26%|██▌       | 6/23 [00:07<00:09,  1.80it/s] 30%|███       | 7/23 [00:07<00:07,  2.27it/s] 35%|███▍      | 8/23 [00:07<00:05,  2.73it/s] 39%|███▉      | 9/23 [00:07<00:04,  3.16it/s] 43%|████▎     | 10/23 [00:08<00:03,  3.54it/s] 48%|████▊     | 11/23 [00:08<00:03,  3.87it/s] 52%|█████▏    | 12/23 [00:08<00:02,  4.11it/s] 57%|█████▋    | 13/23 [00:08<00:02,  4.32it/s] 61%|██████    | 14/23 [00:08<00:02,  4.43it/s] 65%|██████▌   | 15/23 [00:09<00:01,  4.53it/s] 70%|██████▉   | 16/23 [00:09<00:01,  4.63it/s] 74%|███████▍  | 17/23 [00:09<00:01,  4.70it/s] 78%|███████▊  | 18/23 [00:09<00:01,  4.75it/s] 83%|████████▎ | 19/23 [00:10<00:00,  4.79it/s] 87%|████████▋ | 20/23 [00:10<00:00,  4.78it/s] 91%|█████████▏| 21/23 [00:10<00:00,  4.81it/s] 96%|█████████▌| 22/23 [00:10<00:00,  4.83it/s]100%|██████████| 23/23 [00:10<00:00,  5.42it/s]100%|██████████| 23/23 [00:10<00:00,  2.09it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.805993378162384
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:04<01:43,  4.73s/it]  9%|▊         | 2/23 [00:05<00:52,  2.52s/it] 13%|█▎        | 3/23 [00:05<00:29,  1.46s/it] 17%|█▋        | 4/23 [00:06<00:18,  1.03it/s] 22%|██▏       | 5/23 [00:06<00:12,  1.44it/s] 26%|██▌       | 6/23 [00:06<00:08,  1.89it/s] 30%|███       | 7/23 [00:06<00:06,  2.37it/s] 35%|███▍      | 8/23 [00:06<00:05,  2.83it/s] 39%|███▉      | 9/23 [00:07<00:04,  3.26it/s] 43%|████▎     | 10/23 [00:07<00:03,  3.63it/s] 48%|████▊     | 11/23 [00:07<00:03,  3.94it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.19it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.37it/s] 61%|██████    | 14/23 [00:08<00:01,  4.51it/s] 65%|██████▌   | 15/23 [00:08<00:01,  4.60it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.64it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.70it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.75it/s] 83%|████████▎ | 19/23 [00:09<00:00,  4.78it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.81it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.83it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.84it/s]100%|██████████| 23/23 [00:09<00:00,  5.44it/s]100%|██████████| 23/23 [00:10<00:00,  2.27it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6513437628746033


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82849681111604
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.2413862140927


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59933471679688
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.874007594062824
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.64608064715691
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.397142747007546


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.1343765258789
=========================          END          =========================
0it [00:00, ?it/s]1it [00:01,  1.02s/it]2it [00:01,  1.30it/s]3it [00:02,  1.27it/s]4it [00:03,  1.33it/s]5it [00:03,  1.32it/s]6it [00:04,  1.32it/s]7it [00:05,  1.35it/s]8it [00:06,  1.38it/s]9it [00:06,  1.37it/s]10it [00:07,  1.26it/s]11it [00:08,  1.35it/s]12it [00:09,  1.30it/s]13it [00:10,  1.12it/s]14it [00:11,  1.20it/s]15it [00:12,  1.06s/it]16it [00:13,  1.02it/s]17it [00:14,  1.07it/s]18it [00:15,  1.10it/s]19it [00:15,  1.14it/s]20it [00:16,  1.22it/s]21it [00:17,  1.27it/s]22it [00:18,  1.28it/s]23it [00:18,  1.27it/s]24it [00:19,  1.26it/s]25it [00:20,  1.28it/s]26it [00:21,  1.39it/s]27it [00:21,  1.41it/s]28it [00:22,  1.40it/s]29it [00:23,  1.43it/s]30it [00:23,  1.37it/s]31it [00:24,  1.37it/s]32it [00:25,  1.31it/s]33it [00:26,  1.32it/s]34it [00:27,  1.21it/s]35it [00:28,  1.14it/s]36it [00:28,  1.24it/s]37it [00:29,  1.40it/s]38it [00:30,  1.33it/s]39it [00:30,  1.37it/s]40it [00:31,  1.28it/s]41it [00:32,  1.29it/s]42it [00:33,  1.21it/s]43it [00:34,  1.20it/s]44it [00:35,  1.17it/s]45it [00:36,  1.06it/s]46it [00:37,  1.17it/s]47it [00:37,  1.21it/s]48it [00:38,  1.27it/s]49it [00:39,  1.13it/s]50it [00:40,  1.11it/s]51it [00:41,  1.06it/s]52it [00:42,  1.10it/s]53it [00:43,  1.16it/s]54it [00:43,  1.26it/s]55it [00:44,  1.31it/s]56it [00:45,  1.29it/s]57it [00:46,  1.28it/s]58it [00:46,  1.32it/s]59it [00:47,  1.32it/s]60it [00:48,  1.35it/s]61it [00:48,  1.36it/s]62it [00:49,  1.33it/s]63it [00:50,  1.38it/s]64it [00:51,  1.37it/s]65it [00:51,  1.62it/s]66it [00:52,  1.64it/s]67it [00:52,  1.59it/s]68it [00:53,  1.51it/s]69it [00:53,  1.65it/s]70it [00:54,  1.63it/s]71it [00:55,  1.52it/s]72it [00:56,  1.40it/s]73it [00:57,  1.32it/s]74it [00:57,  1.27it/s]75it [00:58,  1.26it/s]76it [00:59,  1.32it/s]77it [01:00,  1.32it/s]78it [01:00,  1.36it/s]79it [01:01,  1.40it/s]80it [01:02,  1.38it/s]81it [01:03,  1.34it/s]82it [01:03,  1.33it/s]83it [01:04,  1.42it/s]84it [01:05,  1.40it/s]85it [01:06,  1.31it/s]86it [01:06,  1.26it/s]87it [01:07,  1.26it/s]88it [01:08,  1.27it/s]89it [01:09,  1.28it/s]90it [01:09,  1.31it/s]91it [01:10,  1.29it/s]92it [01:11,  1.45it/s]93it [01:11,  1.52it/s]94it [01:12,  1.51it/s]95it [01:13,  1.46it/s]96it [01:14,  1.42it/s]97it [01:14,  1.40it/s]98it [01:15,  1.37it/s]99it [01:16,  1.25it/s]100it [01:17,  1.26it/s]101it [01:18,  1.25it/s]102it [01:19,  1.16it/s]103it [01:19,  1.18it/s]104it [01:20,  1.15it/s]105it [01:21,  1.30it/s]106it [01:22,  1.25it/s]107it [01:22,  1.32it/s]108it [01:23,  1.27it/s]109it [01:24,  1.19it/s]110it [01:25,  1.24it/s]111it [01:26,  1.29it/s]112it [01:26,  1.32it/s]113it [01:27,  1.31it/s]114it [01:28,  1.37it/s]115it [01:30,  1.23s/it]116it [01:31,  1.23s/it]117it [01:33,  1.31s/it]118it [01:34,  1.16s/it]119it [01:35,  1.12s/it]120it [01:35,  1.00s/it]121it [01:36,  1.02s/it]122it [01:37,  1.00it/s]123it [01:38,  1.03it/s]124it [01:39,  1.09it/s]125it [01:40,  1.17it/s]126it [01:41,  1.23it/s]127it [01:41,  1.24it/s]128it [01:42,  1.39it/s]129it [01:43,  1.39it/s]130it [01:43,  1.38it/s]131it [01:44,  1.35it/s]132it [01:45,  1.24it/s]133it [01:46,  1.26it/s]134it [01:47,  1.11it/s]135it [01:48,  1.10it/s]136it [01:49,  1.18it/s]137it [01:49,  1.23it/s]138it [01:50,  1.26it/s]139it [01:51,  1.10it/s]140it [01:52,  1.13it/s]141it [01:53,  1.02s/it]142it [01:54,  1.01s/it]143it [01:55,  1.00s/it]144it [01:56,  1.02it/s]145it [01:58,  1.04s/it]146it [01:59,  1.09s/it]147it [02:00,  1.11s/it]148it [02:01,  1.18s/it]149it [02:02,  1.21s/it]150it [02:03,  1.14s/it]151it [02:04,  1.07s/it]152it [02:05,  1.05s/it]153it [02:07,  1.14s/it]154it [02:08,  1.25s/it]155it [02:09,  1.05s/it]156it [02:10,  1.12s/it]157it [02:11,  1.06s/it]158it [02:12,  1.13s/it]159it [02:13,  1.02it/s]160it [02:14,  1.12it/s]161it [02:15,  1.11it/s]162it [02:16,  1.04it/s]163it [02:17,  1.05it/s]164it [02:17,  1.13it/s]165it [02:18,  1.19it/s]166it [02:19,  1.17it/s]167it [02:20,  1.18it/s]168it [02:21,  1.12it/s]169it [02:22,  1.04it/s]170it [02:23,  1.09it/s]171it [02:24,  1.07it/s]172it [02:24,  1.16it/s]173it [02:25,  1.18it/s]174it [02:26,  1.19it/s]175it [02:27,  1.24it/s]176it [02:28,  1.24it/s]177it [02:28,  1.30it/s]178it [02:29,  1.40it/s]179it [02:30,  1.34it/s]180it [02:31,  1.28it/s]181it [02:31,  1.27it/s]182it [02:32,  1.18it/s]183it [02:34,  1.06it/s]184it [02:35,  1.14s/it]185it [02:38,  1.54s/it]186it [02:39,  1.37s/it]187it [02:39,  1.20s/it]188it [02:40,  1.06s/it]189it [02:41,  1.04s/it]190it [02:42,  1.13s/it]191it [02:43,  1.00it/s]192it [02:44,  1.01s/it]193it [02:45,  1.05s/it]194it [02:46,  1.01s/it]195it [02:47,  1.09it/s]196it [02:48,  1.18it/s]197it [02:48,  1.20it/s]198it [02:49,  1.20it/s]199it [02:50,  1.19it/s]200it [02:51,  1.21it/s]201it [02:51,  1.33it/s]202it [02:53,  1.15it/s]203it [02:54,  1.05s/it]204it [02:56,  1.16s/it]205it [02:57,  1.29s/it]206it [02:59,  1.37s/it]207it [02:59,  1.19s/it]208it [03:00,  1.11s/it]209it [03:01,  1.03s/it]210it [03:02,  1.08it/s]211it [03:03,  1.15it/s]212it [03:03,  1.20it/s]213it [03:04,  1.31it/s]213it [03:04,  1.15it/s]
Number of selected candidates = 144
---> Each Classifier' shapes
	 GT_classifier = 200
	 ViLang_guessed = 144
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:05<02:02,  5.58s/it]  9%|▊         | 2/23 [00:06<00:57,  2.76s/it] 13%|█▎        | 3/23 [00:06<00:31,  1.59s/it] 17%|█▋        | 4/23 [00:06<00:19,  1.05s/it] 22%|██▏       | 5/23 [00:06<00:13,  1.35it/s] 26%|██▌       | 6/23 [00:07<00:09,  1.77it/s] 30%|███       | 7/23 [00:07<00:07,  2.24it/s] 35%|███▍      | 8/23 [00:07<00:05,  2.70it/s] 39%|███▉      | 9/23 [00:07<00:04,  3.13it/s] 43%|████▎     | 10/23 [00:08<00:03,  3.52it/s] 48%|████▊     | 11/23 [00:08<00:03,  3.85it/s] 52%|█████▏    | 12/23 [00:08<00:02,  4.10it/s] 57%|█████▋    | 13/23 [00:08<00:02,  4.29it/s] 61%|██████    | 14/23 [00:08<00:02,  4.41it/s] 65%|██████▌   | 15/23 [00:09<00:01,  4.55it/s] 70%|██████▉   | 16/23 [00:09<00:01,  4.65it/s] 74%|███████▍  | 17/23 [00:09<00:01,  4.71it/s] 78%|███████▊  | 18/23 [00:09<00:01,  4.76it/s] 83%|████████▎ | 19/23 [00:09<00:00,  4.80it/s] 87%|████████▋ | 20/23 [00:10<00:00,  4.82it/s] 91%|█████████▏| 21/23 [00:10<00:00,  4.84it/s] 96%|█████████▌| 22/23 [00:10<00:00,  4.85it/s]100%|██████████| 23/23 [00:10<00:00,  5.40it/s]100%|██████████| 23/23 [00:11<00:00,  2.00it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.8059782385826111
---> Evaluating
  0%|          | 0/23 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
  4%|▍         | 1/23 [00:04<01:45,  4.78s/it]  9%|▊         | 2/23 [00:04<00:43,  2.09s/it] 13%|█▎        | 3/23 [00:05<00:24,  1.23s/it] 17%|█▋        | 4/23 [00:05<00:19,  1.05s/it] 22%|██▏       | 5/23 [00:06<00:13,  1.34it/s] 26%|██▌       | 6/23 [00:06<00:09,  1.78it/s] 30%|███       | 7/23 [00:06<00:07,  2.25it/s] 35%|███▍      | 8/23 [00:06<00:05,  2.71it/s] 39%|███▉      | 9/23 [00:06<00:04,  3.15it/s] 43%|████▎     | 10/23 [00:07<00:03,  3.53it/s] 48%|████▊     | 11/23 [00:07<00:03,  3.83it/s] 52%|█████▏    | 12/23 [00:07<00:02,  4.09it/s] 57%|█████▋    | 13/23 [00:07<00:02,  4.24it/s] 61%|██████    | 14/23 [00:08<00:02,  4.42it/s] 65%|██████▌   | 15/23 [00:08<00:01,  4.55it/s] 70%|██████▉   | 16/23 [00:08<00:01,  4.64it/s] 74%|███████▍  | 17/23 [00:08<00:01,  4.71it/s] 78%|███████▊  | 18/23 [00:08<00:01,  4.75it/s] 83%|████████▎ | 19/23 [00:09<00:00,  4.78it/s] 87%|████████▋ | 20/23 [00:09<00:00,  4.79it/s] 91%|█████████▏| 21/23 [00:09<00:00,  4.82it/s] 96%|█████████▌| 22/23 [00:09<00:00,  4.84it/s]100%|██████████| 23/23 [00:09<00:00,  5.44it/s]100%|██████████| 23/23 [00:10<00:00,  2.18it/s]
Loading all-mpnet-base-v2...
Encoding...
pred_feats.shape torch.Size([5794, 768])
gt_feats.shape torch.Size([5794, 768])
Semantic similarity score = 0.6521774530410767


========================= UPPERBOUND: CLIP ZERO-SHOT-based Final Results =========================


[Clustering]
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Acc: 57.30065585088022
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Nmi: 77.82490197137021
Total UPPERBOUND: CLIP ZERO-SHOT-based Clustering Ari: 42.24100114804811


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 80.59782409667969
=========================          END          =========================


========================= OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Final Results =========================


[Clustering]
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Acc: 44.891266827752844
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Nmi: 73.52490626722896
Total OURS: VILANGGUESSED W/ ALPHA=0.7, N_TTA=10-based Clustering Ari: 31.67631731075239


[ssACC (semantic similarity ACC]
ssACC_sbert_base: 65.21774291992188
=========================          END          =========================


========================= ViLang Final Results of 10 runs, w/ random imgs per class=========================


[Clustering]
Clustering ACC: 44.659993096306515
Semantic ACC:   65.13904571533203
=========================          END          =========================
